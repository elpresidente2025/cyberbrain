
import asyncio
import re
import logging
from difflib import SequenceMatcher
from typing import Dict, Any, List, Optional
from .election_rules import get_election_stage

logger = logging.getLogger(__name__)

TITLE_LENGTH_HARD_MIN = 12
TITLE_LENGTH_HARD_MAX = 35
TITLE_LENGTH_OPTIMAL_MIN = 15
TITLE_LENGTH_OPTIMAL_MAX = 30

EVENT_NAME_MARKERS = (
    'ì¶œíŒê¸°ë…íšŒ',
    'ê°„ë‹´íšŒ',
    'ì„¤ëª…íšŒ',
    'í† ë¡ íšŒ',
    'ê¸°ìíšŒê²¬',
    'ì„¸ë¯¸ë‚˜',
    'ê°•ì—°',
    'ë¶í† í¬',
    'í† í¬ì½˜ì„œíŠ¸',
    'íŒ¬ë¯¸íŒ…',
)

SLOT_PLACEHOLDER_NAMES = (
    'ì§€ì—­ëª…', 'ì¥ì†Œëª…', 'ì¸ë¬¼ëª…', 'í–‰ì‚¬ëª…', 'ë‚ ì§œ', 'ì£¼ì œëª…', 'ì •ì±…ëª…', 'ì‚¬ì—…ëª…',
    'ìˆ˜ì¹˜', 'ìˆ˜ëŸ‰', 'ê¸ˆì•¡', 'ë‹¨ìœ„', 'ì„±ê³¼ì§€í‘œ', 'ì§€ì›í•­ëª©', 'í˜„ì•ˆ', 'ë¯¼ì›ì£¼ì œ',
    'ì´ìŠˆëª…', 'ì •ì±…ìŸì ', 'ë¬¸ì œëª…', 'ëŒ€ì•ˆìˆ˜', 'ì´ì „ê°’', 'í˜„ì¬ê°’', 'ê°œì„ í­',
    'ê¸°ì¡´ì•ˆ', 'ê°œì„ ì•ˆ', 'ë¹„ìš©í•­ëª©', 'ì´ì „ê¸ˆì•¡', 'í˜„ì¬ê¸ˆì•¡', 'ê°œê´€ì‹œê¸°', 'ê¸°ê°„',
    'ê°œì„ ìˆ˜ì¹˜', 'ë²•ì•ˆëª…', 'í•µì‹¬ì§€ì›', 'ì¡°ë¡€ëª…', 'í•µì‹¬ë³€ê²½', 'ìˆ«ì', 'í•µì‹¬í˜œíƒ',
    'í•µì‹¬ë³€í™”', 'ì—°ë„/ë¶„ê¸°', 'ë³´ê³ ì„œëª…', 'í•µì‹¬ì„±ê³¼ìˆ˜', 'ì›”/ë¶„ê¸°', 'ì—…ë¬´ëª…',
    'ê±´ìˆ˜', 'ì •ê¸°ë¸Œë¦¬í•‘ëª…', 'ì›”í˜¸', 'í•µì‹¬ì£¼ì œ', 'ì˜ˆì‚°í•­ëª©', 'í˜œíƒìˆ˜ì¹˜', 'ì„±ê³¼ìˆ˜',
)

# ì‚¬ìš©ì ì œê³µ "ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì œëª© ì „ëµ (ì •ì¹˜ì¸ íŠ¹í™”)"ë¥¼
# ê³ ì • ë¬¸êµ¬ê°€ ì•„ë‹Œ ìŠ¬ë¡¯ ê¸°ë°˜ í…œí”Œë¦¿ few-shotìœ¼ë¡œ ì£¼ì…í•œë‹¤.
USER_PROVIDED_TITLE_FEW_SHOT: Dict[str, Dict[str, Any]] = {
    'DATA_BASED': {
        'name': 'êµ¬ì²´ì  ë°ì´í„° ê¸°ë°˜ (ì„±ê³¼ ë³´ê³ )',
        'templates': [
            {'template': '[ì •ì±…ëª…] [ìˆ˜ì¹˜][ë‹¨ìœ„] ë‹¬ì„±, [ì„±ê³¼ì§€í‘œ] ê°œì„ ', 'intent': 'ìˆ˜ì¹˜ ê¸°ë°˜ ì„±ê³¼ ì „ë‹¬'},
            {'template': '[ì‚¬ì—…ëª…] [ìˆ˜ëŸ‰][ë‹¨ìœ„] ì§€ì› ì™„ë£Œ', 'intent': 'ì™„ë£Œëœ ì‹¤ì  ê°•ì¡°'},
            {'template': '[ì˜ˆì‚°í•­ëª©] [ê¸ˆì•¡] í™•ë³´, [ì‚¬ì—…ëª…] ì¶”ì§„', 'intent': 'ì˜ˆì‚°Â·ì§‘í–‰ ì‹ ë¢° ê°•í™”'},
        ],
        'bad_to_fix': [
            {'bad': 'ì¢‹ì€ ì„±ê³¼ ê±°ë’€ìŠµë‹ˆë‹¤', 'fix_template': '[ì‚¬ì—…ëª…] [ìˆ˜ëŸ‰][ë‹¨ìœ„] ì§€ì› ì™„ë£Œ'},
            {'bad': 'ì˜ˆì‚° ë§ì´ í™•ë³´í–ˆì–´ìš”', 'fix_template': '[ì˜ˆì‚°í•­ëª©] [ê¸ˆì•¡] í™•ë³´'},
        ],
    },
    'QUESTION_ANSWER': {
        'name': 'ì§ˆë¬¸-í•´ë‹µ êµ¬ì¡° (AEO ìµœì í™”)',
        'templates': [
            {'template': '[ì§€ì—­ëª…] [ì •ì±…ëª…], [ì§€ì›í•­ëª©] ì–¼ë§ˆê¹Œì§€?', 'intent': 'ê²€ìƒ‰ ì§ˆë¬¸ ì§ì ‘ ëŒ€ì‘'},
            {'template': '[ì§€ì—­ëª…] [í˜„ì•ˆ], ì–´ë–»ê²Œ í’€ê¹Œ?', 'intent': 'ë¬¸ì œ-í•´ê²° í”„ë ˆì´ë°'},
            {'template': '[ë¯¼ì›ì£¼ì œ], ì‹¤ì œë¡œ ì–¸ì œ í•´ê²°ë˜ë‚˜?', 'intent': 'ì‹¤í–‰ ì‹œì  ê¶ê¸ˆì¦ ìœ ë„'},
        ],
        'bad_to_fix': [
            {'bad': 'ì •ì±…ì— ëŒ€í•´ ì„¤ëª…ë“œë¦½ë‹ˆë‹¤', 'fix_template': '[ì •ì±…ëª…], ë¬´ì—‡ì´ ë‹¬ë¼ì¡Œë‚˜?'},
            {'bad': 'ì£¼ê±° ê´€ë ¨ ì•ˆë‚´', 'fix_template': '[ì´ìŠˆëª…], ë³´ìƒì€ ì–´ë–»ê²Œ ë˜ë‚˜?'},
        ],
    },
    'COMPARISON': {
        'name': 'ë¹„êµÂ·ëŒ€ì¡° êµ¬ì¡° (ì„±ê³¼ ì¦ëª…)',
        'templates': [
            {'template': '[ì§€í‘œëª…] [ì´ì „ê°’]â†’[í˜„ì¬ê°’], [ê°œì„ í­] ê°œì„ ', 'intent': 'ì „í›„ ë³€í™” ì¦ëª…'},
            {'template': '[ì •ì±…ëª…] [ê¸°ì¡´ì•ˆ]â†’[ê°œì„ ì•ˆ] í™•ëŒ€', 'intent': 'ì •ì±… ì—…ê·¸ë ˆì´ë“œ ê°•ì¡°'},
            {'template': '[ë¹„ìš©í•­ëª©] [ì´ì „ê¸ˆì•¡]â†’[í˜„ì¬ê¸ˆì•¡], ì ˆê° ì‹¤í˜„', 'intent': 'ì˜ˆì‚° íš¨ìœ¨ ì–´í•„'},
        ],
        'bad_to_fix': [
            {'bad': 'ì´ì „ë³´ë‹¤ ë‚˜ì•„ì¡Œì–´ìš”', 'fix_template': '[ì§€í‘œëª…] [ì´ì „ê°’]â†’[í˜„ì¬ê°’] ê°œì„ '},
            {'bad': 'ì‹œê°„ì´ ë‹¨ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤', 'fix_template': '[ì—…ë¬´ëª…] [ì´ì „ê¸°ê°„]â†’[í˜„ì¬ê¸°ê°„] ë‹¨ì¶•'},
        ],
    },
    'LOCAL_FOCUSED': {
        'name': 'ì§€ì—­ ë§ì¶¤í˜• ì •ë³´ (ì´ˆì§€ì—­í™”)',
        'templates': [
            {'template': '[ì§€ì—­ëª…] [ì •ì±…ëª…], [ìˆ˜ì¹˜][ë‹¨ìœ„] ì§€ì›', 'intent': 'ì§€ì—­+ì •ì±…+ìˆ˜ì¹˜ ê²°í•©'},
            {'template': '[ì§€ì—­ëª…] [ì‚¬ì—…ëª…], [ê°œê´€ì‹œê¸°] ê°œì‹œ', 'intent': 'ì¼ì • ëª…í™•í™”'},
            {'template': '[ì§€ì—­ëª…] [í˜„ì•ˆ], [ê¸°ê°„]ê°„ [ê°œì„ ìˆ˜ì¹˜]% ê°œì„ ', 'intent': 'ì²´ê° ì„±ê³¼ ì „ë‹¬'},
        ],
        'bad_to_fix': [
            {'bad': 'ìš°ë¦¬ ì§€ì—­ì„ ìœ„í•´ ë…¸ë ¥í•©ë‹ˆë‹¤', 'fix_template': '[ì§€ì—­ëª…] [ì‚¬ì—…ëª…] [ìˆ˜ì¹˜][ë‹¨ìœ„] í™•ë³´'},
            {'bad': 'ì§€ì—­ ì •ì±… ì•ˆë‚´', 'fix_template': '[ì§€ì—­ëª…] [ì •ì±…ëª…] [í˜œíƒìˆ˜ì¹˜] ì§€ì›'},
        ],
    },
    'EXPERT_KNOWLEDGE': {
        'name': 'ì „ë¬¸ ì§€ì‹ ê³µìœ  (ë²•ì•ˆÂ·ì¡°ë¡€Â·ì •ì±…)',
        'templates': [
            {'template': '[ë²•ì•ˆëª…] ë°œì˜, [í•µì‹¬ì§€ì›] ì¶”ì§„', 'intent': 'ì…ë²• ì•¡ì…˜ ëª…ì‹œ'},
            {'template': '[ì¡°ë¡€ëª…] ê°œì •, [í•µì‹¬ë³€ê²½] ë°˜ì˜', 'intent': 'ì œë„ ë³€ê²½ì  ì „ë‹¬'},
            {'template': '[ì •ì±…ëª…], í•µì‹¬ [ìˆ«ì]ê°€ì§€ ì •ë¦¬', 'intent': 'ì „ë¬¸ ì •ë³´ ìš”ì•½'},
        ],
        'bad_to_fix': [
            {'bad': 'ë²•ì•ˆì„ ë°œì˜í–ˆìŠµë‹ˆë‹¤', 'fix_template': '[ë²•ì•ˆëª…] ë°œì˜, [í•µì‹¬í˜œíƒ] ë°˜ì˜'},
            {'bad': 'ì •ì±…ì„ ì¶”ì§„í•˜ê² ìŠµë‹ˆë‹¤', 'fix_template': '[ì •ì±…ëª…] [í•µì‹¬ë³€í™”] ì¶”ì§„'},
        ],
    },
    'TIME_BASED': {
        'name': 'ì‹œê°„ ì¤‘ì‹¬ ì‹ ë¢°ì„± (ì •ê¸° ë³´ê³ )',
        'templates': [
            {'template': '[ì—°ë„/ë¶„ê¸°] [ë³´ê³ ì„œëª…], [í•µì‹¬ì„±ê³¼ìˆ˜]ëŒ€ ì„±ê³¼', 'intent': 'ì •ê¸°ì„±+ì„±ê³¼ ìš”ì•½'},
            {'template': '[ì›”/ë¶„ê¸°] [ì—…ë¬´ëª…] ë¦¬í¬íŠ¸, [ê±´ìˆ˜]ê±´ ì²˜ë¦¬', 'intent': 'ì›”ê°„ ì‹¤ì  ì‹ ë¢° ê°•í™”'},
            {'template': '[ì •ê¸°ë¸Œë¦¬í•‘ëª…]([ì›”í˜¸]), [í•µì‹¬ì£¼ì œ] ê³µê°œ', 'intent': 'ì •ë¡€ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ê³ ì •í™”'},
        ],
        'bad_to_fix': [
            {'bad': 'ë³´ê³ ì„œë¥¼ ì˜¬ë¦½ë‹ˆë‹¤', 'fix_template': '[ì—°ë„/ë¶„ê¸°] [ë³´ê³ ì„œëª…], [ì„±ê³¼ìˆ˜]ëŒ€ ì„±ê³¼'},
            {'bad': 'ìµœê·¼ í™œë™ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤', 'fix_template': '[ì›”/ë¶„ê¸°] [ì—…ë¬´ëª…] ë¦¬í¬íŠ¸, [ê±´ìˆ˜]ê±´ ì²˜ë¦¬'},
        ],
    },
    'ISSUE_ANALYSIS': {
        'name': 'ì •ê³„ ì´ìŠˆÂ·ë¶„ì„ (êµ­ê°€ ì •ì±…Â·ê±°ì‹œ)',
        'templates': [
            {'template': '[ì´ìŠˆëª…], ì‹¤ì œë¡œ ë­ê°€ ë‹¬ë¼ì§ˆê¹Œ?', 'intent': 'ë³€í™” ê¶ê¸ˆì¦ ìœ ë„'},
            {'template': '[ì •ì±…ìŸì ], ì–´ë–»ê²Œ ê°œì„ í• ê¹Œ?', 'intent': 'í•´ë²• íƒìƒ‰í˜•'},
            {'template': '[ë¬¸ì œëª…], [ëŒ€ì•ˆìˆ˜]ëŒ€ ëŒ€ì•ˆ ì œì‹œ', 'intent': 'ë¶„ì„-ëŒ€ì•ˆ êµ¬ì¡°'},
        ],
        'bad_to_fix': [
            {'bad': 'ì •ì¹˜ í˜„ì‹¤ì— ëŒ€í•´ ìƒê°í•´ ë´…ì‹œë‹¤', 'fix_template': '[ì´ìŠˆëª…], ì‹¤ì œë¡œ ë­ê°€ ë‹¬ë¼ì§ˆê¹Œ?'},
            {'bad': 'ë¬¸ì œê°€ ë§ìŠµë‹ˆë‹¤', 'fix_template': '[ë¬¸ì œëª…], [ëŒ€ì•ˆìˆ˜]ëŒ€ ëŒ€ì•ˆ ì œì‹œ'},
        ],
    },
}

TITLE_TYPES = {
    'VIRAL_HOOK': {
        'id': 'VIRAL_HOOK',
        'name': 'âš¡ ì„œì‚¬ì  ê¸´ì¥ê° (Narrative Hook)',
        'when': 'ë…ìì˜ í˜¸ê¸°ì‹¬ì„ ìœ ë°œí•˜ë˜, êµ¬ì²´ì  ì‚¬ì‹¤ ê¸°ë°˜ì˜ ì„œì‚¬ì  ê¸´ì¥ê°ìœ¼ë¡œ í´ë¦­ì„ ìœ ë„í•  ë•Œ (ê¸°ë³¸ê°’)',
        'pattern': 'ì •ë³´ ê²©ì°¨(Information Gap) êµ¬ì¡°: êµ¬ì²´ì  íŒ©íŠ¸ + ë¯¸ì™„ê²° ì„œì‚¬ or ì˜ì™¸ì˜ ëŒ€ë¹„',
        'naverTip': 'ì œëª©ì´ "ë‹µ"ì´ ì•„ë‹ˆë¼ "ì§ˆë¬¸"ì„ ë‚¨ê¸¸ ë•Œ CTRì´ ê°€ì¥ ë†’ìŒ. êµ¬ì²´ì  ìˆ˜ì¹˜+ë¯¸ì™„ê²° ë¬¸ì¥ì´ ìµœì .',
        'principle': 'ã€ì¢‹ì€ ì œëª©ì˜ íŒë‹¨ ê¸°ì¤€ã€‘\n'
            '- ì½ì—ˆì„ ë•Œ "ê·¸ë˜ì„œ ì–´ë–»ê²Œ ëì§€?" ë˜ëŠ” "ì™œ?"ë¼ëŠ” ìƒê°ì´ ë“œëŠ”ê°€?\n'
            '- ì •ë³´ ìš”ì†Œê°€ 3ê°œ ì´í•˜ì¸ê°€? (ê³¼ë°€ = ì½íˆì§€ ì•ŠìŒ)\n'
            '- ê¸°ë²• í•˜ë‚˜ë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì•„ ìˆëŠ”ê°€? (ê¸°ë²• 2ê°œ ì´ìƒ = ì–µì§€)\n'
            '\n'
            'ã€ì•ˆí‹°íŒ¨í„´: ì´ë ‡ê²Œ í•˜ë©´ ì•ˆ ëœë‹¤ã€‘\n'
            '- âŒ ì•„ë¬´ ë¬¸ì¥ ëì— "~ì˜ ì„ íƒì€?" ë¶™ì´ê¸° (í˜•ì‹ë§Œ ë¯¸ì™„ê²°, ë‚´ìš©ì€ ê³µí—ˆ)\n'
            '- âŒ í‚¤ì›Œë“œ 4ê°œ ì´ìƒ ìš±ì—¬ë„£ê¸° (ì½ëŠ” ìˆœê°„ í”¼ë¡œ)\n'
            '- âŒ ì˜ˆì‹œ ì œëª©ì˜ ì–´ë¯¸ë§Œ ë³µì‚¬í•˜ê¸° (íŒ¨í„´ ëª¨ë°© â‰  ê¸´ì¥ê°)',
        'good': [
            {'title': 'ë¶€ì‚° ì§€ë°©ì„ ê±°, ì™œ ì´ ë‚¨ìê°€ ë›°ì–´ë“¤ì—ˆë‚˜', 'chars': 20, 'analysis': 'ì™œ ì§ˆë¬¸í˜• â€” êµ¬ì²´ì  ì¸ë¬¼ + ë¯¸ì™„ê²° ì§ˆë¬¸'},
            {'title': 'ë¶€ì‚° ì§€ë°©ì„ ê±°ì— ë›°ì–´ë“  ë¶€ë‘ ë…¸ë™ìì˜ ì•„ë“¤', 'chars': 21, 'analysis': 'ì„œì‚¬ ì•„í¬ â€” ì¶œì‹  ë°°ê²½ì´ í˜¸ê¸°ì‹¬ ìœ ë°œ'},
            {'title': 'ë¶€ì‚° ì§€ë°©ì„ ê±°, ì´ì¬ì„±ì€ ì™œ ë‹¤ë¥¸ê°€', 'chars': 17, 'analysis': 'ê°„ê²° ë„ë°œí˜• â€” ì§§ê³  ê°•ë ¬í•œ ì§ˆë¬¸'},
            {'title': 'ë¶€ì‚° ì§€ë°©ì„ ê±°, 10ë§Œ ì²­ë…„ì´ ë– ë‚œ ë„ì‹œì˜ ë°˜ë€', 'chars': 22, 'analysis': 'ìˆ˜ì¹˜+ì‚¬ê±´í˜• â€” íŒ©íŠ¸ ì¶©ê²© + ì‚¬ê±´ ì•”ì‹œ'},
            {'title': 'ë¶€ì‚° ì§€ë°©ì„ ê±°, ì›ì¹™ë§Œìœ¼ë¡œ ì´ê¸¸ ìˆ˜ ìˆì„ê¹Œ', 'chars': 20, 'analysis': 'ë„ë°œì  ì§ˆë¬¸ â€” ê°€ì¹˜ ë…¼ìŸ ìœ ë°œ'}
        ],
        'bad': [
            {'title': 'ë¶€ì‚° ì§€ë°©ì„ ê±°, AI ì „ë¬¸ê°€ ì´ì¬ì„±ì´ ê²½ì œë¥¼ ë°”ê¾¼ë‹¤', 'problem': 'ì„ ì–¸í˜• â€” ë‹µì„ ë‹¤ ì•Œë ¤ì¤˜ì„œ í´ë¦­í•  ì´ìœ  ì—†ìŒ', 'fix': 'ë¶€ì‚° ì§€ë°©ì„ ê±°, ì™œ ì´ ë‚¨ìê°€ ë›°ì–´ë“¤ì—ˆë‚˜'},
            {'title': 'ì´ì¬ì„± ë¶€ì‚° ì§€ë°©ì„ ê±°, AI 3ëŒ€ ê°•êµ­?', 'problem': 'í‚¤ì›Œë“œ ë‚˜ì—´ â€” ë¬¸ì¥ì´ ì•„ë‹˜, ì˜ë¯¸ ë¶ˆë¶„ëª…', 'fix': 'ë¶€ì‚° ì§€ë°©ì„ ê±°, ì´ì¬ì„±ì€ ì™œ ë‹¤ë¥¸ê°€'},
            {'title': 'ê²°êµ­ í„°ì§ˆ ê²Œ í„°ì¡ŒìŠµë‹ˆë‹¤... ì¶©ê²©ì  í˜„ì‹¤', 'problem': 'ë‚šì‹œ ìê·¹ â€” êµ¬ì²´ì„± ì œë¡œ, ì‹ ë¢° íŒŒê´´', 'fix': 'ë¶€ì‚° ì§€ë°©ì„ ê±°, 10ë§Œ ì²­ë…„ì´ ë– ë‚œ ë„ì‹œì˜ ë°˜ë€'},
            {'title': 'ë¶€ì‚° ì§€ë°©ì„ ê±°, ì´ì¬ëª… 2í˜¸ ì´ì¬ì„± ì›ì¹™ ë‚´ê±´ ê·¸ì˜ ì„ íƒì€', 'problem': 'ê¸°ê³„ì  ëª¨ë°© â€” ìš”ì†Œ ê³¼ë°€(5ê°œ) + í˜•ì‹ì  ë¯¸ì™„ê²° ê¼¬ë¦¬', 'fix': 'ë¶€ì‚° ì§€ë°©ì„ ê±°, ì´ì¬ì„±ì€ ì™œ ë‹¤ë¥¸ê°€'}
        ]
    },
    'DATA_BASED': {
        'id': 'DATA_BASED',
        'name': 'ğŸ“Š êµ¬ì²´ì  ë°ì´í„° ê¸°ë°˜ (ì„±ê³¼ ë³´ê³ )',
        'when': 'ì •ì±… ì™„ë£Œ, ì˜ˆì‚° í™•ë³´, ì‚¬ì—… ì™„ê³µ ë“± êµ¬ì²´ì  ì„±ê³¼ê°€ ìˆì„ ë•Œ',
        'pattern': 'ìˆ«ì 2ê°œ ì´ìƒ + í•µì‹¬ í‚¤ì›Œë“œ',
        'naverTip': '"ì–µ ì›", "ëª…", "%" ë“± êµ¬ì²´ì  ë‹¨ìœ„ê°€ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ìƒìŠ¹',
        'good': [
            {'title': 'ì²­ë…„ ì¼ìë¦¬ 274ëª… ì°½ì¶œ, ì§€ì›ê¸ˆ 85ì–µ ë‹¬ì„±', 'chars': 22, 'analysis': 'ìˆ«ì 2ê°œ + ì„±ê³¼'},
            {'title': 'ì£¼íƒ 234ê°€êµ¬ ë¦¬ëª¨ë¸ë§ ì§€ì› ì™„ë£Œ', 'chars': 16, 'analysis': 'ìˆ˜ëŸ‰ + ì™„ê²°'},
            {'title': 'ë…¸í›„ ì‚°ì—…ë‹¨ì§€ ì¬ìƒ, êµ­ë¹„ 120ì–µ í™•ë³´', 'chars': 19, 'analysis': 'ì‚¬ì—… + ê¸ˆì•¡'},
            {'title': 'êµí†µ ì‹ í˜¸ë“± 15ê³³ ê°œì„ , ì‚¬ê³ ìœ¨ 40% ê°ì†Œ', 'chars': 21, 'analysis': 'ì‹œì„¤ + íš¨ê³¼'},
            {'title': '2025ë…„ ìƒë°˜ê¸° ë¯¼ì› ì²˜ë¦¬ 3ì¼ ì´ë‚´ ë‹¬ì„±', 'chars': 20, 'analysis': 'ê¸°ê°„ + ê¸°ì¤€'}
        ],
        'bad': [
            {'title': 'ì¢‹ì€ ì„±ê³¼ ê±°ë’€ìŠµë‹ˆë‹¤', 'problem': 'êµ¬ì²´ì  ì •ë³´ ì „ë¬´', 'fix': 'ì£¼íƒ 234ê°€êµ¬ ì§€ì› ì™„ë£Œ'},
            {'title': 'ìµœì„ ì„ ë‹¤í–ˆìŠµë‹ˆë‹¤', 'problem': 'ì„±ê³¼ ë¯¸ì œì‹œ', 'fix': 'ë¯¼ì› 3ì¼ ì´ë‚´ ì²˜ë¦¬ìœ¨ 95%'},
            {'title': 'ì˜ˆì‚° ë§ì´ í™•ë³´í–ˆì–´ìš”', 'problem': '"ë§ì´"ê°€ ëª¨í˜¸', 'fix': 'êµ­ë¹„ 120ì–µ í™•ë³´'}
        ]
    },
    'QUESTION_ANSWER': {
        'id': 'QUESTION_ANSWER',
        'name': 'â“ ì§ˆë¬¸-í•´ë‹µ êµ¬ì¡° (AEO ìµœì í™”)',
        'when': 'ì£¼ë¯¼ì´ ì‹¤ì œë¡œ ê²€ìƒ‰í•˜ëŠ” ì§ˆë¬¸ì— ë‹µí•  ë•Œ (ì •ë³´ì„±)',
        'pattern': '"ì–´ë–»ê²Œ", "ë¬´ì—‡ì„", "ì™œ", "ì–¼ë§ˆ" + ì§ˆë¬¸í˜•',
        'naverTip': 'ì§ˆë¬¸í˜•ìœ¼ë¡œ ì‹œì‘í•˜ë©´ ê²€ìƒ‰ ì‚¬ìš©ìì˜ í´ë¦­ ìœ ë„',
        'good': [
            {'title': 'ë¶„ë‹¹êµ¬ ì²­ë…„ ì£¼ê±°, ì›”ì„¸ ì§€ì› ì–¼ë§ˆê¹Œì§€?', 'chars': 19, 'analysis': 'ì§€ì—­ + í˜œíƒ + ì§ˆë¬¸'},
            {'title': 'ì„±ë‚¨ êµí†µ ì²´ì¦, ì–´ë–»ê²Œ í’€ê¹Œ?', 'chars': 14, 'analysis': 'ë¬¸ì œ + í•´ê²°ì±… ì§ˆë¬¸'},
            {'title': 'ì–´ë¥´ì‹  ì¼ìë¦¬, ì–´ë–¤ í”„ë¡œê·¸ë¨ì´ ìˆë‚˜?', 'chars': 19, 'analysis': 'ëŒ€ìƒ + ì •ë³´ ì§ˆë¬¸'},
            {'title': '2025ë…„ ë³´ìœ¡ë£Œ, ì§€ì› ê¸°ì¤€ ë°”ë€Œì—ˆì–´ìš”?', 'chars': 20, 'analysis': 'ì‹œê¸° + ë³€ê²½ í™•ì¸'},
            {'title': 'ì£¼ë¯¼ ë¯¼ì›, ì‹¤ì œë¡œ ì–¸ì œ í•´ê²°ë¼ìš”?', 'chars': 17, 'analysis': 'í˜„ì‹¤ì  ì§ˆë¬¸'}
        ],
        'bad': [
            {'title': 'ì •ì±…ì— ëŒ€í•´ ì„¤ëª…ë“œë¦½ë‹ˆë‹¤', 'problem': 'ì§€ë£¨í•œ ì„œìˆ í˜•', 'fix': 'ì²­ë…„ ì§€ì› ì •ì±…, ë¬´ì—‡ì´ ë‹¬ë¼ì¡Œë‚˜?'},
            {'title': 'ê¶ê¸ˆí•œ ì ì„ í•´ê²°í•´ ë“œë¦½ë‹ˆë‹¤', 'problem': 'ë„ˆë¬´ ë²”ìš©ì ', 'fix': 'ì•„ì´ êµìœ¡ë¹„, ì§€ì› ê¸ˆì•¡ ì–¼ë§ˆë‚˜?'}
        ]
    },
    'COMPARISON': {
        'id': 'COMPARISON',
        'name': 'ğŸ†š ë¹„êµÂ·ëŒ€ì¡° êµ¬ì¡° (ì„±ê³¼ ì¦ëª…)',
        'when': 'ì •ì±…ì˜ ë³€í™”, ê°œì„ , í•´ê²°ì„ ê°•ì¡°í•  ë•Œ',
        'pattern': 'ì „í›„ ëŒ€ë¹„ ìˆ˜ì¹˜ + "â†’", "vs", "ëŒ€ë¹„"',
        'naverTip': '"â†’", "ë‹¬ë¼ì¡Œë‹¤", "ê°œì„ " ë“±ì´ ëª…í™•í•œ ê°€ì¹˜ ì „ë‹¬',
        'good': [
            {'title': 'ë¯¼ì› ì²˜ë¦¬ 14ì¼ â†’ 3ì¼, 5ë°° ë¹¨ë¼ì¡Œì–´ìš”', 'chars': 21, 'analysis': 'Before/After í™•ì‹¤'},
            {'title': 'ì²­ë…„ ê¸°ë³¸ì†Œë“ ì›” 30ë§Œ â†’ 50ë§Œì› í™•ëŒ€', 'chars': 20, 'analysis': 'ìˆ˜ì¹˜ ì¦ëŒ€ ê°•ì¡°'},
            {'title': 'êµí†µ ì‚¬ê³ ìœ¨, ì „ë…„ ëŒ€ë¹„ 40% ê°ì†Œ', 'chars': 17, 'analysis': 'ê°ì†Œ íš¨ê³¼ ë°ì´í„°'},
            {'title': 'ì“°ë ˆê¸° ë¹„ìš© 99ì–µ â†’ 65ì–µ, ì ˆê° ì‹¤í˜„', 'chars': 20, 'analysis': 'ì˜ˆì‚° ì ˆê° ì¦ëª…'},
            {'title': 'ì£¼ì°¨ì¥ ë¶€ì¡± ì§€ì—­, 12ê°œì›” ë§Œì— í•´ê²°', 'chars': 19, 'analysis': 'ê¸°ê°„ ë‹¨ì¶• ê°•ì¡°'}
        ],
        'bad': [
            {'title': 'ì´ì „ë³´ë‹¤ ë‚˜ì•„ì¡Œì–´ìš”', 'problem': 'ì–¼ë§ˆë‚˜?', 'fix': 'ë¯¼ì› ì²˜ë¦¬ 14ì¼â†’3ì¼ ê°œì„ '},
            {'title': 'ë§ì´ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤', 'problem': 'ì¶”ìƒì ', 'fix': 'êµí†µ ì‚¬ê³ ìœ¨ 40% ê°ì†Œ'}
        ]
    },
    'LOCAL_FOCUSED': {
        'id': 'LOCAL_FOCUSED',
        'name': 'ğŸ“ ì§€ì—­ ë§ì¶¤í˜• ì •ë³´ (ì´ˆì§€ì—­í™”)',
        'when': 'íŠ¹ì • ë™Â·ë©´Â·ìì˜ ì£¼ë¯¼ì„ íƒ€ê²Ÿí•  ë•Œ',
        'pattern': 'í–‰ì •êµ¬ì—­ëª…(ë™ ë‹¨ìœ„) + ì •ì±… + ìˆ«ì',
        'naverTip': 'ë™ë‹¨ìœ„ í‚¤ì›Œë“œëŠ” ê²½ìŸë„ ë‚®ì•„ ìƒìœ„ë…¸ì¶œ ìœ ë¦¬',
        'good': [
            {'title': 'ë¶„ë‹¹êµ¬ ì •ìë™ ë„ì‹œê°€ìŠ¤, ê¸°ê¸ˆ 70ì–µ í™•ë³´', 'chars': 21, 'analysis': 'êµ¬/ë™ + êµ¬ì²´ì  ì˜ˆì‚°'},
            {'title': 'ìˆ˜ì§€êµ¬ í’ë•ì²œë™ í•™êµ ì‹ ì„¤, ì˜¬ 9ì›” ê°œêµ', 'chars': 21, 'analysis': 'ì§€ì—­ + ì‹œì„¤ + ì‹œê¸°'},
            {'title': 'ì„±ë‚¨ì‹œ ì¤‘ì›êµ¬ ë³´ìœ¡ë£Œ ì§€ì›, ì›” 15ë§Œì› ì¶”ê°€', 'chars': 22, 'analysis': 'ì§€ì—­ + í˜œíƒ êµ¬ì²´í™”'},
            {'title': 'ìš©ì¸ì‹œ ê¸°í¥êµ¬ ì–´ë¥´ì‹  ìš”ì–‘ì›, ì‹ ì²­ ë§ˆê° 1ì£¼', 'chars': 23, 'analysis': 'ì§€ì—­ + ê¸´ê¸‰ì„±'},
            {'title': 'ì˜í†µêµ¬ ê´‘êµë™ êµí†µ í˜¼ì¡ë„, 6ê°œì›”ê°„ 35% ê°œì„ ', 'chars': 24, 'analysis': 'ì§€ì—­ + ê°œì„  ìˆ˜ì¹˜'}
        ],
        'bad': [
            {'title': 'ìš°ë¦¬ ì§€ì—­ì„ ìœ„í•´ ë…¸ë ¥í•©ë‹ˆë‹¤', 'problem': 'ì–´ë””?', 'fix': 'ë¶„ë‹¹êµ¬ ì •ìë™ ë„ì‹œê°€ìŠ¤ ê¸°ê¸ˆ 70ì–µ'},
            {'title': 'ì§€ì—­ í˜„ì•ˆ í•´ê²°í•˜ê² ìŠµë‹ˆë‹¤', 'problem': 'ë¬´ì—‡ì„?', 'fix': 'ìš©ì¸ì‹œ ê¸°í¥êµ¬ ì–´ë¦°ì´ì§‘ 5ê³³ ì‹ ì¶•'}
        ]
    },
    'EXPERT_KNOWLEDGE': {
        'id': 'EXPERT_KNOWLEDGE',
        'name': 'ğŸ“ ì „ë¬¸ ì§€ì‹ ê³µìœ  (ë²•ì•ˆÂ·ì¡°ë¡€)',
        'when': 'ë²•ì•ˆ ë°œì˜, ì¡°ë¡€ ì œì •, ì •ì±… ë¶„ì„ ê¸€ì„ ì“¸ ë•Œ',
        'pattern': '"ë²•ì•ˆ", "ì¡°ë¡€", "ì œë„" + í•µì‹¬ ë‚´ìš©',
        'naverTip': 'ì „ë¬¸ ìš©ì–´ë¡œ E-E-A-T(ì „ë¬¸ì„±) ê°•ì¡°',
        'good': [
            {'title': 'ì²­ë…„ ê¸°ë³¸ì†Œë“ë²• ë°œì˜, ì›” 50ë§Œì› ì§€ì›ì•ˆ', 'chars': 21, 'analysis': 'ë²•ì•ˆëª… + í˜œíƒ'},
            {'title': 'ì£¼ì°¨ì¥ ì„¤ì¹˜ ì˜ë¬´ ì¡°ë¡€ ê°œì • ì¶”ì§„', 'chars': 16, 'analysis': 'ì¡°ë¡€ëª… + í–‰ìœ„'},
            {'title': 'ì „ì„¸ ì‚¬ê¸° í”¼í•´ì ë³´í˜¸ë²•, í•µì‹¬ 3ê°€ì§€', 'chars': 19, 'analysis': 'ë²•ì•ˆ + ìš”ì•½ ì •ë³´'},
            {'title': 'ì•¼ê°„ ìƒì  CCTV ì˜ë¬´í™” ì¡°ë¡€ì•ˆ í†µê³¼', 'chars': 19, 'analysis': 'ì¡°ë¡€ + ê²°ê³¼'},
            {'title': 'ìì˜ì—…ì ì‹ ìš©ëŒ€ì¶œ, ê¸ˆë¦¬ ì¸í•˜ ì •ì±… ì¶”ì§„', 'chars': 20, 'analysis': 'ëŒ€ìƒ + ì •ì±… í˜œíƒ'}
        ],
        'bad': [
            {'title': 'ë²•ì•ˆì„ ë°œì˜í–ˆìŠµë‹ˆë‹¤', 'problem': 'ë¬´ìŠ¨ ë²•ì•ˆ?', 'fix': 'ì²­ë…„ ê¸°ë³¸ì†Œë“ë²• ë°œì˜, ì›” 50ë§Œì›'},
            {'title': 'ì¢‹ì€ ì •ì±…ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤', 'problem': 'ì¶”ìƒì ', 'fix': 'ìì˜ì—…ì ì‹ ìš©ëŒ€ì¶œ ê¸ˆë¦¬ ì¸í•˜ ì¶”ì§„'}
        ]
    },
    'TIME_BASED': {
        'id': 'TIME_BASED',
        'name': 'ğŸ“… ì‹œê°„ ì¤‘ì‹¬ ì‹ ë¢°ì„± (ì •ê¸° ë³´ê³ )',
        'when': 'ì›”ê°„ ë³´ê³ ì„œ, ë¶„ê¸° ë¦¬í¬íŠ¸, ì—°ê°„ ì„±ê³¼ ì •ë¦¬ ì‹œ',
        'pattern': '"2025ë…„", "ìƒë°˜ê¸°", "ì›”ê°„" + ì„±ê³¼ ë‚´ìš©',
        'naverTip': 'ìµœì‹ ì„±ì„ ê°•ì¡°í•˜ì—¬ ê²€ìƒ‰ í´ë¦­ ìœ ë„',
        'good': [
            {'title': '2025ë…„ ìƒë°˜ê¸° ì˜ì • ë³´ê³ ì„œ, 5ëŒ€ ì„±ê³¼', 'chars': 20, 'analysis': 'ì‹œì  + ìˆ«ì'},
            {'title': '6ì›” ë¯¼ì› ì²˜ë¦¬ ë¦¬í¬íŠ¸, 1,234ê±´ í•´ê²°', 'chars': 20, 'analysis': 'ì›” + êµ¬ì²´ì  ê±´ìˆ˜'},
            {'title': '2025ë…„ 1ë¶„ê¸° ì˜ˆì‚° ì§‘í–‰ í˜„í™© ê³µê°œ', 'chars': 19, 'analysis': 'ë¶„ê¸° + íˆ¬ëª…ì„±'},
            {'title': 'ìƒë°˜ê¸° ì£¼ë¯¼ ì˜ê²¬ ë¶„ì„, 88ê±´ ë°˜ì˜ ì¶”ì§„', 'chars': 21, 'analysis': 'ê¸°ê°„ + ë°˜ì˜ ê±´ìˆ˜'},
            {'title': 'ì›”ê°„ ì˜ì • ë‰´ìŠ¤ë ˆí„° (7ì›”í˜¸) ë°°í¬', 'chars': 17, 'analysis': 'ì •ê¸° ê°„í–‰ë¬¼'}
        ],
        'bad': [
            {'title': 'ë³´ê³ ì„œë¥¼ ì˜¬ë¦½ë‹ˆë‹¤', 'problem': 'ì‹œê°„ ë¯¸ëª…ì‹œ', 'fix': '2025ë…„ ìƒë°˜ê¸° ì˜ì • ë³´ê³ ì„œ, 5ëŒ€ ì„±ê³¼'},
            {'title': 'ìµœê·¼ í™œë™ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤', 'problem': 'ëª¨í˜¸í•¨', 'fix': '6ì›” ë¯¼ì› ì²˜ë¦¬ ë¦¬í¬íŠ¸, 1,234ê±´ í•´ê²°'}
        ]
    },
    'ISSUE_ANALYSIS': {
        'id': 'ISSUE_ANALYSIS',
        'name': 'âš–ï¸ ì •ê³„ ì´ìŠˆÂ·ë¶„ì„ (êµ­ê°€ ì •ì±…)',
        'when': 'ì •ê³„ ì´ìŠˆ, êµ­ê°€ ì •ì±… ë¶„ì„, ì œë„ ê°œí˜ ë…¼ì˜ ì‹œ',
        'pattern': 'ì´ìŠˆëª… + ì§ˆë¬¸í˜• ë˜ëŠ” ëŒ€ì•ˆ ì œì‹œ',
        'naverTip': 'ì§ˆë¬¸í˜•(?)ìœ¼ë¡œ í˜¸ê¸°ì‹¬ ìê·¹',
        'good': [
            {'title': 'ì§€ë°© ë¶„ê¶Œ ê°œí˜, ì‹¤ì œë¡œ ë­ê°€ ë‹¬ë¼ì§ˆê¹Œ?', 'chars': 19, 'analysis': 'ì´ìŠˆ + ê¶ê¸ˆì¦'},
            {'title': 'ì •ì¹˜ ìê¸ˆ íˆ¬ëª…ì„±, ì–´ë–»ê²Œ ê°œì„ í• ê¹Œ?', 'chars': 18, 'analysis': 'ì´ìŠˆ + í•´ê²°ì±… ì§ˆë¬¸'},
            {'title': 'ì–‘ê·¹í™” ë¬¸ì œ, 4ëŒ€ ëŒ€ì•ˆ ì œì‹œ', 'chars': 14, 'analysis': 'ë¬¸ì œ + ëŒ€ì•ˆ ê°œìˆ˜'},
            {'title': 'êµìœ¡ ê²©ì°¨, ì¬ì • íˆ¬ìë¡œ ë­ê°€ ë‹¬ë¼ì§ˆê¹Œ?', 'chars': 19, 'analysis': 'ìˆ˜ë‹¨ + íš¨ê³¼ ì§ˆë¬¸'},
            {'title': 'ì„ ê±° ì œë„ ê°œí˜, ì™œ ì‹œê¸‰í•œê°€?', 'chars': 15, 'analysis': 'ì´ìŠˆ + ë‹¹ìœ„ì„±'}
        ],
        'bad': [
            {'title': 'ì •ì¹˜ í˜„ì‹¤ì— ëŒ€í•´ ìƒê°í•´ ë´…ì‹œë‹¤', 'problem': 'ë„ˆë¬´ ì² í•™ì ', 'fix': 'ì§€ë°© ë¶„ê¶Œ ê°œí˜, ì‹¤ì œë¡œ ë­ê°€ ë‹¬ë¼ì§ˆê¹Œ?'},
            {'title': 'ë¬¸ì œê°€ ë§ìŠµë‹ˆë‹¤', 'problem': 'ë¶ˆë§Œ í† ë¡œ', 'fix': 'ì–‘ê·¹í™” ë¬¸ì œ, 4ëŒ€ ëŒ€ì•ˆ ì œì‹œ'}
        ]
    },
    'COMMENTARY': {
         'id': 'COMMENTARY',
         'name': 'ğŸ’¬ ë…¼í‰/í™”ì ê´€ì ',
         'when': 'ë‹¤ë¥¸ ì •ì¹˜ì¸ ë…¼í‰, ì¸ë¬¼ í‰ê°€, ì •ì¹˜ì  ì…ì¥ í‘œëª… ì‹œ',
         'pattern': 'í™”ì + ê´€ì  í‘œí˜„ + ëŒ€ìƒ/ì´ìŠˆ',
         'naverTip': 'í™”ì ì´ë¦„ì„ ì•ì— ë°°ì¹˜í•˜ë©´ ê°œì¸ ë¸Œëœë”© + SEO íš¨ê³¼',
         'good': [
             {'title': 'ì´ì¬ì„±, ë°•í˜•ì¤€ ì‹œì¥ 0.7% ì„±ì¥ë¥  ì§ˆíƒ€', 'chars': 19, 'analysis': 'í™”ì + ëŒ€ìƒ + ë¹„íŒ'},
             {'title': 'ì¡°ê²½íƒœ ì¹­ì°¬í•œ ì´ì¬ì„±, å°¹ ì‚¬í˜• ë…¼í‰', 'chars': 18, 'analysis': 'ê´€ê³„ + í™”ì + ì´ìŠˆ'},
             {'title': 'ì´ì¬ì„± "ë¶€ì‚° AI ì˜ˆì‚° ì „ì•¡ ì‚­ê° ì¶©ê²©"', 'chars': 19, 'analysis': 'í™”ì + ì¸ìš© + ê°ì •'},
             {'title': 'ë°•í˜•ì¤€ ì‹œì¥ ë°œì–¸ì— ëŒ€í•œ ì´ì¬ì„± ë°˜ë°•', 'chars': 18, 'analysis': 'ëŒ€ìƒ + ì´ìŠˆ + ë°˜ì‘'},
             {'title': 'ì´ì¬ì„± "ë°•í˜•ì¤€, ê²½ì œ ì„±ì  ë‚™ì œì "', 'chars': 18, 'analysis': 'í™”ì + ì¸ìš©'}
         ],
         'bad': [
             {'title': 'ì‹œì¥ì˜ ë°œì–¸ì— ëŒ€í•´', 'problem': 'ëˆ„êµ¬? ë‚´ìš©?', 'fix': 'ì´ì¬ì„±, ë°•í˜•ì¤€ ì‹œì¥ ë°œì–¸ ë°˜ë°•'},
             {'title': 'ì˜¤ëŠ˜ì˜ ë…¼í‰ì…ë‹ˆë‹¤', 'problem': 'ì •ë³´ ì—†ìŒ', 'fix': 'ì´ì¬ì„± "ë¶€ì‚° ì˜ˆì‚° ì‚­ê° ìœ ê°"'}
         ]
     }
}

def detect_content_type(content_preview: str, category: str) -> str:
    try:
        text = content_preview.lower()
        
        has_numbers = re.search(r'\d+ì–µ|\d+ë§Œì›|\d+%|\d+ëª…|\d+ê±´|\d+ê°€êµ¬|\d+ê³³', content_preview)
        has_comparison = re.search(r'â†’|ì—ì„œ|ìœ¼ë¡œ|ì „ë…„|ëŒ€ë¹„|ê°œì„ |ê°ì†Œ|ì¦ê°€|ë³€í™”', text)
        has_question = re.search(r'\?|ì–´ë–»ê²Œ|ë¬´ì—‡|ì™œ|ì–¼ë§ˆ|ì–¸ì œ', text)
        has_legal_terms = re.search(r'ë²•ì•ˆ|ì¡°ë¡€|ë²•ë¥ |ì œë„|ê°œì •|ë°œì˜|í†µê³¼', text)
        has_time_terms = re.search(r'2025ë…„|ìƒë°˜ê¸°|í•˜ë°˜ê¸°|ë¶„ê¸°|ì›”ê°„|ì—°ê°„|ë³´ê³ ì„œ|ë¦¬í¬íŠ¸', text)
        has_local_terms = re.search(r'[ê°€-í£]+(ë™|êµ¬|êµ°|ì‹œ|ì|ë©´|ë¦¬)(?:[ê°€-í£]|\s|,|$)', content_preview)
        has_issue_terms = re.search(r'ê°œí˜|ë¶„ê¶Œ|ì–‘ê·¹í™”|ê²©ì°¨|íˆ¬ëª…ì„±|ë¬¸ì œì |ëŒ€ì•ˆ', text)
        has_commentary_terms = re.search(r'ì¹­ì°¬|ì§ˆíƒ€|ë¹„íŒ|ë…¼í‰|í‰ê°€|ì†Œì‹ |ì¹¨ë¬µ|ì—­ë¶€ì¡±|ë‚™ì œ|ì‹¬íŒ', text)
        has_politician_names = re.search(r'ë°•í˜•ì¤€|ì¡°ê²½íƒœ|ìœ¤ì„ì—´|ì´ì¬ëª…|í•œë™í›ˆ', content_preview)
        
        # Priority for user content signals
        if has_time_terms and ('ë³´ê³ ' in text or 'ë¦¬í¬íŠ¸' in text or 'í˜„í™©' in text):
            return 'TIME_BASED'
        if has_legal_terms:
            return 'EXPERT_KNOWLEDGE'
        if has_commentary_terms and has_politician_names:
            return 'COMMENTARY'
        if has_comparison and has_numbers:
            return 'COMPARISON'
        if has_question:
            return 'QUESTION_ANSWER'
        if has_numbers and not has_issue_terms:
            return 'DATA_BASED'
        if has_issue_terms and not has_local_terms:
            return 'ISSUE_ANALYSIS'
        if has_local_terms:
            return 'LOCAL_FOCUSED'
        
        category_mapping = {
            'activity-report': 'DATA_BASED',
            'policy-proposal': 'EXPERT_KNOWLEDGE',
            'local-issues': 'LOCAL_FOCUSED',
            'current-affairs': 'ISSUE_ANALYSIS',
            'daily-communication': 'VIRAL_HOOK', # Changed to VIRAL_HOOK for daily coms
            'bipartisan-cooperation': 'COMMENTARY'
        }
        
        return category_mapping.get(category, 'VIRAL_HOOK') # Default to VIRAL_HOOK
    except Exception as e:
        logger.error(f'Error in detect_content_type: {e}')
        return 'VIRAL_HOOK'

def extract_numbers_from_content(content: str) -> Dict[str, Any]:
    if not content:
        return {'numbers': [], 'instruction': ''}
        
    try:
        patterns = [
            r'\d+(?:,\d{3})*ì–µì›?',
            r'\d+(?:,\d{3})*ë§Œì›?',
            r'\d+(?:\.\d+)?%',
            r'\d+(?:,\d{3})*ëª…',
            r'\d+(?:,\d{3})*ê±´',
            r'\d+(?:,\d{3})*ê°€êµ¬',
            r'\d+(?:,\d{3})*ê³³',
            r'\d+(?:,\d{3})*ê°œ',
            r'\d+(?:,\d{3})*íšŒ',
            r'\d+ë°°',
            r'\d+(?:,\d{3})*ì›',
            r'\d+ì¼',
            r'\d+ê°œì›”',
            r'\d+ë…„',
            r'\d+ë¶„ê¸°'
        ]
        
        all_matches = set()
        for pattern in patterns:
            matches = re.findall(pattern, content)
            all_matches.update(matches)
            
        numbers = list(all_matches)
        
        if not numbers:
            return {
                'numbers': [],
                'instruction': '\\nã€ìˆ«ì ì œì•½ã€‘ë³¸ë¬¸ì— êµ¬ì²´ì  ìˆ˜ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤. ìˆ«ì ì—†ì´ ì œëª©ì„ ì‘ì„±í•˜ì„¸ìš”.\\n'
            }
            
        formatted_numbers = ', '.join(numbers[:10])
        if len(numbers) > 10:
            formatted_numbers += f' (ì™¸ {len(numbers) - 10}ê°œ)'
            
        instruction = f"""
<number_validation priority="critical">
  <description>ë³¸ë¬¸ì— ë“±ì¥í•˜ëŠ” ìˆ«ìë§Œ ì‚¬ìš© ê°€ëŠ¥</description>
  <allowed_numbers>{formatted_numbers}</allowed_numbers>
  <rule type="must-not">ìœ„ ëª©ë¡ì— ì—†ëŠ” ìˆ«ìëŠ” ì ˆëŒ€ ì œëª©ì— ë„£ì§€ ë§ˆì„¸ìš”</rule>
  <examples>
    <good>ë³¸ë¬¸ì— "274ëª…"ì´ ìˆìœ¼ë©´ "ì²­ë…„ ì¼ìë¦¬ 274ëª…"</good>
    <bad reason="ë‚ ì¡°">ë³¸ë¬¸ì— "85ì–µ"ì´ ì—†ëŠ”ë° "ì§€ì›ê¸ˆ 85ì–µ"</bad>
  </examples>
</number_validation>
"""
        return {'numbers': numbers, 'instruction': instruction}
    except Exception as e:
        logger.error(f'Error in extract_numbers_from_content: {e}')
        return {'numbers': [], 'instruction': ''}

def get_election_compliance_instruction(status: str) -> str:
    try:
        election_stage = get_election_stage(status)
        is_pre_candidate = election_stage.get('name') == 'STAGE_1'
        
        if not is_pre_candidate: return ''
        
        return f"""
<election_compliance status="{status}" stage="pre-candidate" priority="critical">
  <description>ì„ ê±°ë²• ì¤€ìˆ˜ (í˜„ì¬ ìƒíƒœ: {status} - ì˜ˆë¹„í›„ë³´ ë“±ë¡ ì´ì „)</description>
  <banned_expressions>
    <expression>"ì•½ì†", "ê³µì•½", "ì•½ì†ë“œë¦½ë‹ˆë‹¤"</expression>
    <expression>"ë‹¹ì„ ë˜ë©´", "ë‹¹ì„  í›„"</expression>
    <expression>"~í•˜ê² ìŠµë‹ˆë‹¤" (ê³µì•½ì„± ë¯¸ë˜ ì•½ì†)</expression>
    <expression>"ì§€ì§€í•´ ì£¼ì‹­ì‹œì˜¤"</expression>
  </banned_expressions>
  <allowed_expressions>
    <expression>"ì •ì±… ë°©í–¥", "ì •ì±… ì œì‹œ", "ë¹„ì „ ê³µìœ "</expression>
    <expression>"ì—°êµ¬í•˜ê² ìŠµë‹ˆë‹¤", "ë…¸ë ¥í•˜ê² ìŠµë‹ˆë‹¤"</expression>
    <expression>"ì¶”ì§„", "ì¶”êµ¬", "ê²€í† "</expression>
  </allowed_expressions>
  <examples>
    <bad>"ì²­ë…„ ê¸°ë³¸ì†Œë“, ê¼­ ì•½ì†ë“œë¦¬ê² ìŠµë‹ˆë‹¤"</bad>
    <good>"ì²­ë…„ ê¸°ë³¸ì†Œë“, ì •ì±… ë°©í–¥ ì œì‹œ"</good>
  </examples>
</election_compliance>
"""
    except Exception as e:
        logger.error(f'Error in get_election_compliance_instruction: {e}')
        return ''

def are_keywords_similar(kw1: str, kw2: str) -> bool:
    """
    ë‘ í‚¤ì›Œë“œê°€ ìœ ì‚¬í•œì§€ íŒë³„ (ê³µí†µ ì–´ì ˆì´ ìˆëŠ”ì§€)
    ì˜ˆ: "ì„œë©´ ì˜ê´‘ë„ì„œ", "ë¶€ì‚° ì˜ê´‘ë„ì„œ" â†’ ê³µí†µ "ì˜ê´‘ë„ì„œ" â†’ ìœ ì‚¬
    ì˜ˆ: "ê³„ì–‘ì‚° ëŸ¬ë¸Œë²„ê·¸ ë°©ì—­", "ê³„ì–‘êµ¬ì²­" â†’ ê³µí†µ ì—†ìŒ â†’ ë…ë¦½
    """
    if not kw1 or not kw2:
        return False
    words1 = kw1.split()
    words2 = kw2.split()
    return any(w in words2 and len(w) >= 2 for w in words1)

def get_keyword_strategy_instruction(user_keywords: List[str], keywords: List[str]) -> str:
    try:
        has_user_keywords = bool(user_keywords)
        primary_kw = user_keywords[0] if has_user_keywords else (keywords[0] if keywords else '')
        secondary_kw = (user_keywords[1] if len(user_keywords) > 1 else (keywords[0] if keywords else '')) if has_user_keywords else (keywords[1] if len(keywords) > 1 else '')
        if primary_kw and secondary_kw:
            primary_compact = re.sub(r'\s+', '', primary_kw)
            secondary_compact = re.sub(r'\s+', '', secondary_kw)
            if (
                primary_compact
                and secondary_compact
                and primary_compact in secondary_compact
                and len(secondary_compact) > len(primary_compact)
            ):
                primary_kw, secondary_kw = secondary_kw, primary_kw

        # ë‘ í‚¤ì›Œë“œ ê°„ ìœ ì‚¬/ë…ë¦½ íŒë³„
        has_two_keywords = bool(primary_kw and secondary_kw and primary_kw != secondary_kw)
        similar = has_two_keywords and are_keywords_similar(primary_kw, secondary_kw)

        title_keyword_rule = ""
        if has_two_keywords:
            if similar:
                kw2_words = secondary_kw.split()
                kw1_words = primary_kw.split()
                unique_words = [w for w in kw2_words if w not in kw1_words]
                unique_hint = f'"{", ".join(unique_words)}"ë¥¼ ì œëª© ë’¤ìª½ì— ë…¹ì—¬ë„£ê¸°' if unique_words else 'ê³µí†µ ì–´ì ˆë¡œ ìë™ ì¶©ì¡±'
                title_keyword_rule = f"""
<keyword_placement type="similar">
  <description>ë‘ ê²€ìƒ‰ì–´("{primary_kw}", "{secondary_kw}")ê°€ ê³µí†µ ì–´ì ˆì„ ê³µìœ </description>
  <rule type="must">ì œëª©ì€ ë°˜ë“œì‹œ "{primary_kw}"ë¡œ ì‹œì‘</rule>
  <rule type="must">"{secondary_kw}"ëŠ” ì–´ì ˆ ë‹¨ìœ„ë¡œ í•´ì²´í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë°°ì¹˜ ({unique_hint})</rule>
  <example>"ì„œë©´ ì˜ê´‘ë„ì„œ, &lt;ë³´ê³ ìˆë‚˜, ë¶€ì‚°&gt; ì¶œíŒê¸°ë…íšŒì— ì´ˆëŒ€í•©ë‹ˆë‹¤"</example>
  <example>"ë¶€ì‚° ëŒ€í˜•ë³‘ì›, ì•”ì„¼í„° í™•ì¶©ìœ¼ë¡œ ê³ ë ¹ í™˜ìë„ ì•ˆì‹¬"</example>
</keyword_placement>
"""
            else:
                title_keyword_rule = f"""
<keyword_placement type="independent">
  <description>ë‘ ê²€ìƒ‰ì–´("{primary_kw}", "{secondary_kw}")ê°€ ë…ë¦½ì </description>
  <rule type="must">ì œëª©ì€ ë°˜ë“œì‹œ "{primary_kw}"ë¡œ ì‹œì‘</rule>
  <rule type="must">"{secondary_kw}"ëŠ” ì œëª© ë’¤ìª½ì— ìì—°ìŠ¤ëŸ½ê²Œ ë°°ì¹˜</rule>
  <example>"ê³„ì–‘ì‚° ëŸ¬ë¸Œë²„ê·¸ ë°©ì—­, ê³„ì–‘êµ¬ì²­ì— ì ê·¹ êµ¬ì œ ì´‰êµ¬"</example>
  <example>"ì„±ìˆ˜ì—­ 3ë²ˆ ì¶œêµ¬, í™•ì¥ ê³µì‚¬ ì „í˜„í¬ ë•ì´ë¼ëŠ” ì½”ë ˆì¼ ë…¸ì¡°"</example>
  <example>"ë¶€ì‚° ë””ì¦ˆë‹ˆëœë“œ ìœ ì¹˜, ì„œë¶€ì‚° ë°œì „ì˜ ì—´ì‡ ?"</example>
</keyword_placement>
"""

        kw_instructions = []
        if primary_kw:
            kw_instructions.append(f'  <keyword priority="1" value="{primary_kw}">ì œëª© ì• 8ì ì´ë‚´ ë°°ì¹˜ ê¶Œì¥ (í•„ìˆ˜ ì•„ë‹˜, ìì—°ìŠ¤ëŸ¬ì›€ ìš°ì„ )</keyword>')
        if secondary_kw:
            placement = 'ì–´ì ˆ í•´ì²´í•˜ì—¬ ìì—° ë°°ì¹˜' if similar else 'ì œëª© ë’¤ìª½ ë°°ì¹˜'
            kw_instructions.append(f'  <keyword priority="2" value="{secondary_kw}">{placement}</keyword>')
        kw_instruction_xml = '\n'.join(kw_instructions)

        return f"""
<seo_keyword_strategy>

<front_third_rule priority="highest">
  <description>ë„¤ì´ë²„ëŠ” ì œëª© ì• 8-10ìë¥¼ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ í‰ê°€í•©ë‹ˆë‹¤. í•µì‹¬ í‚¤ì›Œë“œëŠ” ì œëª© ì‹œì‘ ë¶€ë¶„ ë°°ì¹˜ë¥¼ ê¶Œì¥í•˜ë‚˜, ê°•ë ¬í•œ ì¹´í”¼(Viral Hook)ë¥¼ ìœ„í•´ ë¬¸ì¥ ì¤‘ê°„ì— ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ë„ ë©ë‹ˆë‹¤.</description>
  <examples>
    <bad>"ìš°ë¦¬ ì§€ì—­ ì²­ë…„ë“¤ì„ ìœ„í•œ ì²­ë…„ ê¸°ë³¸ì†Œë“"</bad>
    <good>"ì²­ë…„ ê¸°ë³¸ì†Œë“, ë¶„ë‹¹êµ¬ ì›” 50ë§Œì› ì§€ì›"</good>
  </examples>
</front_third_rule>

<keyword_separator priority="critical">
  <description>í‚¤ì›Œë“œ ì§í›„ì— ì‰¼í‘œ(,) ë˜ëŠ” ì¡°ì‚¬(ì—, ì˜, ì—ì„œ ë“±)ë¥¼ ë„£ì–´ ë‹¤ìŒ ë‹¨ì–´ì™€ ë¶„ë¦¬í•˜ì„¸ìš”. ë„¤ì´ë²„ëŠ” ê³µë°±ë§Œìœ¼ë¡œëŠ” í‚¤ì›Œë“œ ê²½ê³„ë¥¼ ì¸ì‹í•˜ì§€ ëª»í•©ë‹ˆë‹¤.</description>
  <examples>
    <good reason="í‚¤ì›Œë“œ=ë¶€ì‚° ì§€ë°©ì„ ê±°">"ë¶€ì‚° ì§€ë°©ì„ ê±°, ì™œ ì´ ë‚¨ìê°€"</good>
    <good reason="í‚¤ì›Œë“œ=ë¶€ì‚° ì§€ë°©ì„ ê±°">"ë¶€ì‚° ì§€ë°©ì„ ê±°ì— ë›°ì–´ë“  ë¶€ë‘ ë…¸ë™ì"</good>
    <bad reason="ì˜ëª» ì¸ì‹: ë¶€ì‚° ì§€ë°©ì„ ê±° ì´ì¬ì„± ì›ì¹™">"ë¶€ì‚° ì§€ë°©ì„ ê±° ì´ì¬ì„± ì›ì¹™"</bad>
  </examples>
</keyword_separator>
{title_keyword_rule}
<keyword_density>
  <optimal count="2">ê°€ì¥ ìì—°ìŠ¤ëŸ½ê³  íš¨ê³¼ì </optimal>
  <max count="3"/>
  <warning>4ê°œ ì´ìƒ: ìŠ¤íŒ¸ìœ¼ë¡œ íŒë‹¨, CTR ê°ì†Œ</warning>
</keyword_density>

  <position_strategy>
  <zone range="0-8ì" weight="100%" use="ì§€ì—­ëª…, ì •ì±…ëª…, í•µì‹¬ ì£¼ì œ"/>
  <zone range="9-{TITLE_LENGTH_OPTIMAL_MAX}ì" weight="80%" use="ìˆ˜ì¹˜, LSI í‚¤ì›Œë“œ"/>
  <zone range="{TITLE_LENGTH_OPTIMAL_MAX + 1}-{TITLE_LENGTH_HARD_MAX}ì" weight="60%" use="í–‰ë™ ìœ ë„, ê¸´ê¸‰ì„±"/>
</position_strategy>

<keyword_priority>
{kw_instruction_xml}
</keyword_priority>

<synonym_guide description="ë°˜ë³µ ë°©ì§€">
  <synonym from="ì§€ì›" to="ì§€ì›ê¸ˆ, ë³´ì¡°ê¸ˆ, í˜œíƒ"/>
  <synonym from="ë¬¸ì œ" to="í˜„ì•ˆ, ê³¼ì œ, ì–´ë ¤ì›€"/>
  <synonym from="í•´ê²°" to="ê°œì„ , ì™„í™”, í•´ì†Œ"/>
</synonym_guide>

</seo_keyword_strategy>
"""
    except Exception as e:
        logger.error(f'Error in get_keyword_strategy_instruction: {e}')
        return ''


def _detect_event_label(topic: str) -> str:
    for marker in EVENT_NAME_MARKERS:
        if marker in (topic or ''):
            return marker
    return 'í–‰ì‚¬'


def _build_few_shot_slot_values(params: Dict[str, Any]) -> Dict[str, str]:
    topic = str(params.get('topic') or '')
    content_preview = str(params.get('contentPreview') or '')
    full_name = str(params.get('fullName') or '').strip()
    user_keywords = params.get('userKeywords') if isinstance(params.get('userKeywords'), list) else []
    context_analysis = params.get('contextAnalysis') if isinstance(params.get('contextAnalysis'), dict) else {}
    must_preserve = context_analysis.get('mustPreserve') if isinstance(context_analysis.get('mustPreserve'), dict) else {}

    primary_kw = str(user_keywords[0]).strip() if user_keywords else ''
    location_hint = str(must_preserve.get('eventLocation') or '').strip()
    date_hint = _extract_date_hint(str(must_preserve.get('eventDate') or '')) or _extract_date_hint(topic)
    event_label = _detect_event_label(topic)
    topic_label = _extract_book_title(topic, params) or (topic[:14].strip() if topic else '')
    numbers = extract_numbers_from_content(content_preview).get('numbers', [])
    first_number = str(numbers[0]).strip() if numbers else 'ìˆ˜ì¹˜'

    return {
        'ì§€ì—­ëª…': primary_kw or location_hint or 'ì§€ì—­ëª…',
        'ì¥ì†Œëª…': location_hint or primary_kw or 'ì¥ì†Œëª…',
        'ì¸ë¬¼ëª…': full_name or 'ì¸ë¬¼ëª…',
        'í–‰ì‚¬ëª…': event_label or 'í–‰ì‚¬ëª…',
        'ë‚ ì§œ': date_hint or 'ë‚ ì§œ',
        'ì£¼ì œëª…': topic_label or 'ì£¼ì œëª…',
        'ì •ì±…ëª…': topic_label or 'ì •ì±…ëª…',
        'ì‚¬ì—…ëª…': topic_label or 'ì‚¬ì—…ëª…',
        'ìˆ˜ì¹˜': first_number,
        'ìˆ˜ëŸ‰': first_number,
        'ê¸ˆì•¡': first_number,
        'ë‹¨ìœ„': 'ëª…',
        'ì„±ê³¼ì§€í‘œ': 'ê°œì„ ',
        'ì§€ì›í•­ëª©': 'ì§€ì›',
        'í˜„ì•ˆ': topic_label or 'í˜„ì•ˆ',
        'ë¯¼ì›ì£¼ì œ': topic_label or 'ë¯¼ì› ì£¼ì œ',
        'ì´ìŠˆëª…': topic_label or 'ì´ìŠˆëª…',
        'ì •ì±…ìŸì ': topic_label or 'ì •ì±… ìŸì ',
        'ë¬¸ì œëª…': topic_label or 'ë¬¸ì œëª…',
        'ëŒ€ì•ˆìˆ˜': '3',
        'ì´ì „ê°’': 'ê¸°ì¡´ ìˆ˜ì¹˜',
        'í˜„ì¬ê°’': 'ê°œì„  ìˆ˜ì¹˜',
        'ê°œì„ í­': 'ëŒ€í­',
        'ê¸°ì¡´ì•ˆ': 'ê¸°ì¡´ì•ˆ',
        'ê°œì„ ì•ˆ': 'ê°œì„ ì•ˆ',
        'ë¹„ìš©í•­ëª©': 'ìš´ì˜ë¹„',
        'ì´ì „ê¸ˆì•¡': 'ê¸°ì¡´ ì˜ˆì‚°',
        'í˜„ì¬ê¸ˆì•¡': 'ì ˆê° ì˜ˆì‚°',
        'ê°œê´€ì‹œê¸°': 'ì˜¬í•´ í•˜ë°˜ê¸°',
        'ê¸°ê°„': '6ê°œì›”',
        'ê°œì„ ìˆ˜ì¹˜': '35',
        'ë²•ì•ˆëª…': topic_label or 'ë²•ì•ˆëª…',
        'í•µì‹¬ì§€ì›': 'ì§€ì› í™•ëŒ€',
        'ì¡°ë¡€ëª…': topic_label or 'ì¡°ë¡€ëª…',
        'í•µì‹¬ë³€ê²½': 'í•µì‹¬ ì¡°í•­',
        'ìˆ«ì': '3',
        'í•µì‹¬í˜œíƒ': 'í•µì‹¬ í˜œíƒ',
        'í•µì‹¬ë³€í™”': 'í•µì‹¬ ë³€í™”',
        'ì—°ë„/ë¶„ê¸°': '2026ë…„ ìƒë°˜ê¸°',
        'ë³´ê³ ì„œëª…': 'í™œë™ ë³´ê³ ì„œ',
        'í•µì‹¬ì„±ê³¼ìˆ˜': '5',
        'ì›”/ë¶„ê¸°': '6ì›”',
        'ì—…ë¬´ëª…': 'ë¯¼ì› ì²˜ë¦¬',
        'ê±´ìˆ˜': '1,234',
        'ì •ê¸°ë¸Œë¦¬í•‘ëª…': 'ì›”ê°„ ë¸Œë¦¬í•‘',
        'ì›”í˜¸': '7ì›”í˜¸',
        'í•µì‹¬ì£¼ì œ': topic_label or 'í•µì‹¬ ì£¼ì œ',
        'ì˜ˆì‚°í•­ëª©': 'êµ­ë¹„',
        'í˜œíƒìˆ˜ì¹˜': 'ì›” 15ë§Œì›',
        'ì„±ê³¼ìˆ˜': '5',
        'ì—…ë¬´': 'í•µì‹¬ ì—…ë¬´',
    }


def _render_slot_template(template: str, slot_values: Dict[str, str]) -> str:
    rendered = str(template or '')
    for slot_name, slot_value in slot_values.items():
        rendered = rendered.replace(f'[{slot_name}]', str(slot_value))
    return re.sub(r'\s+', ' ', rendered).strip()


def build_user_provided_few_shot_instruction(type_id: str, params: Optional[Dict[str, Any]] = None) -> str:
    resolved_type_id = str(type_id or '').strip()
    few_shot = USER_PROVIDED_TITLE_FEW_SHOT.get(resolved_type_id)
    if not few_shot:
        logger.info("[TitleGen] ì‚¬ìš©ì few-shot ë¯¸ì •ì˜ íƒ€ì…: %s", resolved_type_id)
        return ''

    slot_values = _build_few_shot_slot_values(params or {})
    slot_guide = '\n'.join([
        f'    <slot name="{k}" value="{v}" />'
        for k, v in list(slot_values.items())[:12]
    ])

    template_examples = '\n'.join([
        f'    <template pattern="{item.get("template", "")}" intent="{item.get("intent", "")}" />'
        for item in few_shot.get('templates', [])
        if isinstance(item, dict)
    ])
    rendered_examples = '\n'.join([
        f'    <example>{_render_slot_template(item.get("template", ""), slot_values)}</example>'
        for item in few_shot.get('templates', [])
        if isinstance(item, dict)
    ])
    bad_examples = '\n'.join([
        f'    <example bad="{item.get("bad", "")}" fix_template="{item.get("fix_template", "")}" />'
        for item in few_shot.get('bad_to_fix', [])
        if isinstance(item, dict)
    ])

    return f"""
<user_provided_few_shot priority="high" source="ì‚¬ìš©ì_ì •ì¹˜ì¸_7ìœ í˜•_ì „ëµ">
  <description>ì•„ë˜ ì˜ˆì‹œëŠ” ê³ ì • ì¹´í”¼ê°€ ì•„ë‹ˆë¼ ìŠ¬ë¡¯ ê¸°ë°˜ í…œí”Œë¦¿ì´ë‹¤. í˜„ì¬ ì£¼ì œ/ì§€ì—­/ì¸ë¬¼ì— ë§ê²Œ ìŠ¬ë¡¯ë§Œ ì¹˜í™˜í•´ ì‚¬ìš©í•˜ë¼.</description>
  <type requested="{type_id}" resolved="{resolved_type_id}" name="{few_shot.get('name', '')}" />
  <slot_guide>
{slot_guide}
  </slot_guide>
  <template_examples>
{template_examples}
  </template_examples>
  <rendered_examples>
{rendered_examples}
  </rendered_examples>
  <bad_to_fix_examples>
{bad_examples}
  </bad_to_fix_examples>
</user_provided_few_shot>
""".strip()


def _extract_date_hint(text: str) -> str:
    if not text:
        return ''
    month_day = re.search(r'(\d{1,2}\s*ì›”\s*\d{1,2}\s*ì¼)', text)
    if month_day:
        return re.sub(r'\s+', ' ', month_day.group(1)).strip()
    iso_like = re.search(r'(\d{4}[./-]\d{1,2}[./-]\d{1,2})', text)
    if iso_like:
        return iso_like.group(1).strip()
    return ''


def _contains_date_hint(title: str, date_hint: str) -> bool:
    if not title:
        return False
    if date_hint:
        no_space_title = re.sub(r'\s+', '', title)
        no_space_hint = re.sub(r'\s+', '', date_hint)
        if no_space_hint in no_space_title:
            return True
        month_day = re.search(r'(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼', date_hint)
        if month_day:
            m, d = month_day.group(1), month_day.group(2)
            if re.search(fr'{m}\s*ì›”\s*{d}\s*ì¼', title):
                return True
    return bool(_extract_date_hint(title))


def _normalize_digit_token(value: str) -> str:
    digits = re.sub(r'\D', '', str(value or ''))
    if not digits:
        return ''
    normalized = digits.lstrip('0')
    return normalized or '0'


def _extract_digit_tokens(text: str) -> List[str]:
    if not text:
        return []
    tokens = []
    seen = set()
    for match in re.findall(r'\d+', str(text)):
        normalized = _normalize_digit_token(match)
        if not normalized or normalized in seen:
            continue
        seen.add(normalized)
        tokens.append(normalized)
    return tokens


def _split_hint_tokens(text: str) -> List[str]:
    if not text:
        return []
    clean = re.sub(r'[\(\)\[\]\{\},]', ' ', str(text))
    tokens = [t.strip() for t in re.split(r'\s+', clean) if t.strip()]
    result: List[str] = []
    for token in tokens:
        if len(token) >= 2:
            result.append(token)
    return result


BOOK_TITLE_QUOTE_PATTERNS = (
    ('angle', re.compile(r'<\s*([^<>]{2,80}?)\s*>')),
    ('double_angle', re.compile(r'ã€Š\s*([^ã€Šã€‹]{2,80}?)\s*ã€‹')),
    ('single_angle', re.compile(r'ã€ˆ\s*([^ã€ˆã€‰]{2,80}?)\s*ã€‰')),
    ('double_quote', re.compile(r'"\s*([^"\n]{2,80}?)\s*"')),
    ('single_quote', re.compile(r"'\s*([^'\n]{2,80}?)\s*'")),
    ('curly_double_quote', re.compile(r'â€œ\s*([^â€\n]{2,80}?)\s*â€')),
    ('curly_single_quote', re.compile(r'â€˜\s*([^â€™\n]{2,80}?)\s*â€™')),
    ('corner_quote', re.compile(r'ã€Œ\s*([^ã€Œã€]{2,80}?)\s*ã€')),
    ('white_corner_quote', re.compile(r'ã€\s*([^ã€ã€]{2,80}?)\s*ã€')),
)
BOOK_TITLE_WRAPPER_PAIRS = (
    ('<', '>'),
    ('ã€Š', 'ã€‹'),
    ('ã€ˆ', 'ã€‰'),
    ('ã€Œ', 'ã€'),
    ('ã€', 'ã€'),
    ('"', '"'),
    ("'", "'"),
    ('â€œ', 'â€'),
    ('â€˜', 'â€™'),
)
BOOK_TITLE_CONTEXT_MARKERS = (
    'ì±…',
    'ì €ì„œ',
    'ë„ì„œ',
    'ì‹ ê°„',
    'ì¶œê°„',
    'ì¶œíŒ',
    'ë¶í† í¬',
    'í† í¬ì½˜ì„œíŠ¸',
    'ì¶œíŒí–‰ì‚¬',
    'ì¶œíŒê¸°ë…íšŒ',
    'ì œëª©',
)
BOOK_TITLE_EVENT_MARKERS = (
    'ì¶œíŒê¸°ë…íšŒ',
    'ë¶í† í¬',
    'í† í¬ì½˜ì„œíŠ¸',
    'ì¶œíŒí–‰ì‚¬',
    'ì¶œê°„ê¸°ë…',
)
BOOK_TITLE_DISALLOWED_TOKENS = (
    'ì¶œíŒê¸°ë…íšŒ',
    'ë¶í† í¬',
    'í† í¬ì½˜ì„œíŠ¸',
    'í–‰ì‚¬',
    'ì´ˆëŒ€',
    'ì•ˆë‚´',
    'ê°œìµœ',
)
BOOK_TITLE_LOCATION_HINTS = (
    'ë„ì„œ',
    'ì„¼í„°',
    'í™€',
    'ê´‘ì¥',
    'ì‹œì²­',
    'êµ¬ì²­',
)
BOOK_TITLE_LOCATION_SUFFIXES = (
    'ë„ì„œ',
    'ì„¼í„°',
    'í™€',
    'ê´‘ì¥',
    'ì‹œì²­',
    'êµ¬ì²­',
)


def _normalize_book_title_candidate(text: str) -> str:
    normalized = str(text or '').strip()
    if not normalized:
        return ''

    while True:
        changed = False
        for left, right in BOOK_TITLE_WRAPPER_PAIRS:
            if normalized.startswith(left) and normalized.endswith(right) and len(normalized) > len(left) + len(right):
                normalized = normalized[len(left):len(normalized) - len(right)].strip()
                changed = True
        if not changed:
            break

    normalized = normalize_title_surface(normalized)
    normalized = re.sub(r'\s+', ' ', normalized).strip(' ,')
    normalized = re.sub(r'^[\-â€“â€”:;]+', '', normalized).strip()
    normalized = re.sub(r'[\-â€“â€”:;]+$', '', normalized).strip()
    return normalized


def _collect_book_title_candidates(topic: str) -> List[Dict[str, Any]]:
    text = str(topic or '').strip()
    if not text:
        return []

    candidates: List[Dict[str, Any]] = []

    for source, pattern in BOOK_TITLE_QUOTE_PATTERNS:
        for match in pattern.finditer(text):
            raw = str(match.group(1) or '').strip()
            if not raw:
                continue
            candidates.append(
                {
                    'raw': raw,
                    'start': int(match.start(1)),
                    'end': int(match.end(1)),
                    'source': source,
                }
            )

    event_pattern = re.compile(
        r'([ê°€-í£A-Za-z0-9][^\n]{1,80}?)\s*(?:ì¶œíŒê¸°ë…íšŒ|ë¶í† í¬|í† í¬ì½˜ì„œíŠ¸|ì¶œíŒí–‰ì‚¬)'
    )
    for match in event_pattern.finditer(text):
        raw = str(match.group(1) or '').strip()
        if not raw:
            continue
        candidates.append(
            {
                'raw': raw,
                'start': int(match.start(1)),
                'end': int(match.end(1)),
                'source': 'event_context',
            }
        )

    after_book_pattern = re.compile(
        r'(?:^|[\s\(\[\{\'"â€œâ€˜<ã€Š])(?:ì±…|ì €ì„œ|ë„ì„œ|ì‹ ê°„|ì‘í’ˆ|ì œëª©)\s*(?:(?:ì€|ëŠ”|ì´|ê°€)\s+|[:ï¼š]\s*)?([^\n]{2,80})'
    )
    for match in after_book_pattern.finditer(text):
        raw = str(match.group(1) or '').strip()
        if not raw:
            continue
        clipped = re.split(r'(?:ì¶œíŒê¸°ë…íšŒ|ë¶í† í¬|í† í¬ì½˜ì„œíŠ¸|ì¶œíŒí–‰ì‚¬|ì•ˆë‚´|ì´ˆëŒ€|ê°œìµœ|ì—ì„œ|í˜„ì¥)', raw, maxsplit=1)[0].strip()
        if clipped:
            candidates.append(
                {
                    'raw': clipped,
                    'start': int(match.start(1)),
                    'end': int(match.start(1) + len(clipped)),
                    'source': 'book_context',
                }
            )

    return candidates


def _score_book_title_candidate(
    candidate: Dict[str, Any],
    topic: str,
    full_name: str,
) -> int:
    raw = str(candidate.get('raw') or '')
    text = _normalize_book_title_candidate(raw)
    if not text:
        return -999

    if not re.search(r'[ê°€-í£A-Za-z0-9]', text):
        return -999

    score = 0
    source = str(candidate.get('source') or '')
    start = int(candidate.get('start') or 0)
    end = int(candidate.get('end') or start)
    topic_text = str(topic or '')

    if source in {'angle', 'double_angle', 'single_angle', 'double_quote', 'single_quote', 'curly_double_quote', 'curly_single_quote', 'corner_quote', 'white_corner_quote'}:
        score += 5
    elif source in {'author_event_context', 'event_context', 'book_context'}:
        score += 3

    if 4 <= len(text) <= 30:
        score += 3
    elif 2 <= len(text) <= 45:
        score += 1
    else:
        score -= 4

    if len(text) <= 3:
        score -= 5

    for token in BOOK_TITLE_DISALLOWED_TOKENS:
        if token in text:
            score -= 8

    if full_name and text == full_name:
        score -= 8

    if re.fullmatch(r'[\d\s.,:/-]+', text):
        score -= 6
    if re.search(r'\d+\s*ì›”(?:\s*\d+\s*ì¼)?', text):
        score -= 10
    if any(ch in text for ch in '<>ã€Šã€‹ã€ˆã€‰ã€Œã€ã€ã€'):
        score -= 8

    left_context = topic_text[max(0, start - 22):start]
    right_context = topic_text[end:min(len(topic_text), end + 22)]
    around_context = f'{left_context} {right_context}'

    if any(marker in around_context for marker in BOOK_TITLE_CONTEXT_MARKERS):
        score += 5
    if any(marker in right_context for marker in BOOK_TITLE_EVENT_MARKERS):
        score += 4
    if any(marker in left_context for marker in BOOK_TITLE_EVENT_MARKERS):
        score += 2

    has_location_hint = any(loc in text for loc in BOOK_TITLE_LOCATION_HINTS)
    if has_location_hint:
        score -= 2
        if source in {'event_context', 'book_context'}:
            score -= 4
    if any(text.endswith(suffix) for suffix in BOOK_TITLE_LOCATION_SUFFIXES):
        score -= 12
    if source in {'event_context', 'book_context'} and not any(marker in text for marker in BOOK_TITLE_CONTEXT_MARKERS):
        score -= 3

    if ',' in text or 'Â·' in text:
        score += 1

    return score


def _extract_book_title(topic: str, params: Optional[Dict[str, Any]] = None) -> str:
    if not topic:
        return ''

    full_name = ''
    if isinstance(params, dict):
        full_name = str(params.get('fullName') or '').strip()

        context_analysis = params.get('contextAnalysis') if isinstance(params.get('contextAnalysis'), dict) else {}
        must_preserve = context_analysis.get('mustPreserve') if isinstance(context_analysis.get('mustPreserve'), dict) else {}
        explicit = _normalize_book_title_candidate(str(must_preserve.get('bookTitle') or ''))
        if explicit:
            return explicit

    candidates = _collect_book_title_candidates(topic)
    if full_name:
        author_event_pattern = re.compile(
            rf'{re.escape(full_name)}\s+([^\n]{{2,80}}?)\s*(?:ì¶œíŒê¸°ë…íšŒ|ë¶í† í¬|í† í¬ì½˜ì„œíŠ¸|ì¶œíŒí–‰ì‚¬)'
        )
        for match in author_event_pattern.finditer(str(topic)):
            raw = str(match.group(1) or '').strip()
            if not raw:
                continue
            candidates.append(
                {
                    'raw': raw,
                    'start': int(match.start(1)),
                    'end': int(match.end(1)),
                    'source': 'author_event_context',
                }
            )
    best_title = ''
    best_score = -999
    seen: set[str] = set()

    for candidate in candidates:
        normalized = _normalize_book_title_candidate(str(candidate.get('raw') or ''))
        if not normalized or normalized in seen:
            continue
        seen.add(normalized)

        nested_candidates = _collect_book_title_candidates(normalized)
        nested_title = ''
        nested_score = -999
        for nested in nested_candidates:
            nested_score_candidate = _score_book_title_candidate(nested, normalized, full_name)
            nested_normalized = _normalize_book_title_candidate(str(nested.get('raw') or ''))
            if nested_score_candidate > nested_score and nested_normalized:
                nested_score = nested_score_candidate
                nested_title = nested_normalized

        score = _score_book_title_candidate(candidate, topic, full_name)
        title = normalized
        if nested_title and nested_score > score:
            score = nested_score
            title = nested_title

        if score > best_score:
            best_score = score
            best_title = title

    if best_score >= 5:
        if full_name and best_title.startswith(f'{full_name} '):
            tail = _normalize_book_title_candidate(best_title[len(full_name):])
            if tail:
                best_title = tail
        return best_title

    return ''


def normalize_title_surface(title: str) -> str:
    cleaned = str(title or '').translate(
        str.maketrans(
            {
                'â€œ': '"',
                'â€': '"',
                'â€': '"',
                'â€Ÿ': '"',
            }
        )
    )
    cleaned = cleaned.strip().strip('"\'')
    if not cleaned:
        return ''

    candidate = cleaned
    # ì†Œìˆ˜ì  ì•ë’¤ ê³µë°± ì •ë¦¬: "0. 7%" -> "0.7%", "3 .5" -> "3.5"
    candidate = re.sub(r'(\d)\s*\.\s*(\d)', r'\1.\2', candidate)
    cleaned = candidate

    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    cleaned = re.sub(r'\s+([,.:;!?])', r'\1', cleaned)
    cleaned = re.sub(r'([,.:;!?])(?=[^\s\]\)\}])', r'\1 ', cleaned)
    cleaned = re.sub(r'\(\s+', '(', cleaned)
    cleaned = re.sub(r'\s+\)', ')', cleaned)
    cleaned = re.sub(r'\[\s+', '[', cleaned)
    cleaned = re.sub(r'\s+\]', ']', cleaned)
    cleaned = re.sub(r'\s{2,}', ' ', cleaned)
    cleaned = re.sub(r',(?:\s*,)+', ', ', cleaned)
    cleaned = re.sub(r'[!?]{2,}', '?', cleaned)
    return cleaned.strip(' ,')


def _fit_title_length(title: str) -> str:
    if not title:
        return ''
    normalized = re.sub(r'\s+', ' ', title).strip()
    if len(normalized) <= TITLE_LENGTH_HARD_MAX:
        return normalized
    compact = normalized.replace(' í•µì‹¬ ë©”ì‹œì§€', '').replace(' í•µì‹¬', '').replace(' í˜„ì¥', '')
    compact = re.sub(r'\s+', ' ', compact).strip()
    if len(compact) <= TITLE_LENGTH_HARD_MAX:
        return compact
    return compact[:TITLE_LENGTH_HARD_MAX].rstrip()


def _normalize_generated_title(generated_title: str, params: Dict[str, Any]) -> str:
    if not generated_title:
        return ''

    normalized = normalize_title_surface(generated_title)
    # ë„ì„œëª… êº¾ì‡  í‘œê¸°ëŠ” ìœ ì§€í•˜ë˜ ë‚´ë¶€ ê³µë°±ë§Œ ì •ë¦¬í•œë‹¤.
    normalized = re.sub(r'<\s*([^>]+?)\s*>', r'<\1>', normalized)
    normalized = re.sub(r'ã€Š\s*([^ã€‹]+?)\s*ã€‹', r'ã€Š\1ã€‹', normalized)
    normalized = re.sub(r'\s+,', ',', normalized)
    normalized = re.sub(r',\s*,', ',', normalized)
    normalized = normalize_title_surface(normalized)

    topic = str(params.get('topic') or '')
    title_purpose = resolve_title_purpose(topic, params)
    book_title = _extract_book_title(topic, params) if title_purpose == 'event_announcement' else ''
    if book_title:
        # ëª¨ë¸ì´ ë¹ˆ êº¾ì‡ (<>, ã€Šã€‹)ë¥¼ ì¶œë ¥í•œ ê²½ìš° ì±… ì œëª©ì„ ë³µì›í•œë‹¤.
        if re.search(r'<\s*>', normalized) and book_title not in normalized:
            normalized = re.sub(r'<\s*>', f'<{book_title}>', normalized)
        if re.search(r'ã€Š\s*ã€‹', normalized) and book_title not in normalized:
            normalized = re.sub(r'ã€Š\s*ã€‹', f'ã€Š{book_title}ã€‹', normalized)
        normalized = normalize_title_surface(normalized)

    if len(normalized) <= TITLE_LENGTH_HARD_MAX:
        return normalized

    if title_purpose == 'event_announcement':
        normalized = re.sub(r'\s{2,}', ' ', normalized).strip(' ,')
        if len(normalized) <= TITLE_LENGTH_HARD_MAX:
            return normalized

    return _fit_title_length(normalized)


def _normalize_title_for_similarity(title: str) -> str:
    normalized = str(title or '').lower().strip()
    normalized = re.sub(r'[\s\W_]+', '', normalized, flags=re.UNICODE)
    return normalized


def _title_similarity(a: str, b: str) -> float:
    norm_a = _normalize_title_for_similarity(a)
    norm_b = _normalize_title_for_similarity(b)
    if not norm_a or not norm_b:
        return 0.0
    return SequenceMatcher(None, norm_a, norm_b).ratio()


def _build_title_candidate_prompt(
    base_prompt: str,
    attempt: int,
    candidate_index: int,
    candidate_count: int,
    disallow_titles: List[str],
    title_purpose: str,
) -> str:
    event_variants = [
        'ì¼ì •/ì¥ì†Œ ì „ë‹¬ì„ ìš°ì„ í•˜ë˜, ë§ˆì§€ë§‰ ëª…ì‚¬êµ¬ë¥¼ ë°”ê¿” ìƒˆ ì–´ê°ìœ¼ë¡œ ì‘ì„±',
        'í–‰ë™ ìœ ë„(ì°¸ì—¬/ë°©ë¬¸/ë™í–‰) ì¤‘ì‹¬ìœ¼ë¡œ í›„í‚¹ ë‹¨ì–´ë¥¼ ìƒˆë¡­ê²Œ ì„ íƒ',
        'ì¸ë¬¼/ë„ì„œ/ë‚ ì§œ ì¤‘ 2ê°œë¥¼ ê²°í•©í•´ í˜„ì¥ê° ìˆëŠ” ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±',
        'ê°™ì€ ì •ë³´ë¼ë„ ì–´ìˆœì„ ë°”ê¿” ë‹¤ë¥¸ ë¦¬ë“¬ìœ¼ë¡œ ì‘ì„±',
        'í–‰ì‚¬ ì•ˆë‚´ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ë˜ ì¶”ìƒ í‘œí˜„ ì—†ì´ êµ¬ì²´ ì •ë³´ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±',
    ]
    default_variants = [
        'ì§ˆë¬¸í˜• ê¸´ì¥ê°ì„ ìœ ì§€í•˜ë˜ í•µì‹¬ ë™ì‚¬ë¥¼ ê¸°ì¡´ê³¼ ë‹¤ë¥´ê²Œ ì„ íƒ',
        'ìˆ«ì/íŒ©íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ êµ¬ì„±í•˜ê³  ë¬¸ì¥ ì¢…ê²°ì„ ìƒˆë¡­ê²Œ ì‘ì„±',
        'ì›ì¸-ê²°ê³¼ íë¦„ì„ ë„£ì–´ í´ë¦­ ì´ìœ ê°€ ìƒê¸°ê²Œ ì‘ì„±',
        'í•µì‹¬ í‚¤ì›Œë“œ ì´í›„ì˜ ì–´êµ¬ë¥¼ ì™„ì „íˆ ìƒˆë¡­ê²Œ ì¬êµ¬ì„±',
        'ì •ë³´ìš”ì†Œ 3ê°œ ì´ë‚´ë¥¼ ì§€í‚¤ë©´ì„œ ëŒ€ë¹„/ë³€í™” í¬ì¸íŠ¸ë¥¼ ë¶€ê°',
    ]
    variants = event_variants if title_purpose == 'event_announcement' else default_variants
    variant = variants[(candidate_index - 1) % len(variants)]

    blocked = [f'"{t}"' for t in disallow_titles[-4:] if t]
    blocked_line = (
        f"- ë‹¤ìŒ ì œëª©/ë¬¸êµ¬ë¥¼ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”: {', '.join(blocked)}"
        if blocked else
        "- ì§ì „ ì‹œë„ì™€ ë™ì¼í•œ ë¬¸êµ¬/ì–´ìˆœ ë°˜ë³µ ê¸ˆì§€"
    )

    return f"""{base_prompt}

<diversity_hint attempt="{attempt}" candidate="{candidate_index}/{candidate_count}">
- ì´ë²ˆ í›„ë³´ì˜ ì´ˆì : {variant}
{blocked_line}
- 1ìˆœìœ„ í‚¤ì›Œë“œ ì‹œì‘ ê·œì¹™ì€ ë°˜ë“œì‹œ ì§€í‚¤ë˜, ê·¸ ë’¤ ë¬¸ì¥ì€ ìƒˆë¡­ê²Œ ì‘ì„±
</diversity_hint>
"""


def _compute_similarity_penalty(
    title: str,
    previous_titles: List[str],
    threshold: float,
    max_penalty: int,
) -> Dict[str, Any]:
    if not title or not previous_titles or max_penalty <= 0:
        return {'penalty': 0, 'maxSimilarity': 0.0, 'against': ''}

    best_similarity = 0.0
    against = ''
    for prev in previous_titles:
        if not prev:
            continue
        similarity = _title_similarity(title, prev)
        if similarity > best_similarity:
            best_similarity = similarity
            against = prev

    if best_similarity < threshold:
        return {'penalty': 0, 'maxSimilarity': round(best_similarity, 3), 'against': against}

    span = max(0.01, 1.0 - threshold)
    ratio = (best_similarity - threshold) / span
    penalty = max(1, min(max_penalty, int(round(ratio * max_penalty))))
    return {'penalty': penalty, 'maxSimilarity': round(best_similarity, 3), 'against': against}


def resolve_title_purpose(topic: str, params: Dict[str, Any]) -> str:
    event_markers = EVENT_NAME_MARKERS + (
        'í–‰ì‚¬',
        'ê°œìµœ',
        'ì—´ë¦¬ëŠ”',
        'ì—´ë¦½ë‹ˆë‹¤',
        'ì´ˆëŒ€',
        'ì°¸ì„',
    )
    if any(marker in (topic or '') for marker in event_markers):
        return 'event_announcement'

    context_analysis = params.get('contextAnalysis') if isinstance(params.get('contextAnalysis'), dict) else {}
    intent = str(context_analysis.get('intent') or '').strip().lower()
    offline_intents = {
        'event_announcement',
        'offline_engagement',
        'event_participation',
        'event_attendance',
        'brief_notice',
        'schedule_notice',
    }
    if intent in offline_intents:
        return 'event_announcement'
    if intent:
        return intent
    return ''


def build_event_title_policy_instruction(params: Dict[str, Any]) -> str:
    topic = str(params.get('topic') or '')
    context_analysis = params.get('contextAnalysis') if isinstance(params.get('contextAnalysis'), dict) else {}
    user_keywords = params.get('userKeywords') if isinstance(params.get('userKeywords'), list) else []

    must_preserve = context_analysis.get('mustPreserve') if isinstance(context_analysis.get('mustPreserve'), dict) else {}
    event_date = str(must_preserve.get('eventDate') or '').strip()
    event_location = str(must_preserve.get('eventLocation') or '').strip()
    date_hint = _extract_date_hint(event_date) or _extract_date_hint(topic)
    location_hint = event_location or (user_keywords[0] if user_keywords else '')
    keyword_line = ', '.join([str(k).strip() for k in user_keywords[:2] if str(k).strip()]) or 'í•µì‹¬ ì¥ì†Œ í‚¤ì›Œë“œ'
    event_name_hint = _detect_event_label(topic)

    return f"""
<title_goal purpose="event_announcement" priority="critical">
  <rule>ì œëª©ì€ í–‰ì‚¬ ì•ˆë‚´/ì´ˆëŒ€ ëª©ì ì´ ì¦‰ì‹œ ë“œëŸ¬ë‚˜ì•¼ í•©ë‹ˆë‹¤.</rule>
  <rule>ì¶”ì¸¡í˜•/ë…¼ìŸí˜•/ê³µê²©í˜• ë¬¸êµ¬(ì˜ˆ: ì§„ì§œ ì†ë‚´, ì™œ ì™”ëƒ, ë‹µí• ê¹Œ?)ì™€ ë¬¼ìŒí‘œ(?)ëŠ” ê¸ˆì§€í•©ë‹ˆë‹¤.</rule>
  <rule>ì œëª©ì—ëŠ” ì•ˆë‚´í˜• í‘œí˜„(ì•ˆë‚´, ì´ˆëŒ€, ê°œìµœ, ì—´ë¦½ë‹ˆë‹¤, í–‰ì‚¬) ë˜ëŠ” í–‰ì‚¬ëª…("{event_name_hint}")ì„ í¬í•¨í•˜ì‹­ì‹œì˜¤.</rule>
  <rule>ì œëª©ì—ëŠ” ì•ˆì „í•œ í›„í‚¹ ë‹¨ì–´ë¥¼ 1ê°œ ì´ìƒ í¬í•¨í•˜ì‹­ì‹œì˜¤: í˜„ì¥, ì§ì ‘, ì¼ì •, ì•ˆë‚´, ì´ˆëŒ€, ë§Œë‚¨, ì°¸ì„</rule>
  <rule>ì¶”ìƒ ì¹´í”¼(ì˜ˆ: "í•µì‹¬ ëŒ€í™” ê³µê°œ", "í•µì‹¬ ë©”ì‹œì§€ ê³µê°œ")ë§Œ ë‹¨ë…ìœ¼ë¡œ ì“°ì§€ ë§ê³  ë‚ ì§œ/ì¸ë¬¼/ì±…ì œëª© ê°™ì€ ê³ ìœ  ì •ë³´ë¥¼ í¬í•¨í•˜ì‹­ì‹œì˜¤.</rule>
  <rule>ê°€ëŠ¥í•˜ë©´ ë‚ ì§œì™€ ì¥ì†Œë¥¼ í¬í•¨í•˜ì‹­ì‹œì˜¤. ë‚ ì§œ íŒíŠ¸: {date_hint or '(ì—†ìŒ)'} / ì¥ì†Œ íŒíŠ¸: {location_hint or '(ì—†ìŒ)'}</rule>
  <rule>SEO ê²€ìƒ‰ì–´ëŠ” ì œëª© ì•ë¶€ë¶„ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤: {keyword_line}</rule>
</title_goal>
""".strip()


def validate_event_announcement_title(title: str, params: Dict[str, Any]) -> Dict[str, Any]:
    cleaned = (title or '').strip()
    if not cleaned:
        return {'passed': False, 'reason': 'ì œëª©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.'}

    topic = str(params.get('topic') or '')
    context_analysis = params.get('contextAnalysis') if isinstance(params.get('contextAnalysis'), dict) else {}
    user_keywords = params.get('userKeywords') if isinstance(params.get('userKeywords'), list) else []

    banned_phrases = (
        'ì§„ì§œ ì†ë‚´',
        'ì™œ ì™”ëƒ',
        'ë‹µí• ê¹Œ',
        'ì†ë‚´ëŠ”',
        'ì˜í˜¹',
        'ë…¼ë€',
    )
    if any(phrase in cleaned for phrase in banned_phrases) or '?' in cleaned:
        return {
            'passed': False,
            'reason': (
                "í–‰ì‚¬ ì•ˆë‚´ ëª©ì ê³¼ ë§ì§€ ì•ŠëŠ” ì œëª© í†¤ì…ë‹ˆë‹¤. ì¶”ì¸¡í˜•/ë…¼ìŸí˜• í‘œí˜„ê³¼ ë¬¼ìŒí‘œë¥¼ ì œê±°í•˜ê³  "
                "'ì•ˆë‚´/ì´ˆëŒ€/ê°œìµœ/í–‰ì‚¬ëª…' ê°™ì€ ì•ˆë‚´í˜• í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”."
            ),
        }

    vague_phrases = (
        'í•µì‹¬ ëŒ€í™” ê³µê°œ',
        'í•µì‹¬ ë©”ì‹œì§€ ê³µê°œ',
        'í•µì‹¬ ë©”ì‹œì§€ í˜„ì¥ ê³µê°œ',
    )
    if any(phrase in cleaned for phrase in vague_phrases):
        return {
            'passed': False,
            'reason': "ì¶”ìƒ ë¬¸êµ¬ ì¤‘ì‹¬ ì œëª©ì…ë‹ˆë‹¤. ë‚ ì§œ/ì¸ë¬¼/ì±…ì œëª© ë“± í–‰ì‚¬ ê³ ìœ  ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”.",
        }

    event_tokens = ('ì•ˆë‚´', 'ì´ˆëŒ€', 'ê°œìµœ', 'ì—´ë¦½ë‹ˆë‹¤', 'í–‰ì‚¬') + EVENT_NAME_MARKERS
    if not any(token in cleaned for token in event_tokens):
        return {
            'passed': False,
            'reason': "í–‰ì‚¬ ì•ˆë‚´ ëª©ì ì´ ì œëª©ì— ë“œëŸ¬ë‚˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì•ˆë‚´/ì´ˆëŒ€/ê°œìµœ/í–‰ì‚¬ëª…ì„ í¬í•¨í•˜ì„¸ìš”.",
        }

    hook_tokens = ('í˜„ì¥', 'ì§ì ‘', 'ì¼ì •', 'ì•ˆë‚´', 'ì´ˆëŒ€', 'ë§Œë‚¨', 'ì°¸ì„')
    if not any(token in cleaned for token in hook_tokens):
        return {
            'passed': False,
            'reason': (
                "í›„í‚¹ ìš”ì†Œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. 'í˜„ì¥/ì§ì ‘/ì¼ì •/ì•ˆë‚´/ì´ˆëŒ€/ë§Œë‚¨/ì°¸ì„' ì¤‘ "
                "í•˜ë‚˜ ì´ìƒì„ ì œëª©ì— í¬í•¨í•˜ì„¸ìš”."
            ),
        }

    normalized_keywords = [str(k).strip() for k in user_keywords if str(k).strip()]
    primary_keyword = normalized_keywords[0] if normalized_keywords else ''
    if primary_keyword and primary_keyword not in cleaned:
        return {
            'passed': False,
            'reason': f'1ìˆœìœ„ ê²€ìƒ‰ì–´ "{primary_keyword}"ê°€ ì œëª©ì— ì—†ìŠµë‹ˆë‹¤.',
        }

    must_preserve = context_analysis.get('mustPreserve') if isinstance(context_analysis.get('mustPreserve'), dict) else {}
    event_date = str(must_preserve.get('eventDate') or '').strip()
    date_hint = _extract_date_hint(event_date) or _extract_date_hint(topic)
    if date_hint and not _contains_date_hint(cleaned, date_hint):
        return {
            'passed': False,
            'reason': f'í–‰ì‚¬ ë‚ ì§œ ì •ë³´ê°€ ì œëª©ì— ì—†ìŠµë‹ˆë‹¤. ì˜ˆ: {date_hint}',
        }

    try:
        from services.posts.validation import validate_date_weekday_pairs

        date_weekday_result = validate_date_weekday_pairs(
            cleaned,
            year_hint=f"{event_date} {topic}".strip(),
        )
    except Exception:
        date_weekday_result = {'passed': True, 'issues': []}
    if isinstance(date_weekday_result, dict) and not date_weekday_result.get('passed', True):
        issues = date_weekday_result.get('issues') if isinstance(date_weekday_result.get('issues'), list) else []
        mismatch = next(
            (
                item for item in issues
                if isinstance(item, dict) and str(item.get('type') or '') == 'date_weekday_mismatch'
            ),
            None,
        )
        if mismatch:
            date_text = str(mismatch.get('dateText') or '').strip()
            expected = str(mismatch.get('expectedWeekday') or '').strip()
            found = str(mismatch.get('foundWeekday') or '').strip()
            if date_text and expected:
                return {
                    'passed': False,
                    'reason': f'ë‚ ì§œ-ìš”ì¼ì´ ë¶ˆì¼ì¹˜í•©ë‹ˆë‹¤. {date_text}ì€ {expected}ì…ë‹ˆë‹¤(ì…ë ¥: {found}).',
                }

    book_title = _extract_book_title(topic, params)
    full_name = str(params.get('fullName') or '').strip()

    anchor_tokens: List[str] = []
    anchor_tokens.extend(_split_hint_tokens(date_hint))
    anchor_tokens.extend(_split_hint_tokens(book_title))
    if full_name:
        anchor_tokens.append(full_name)
    deduped_anchor_tokens: List[str] = []
    seen_anchor_tokens = set()
    for token in anchor_tokens:
        normalized = str(token).strip()
        if not normalized:
            continue
        if normalized in seen_anchor_tokens:
            continue
        seen_anchor_tokens.add(normalized)
        deduped_anchor_tokens.append(normalized)
    if deduped_anchor_tokens and not any(token in cleaned for token in deduped_anchor_tokens):
        return {
            'passed': False,
            'reason': (
                "í–‰ì‚¬ ê³ ìœ  ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë‚ ì§œ/ì¸ë¬¼ëª…/ë„ì„œëª… ì¤‘ ìµœì†Œ 1ê°œë¥¼ ì œëª©ì— í¬í•¨í•˜ì„¸ìš”."
            ),
        }

    is_book_event = any(marker in topic for marker in ('ì¶œíŒê¸°ë…íšŒ', 'ë¶í† í¬', 'í† í¬ì½˜ì„œíŠ¸'))
    if is_book_event and book_title:
        book_tokens = _split_hint_tokens(book_title)
        if book_tokens and not any(token in cleaned for token in book_tokens):
            return {
                'passed': False,
                'reason': f'ì¶œíŒ í–‰ì‚¬ ì œëª©ì€ ë„ì„œëª… ë‹¨ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤. ì˜ˆ: {book_title}',
            }

    if full_name and full_name not in cleaned:
        return {
            'passed': False,
            'reason': f'í–‰ì‚¬ ì•ˆë‚´ ì œëª©ì—ëŠ” ì¸ë¬¼ëª…("{full_name}")ì„ í¬í•¨í•˜ì„¸ìš”.',
        }

    event_location = str(must_preserve.get('eventLocation') or '').strip()
    location_tokens = _split_hint_tokens(event_location)
    if location_tokens and not any(token in cleaned for token in location_tokens):
        return {'passed': False, 'reason': f'í–‰ì‚¬ ì¥ì†Œ ì •ë³´ê°€ ì œëª©ì— ì—†ìŠµë‹ˆë‹¤. ì˜ˆ: {event_location}'}

    return {'passed': True}


def _build_event_title_prompt(params: Dict[str, Any]) -> str:
    topic = str(params.get('topic') or '')
    full_name = str(params.get('fullName') or '').strip()
    content_preview = str(params.get('contentPreview') or '')
    user_keywords = params.get('userKeywords') if isinstance(params.get('userKeywords'), list) else []
    context_analysis = params.get('contextAnalysis') if isinstance(params.get('contextAnalysis'), dict) else {}
    must_preserve = context_analysis.get('mustPreserve') if isinstance(context_analysis.get('mustPreserve'), dict) else {}

    primary_keyword = str(user_keywords[0]).strip() if user_keywords else ''
    event_date = str(must_preserve.get('eventDate') or '').strip()
    date_hint = _extract_date_hint(event_date) or _extract_date_hint(topic)
    event_location = str(must_preserve.get('eventLocation') or '').strip() or primary_keyword
    event_label = _detect_event_label(topic)
    book_title = _extract_book_title(topic, params)
    is_book_event = any(marker in topic for marker in ('ì¶œíŒê¸°ë…íšŒ', 'ë¶í† í¬', 'í† í¬ì½˜ì„œíŠ¸'))
    hook_words = "í˜„ì¥, ì§ì ‘, ì¼ì •, ì•ˆë‚´, ì´ˆëŒ€, ë§Œë‚¨, ì°¸ì„"
    number_validation = extract_numbers_from_content(content_preview)

    return f"""<event_title_prompt priority="critical">
<role>ë‹¹ì‹ ì€ í–‰ì‚¬ ì•ˆë‚´í˜• ë¸”ë¡œê·¸ ì œëª© ì—ë””í„°ì…ë‹ˆë‹¤. ëª©ì  ì í•©ì„±ê³¼ ê·œì¹™ ì¤€ìˆ˜ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ í•©ë‹ˆë‹¤.</role>

<input>
  <topic>{topic}</topic>
  <author>{full_name or '(ì—†ìŒ)'}</author>
  <primary_keyword>{primary_keyword or '(ì—†ìŒ)'}</primary_keyword>
  <date_hint>{date_hint or '(ì—†ìŒ)'}</date_hint>
  <location_hint>{event_location or '(ì—†ìŒ)'}</location_hint>
  <book_title>{book_title or '(ì—†ìŒ)'}</book_title>
  <event_label>{event_label}</event_label>
  <content_preview>{content_preview[:500]}</content_preview>
</input>

<hard_rules>
  <rule>ì œëª© ê¸¸ì´ëŠ” {TITLE_LENGTH_HARD_MIN}-{TITLE_LENGTH_HARD_MAX}ì.</rule>
  <rule>ë¬¼ìŒí‘œ(?)ì™€ ì¶”ì¸¡/ë…¼ìŸí˜• ì–´íˆ¬ ê¸ˆì§€.</rule>
  <rule>ì•ˆë‚´ ëª©ì ì´ ì¦‰ì‹œ ë“œëŸ¬ë‚˜ë„ë¡ "{event_label}" ë˜ëŠ” "ì•ˆë‚´/ì´ˆëŒ€/ê°œìµœ/í–‰ì‚¬" í¬í•¨.</rule>
  <rule>í›„í‚¹ ë‹¨ì–´ 1ê°œ ì´ìƒ í¬í•¨: {hook_words}.</rule>
  <rule>1ìˆœìœ„ ê²€ìƒ‰ì–´ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í¬í•¨: "{primary_keyword or '(ì—†ìŒ)'}".</rule>
  <rule>ë‚ ì§œ íŒíŠ¸ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í¬í•¨: "{date_hint or '(ì—†ìŒ)'}".</rule>
  <rule>ì¸ë¬¼ëª…ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í¬í•¨: "{full_name or '(ì—†ìŒ)'}".</rule>
  <rule>ë„ì„œ í–‰ì‚¬({is_book_event})ì´ê³  ë„ì„œëª…ì´ ìˆìœ¼ë©´ ë„ì„œëª… ë‹¨ì„œë¥¼ í¬í•¨: "{book_title or '(ì—†ìŒ)'}".</rule>
  <rule>ì¥ì†Œ íŒíŠ¸ê°€ ìˆìœ¼ë©´ ê°€ëŠ¥í•œ í•œ í¬í•¨: "{event_location or '(ì—†ìŒ)'}".</rule>
</hard_rules>

{number_validation.get('instruction', '')}

<output_format>
  ìˆœìˆ˜í•œ ì œëª© í•œ ì¤„ë§Œ ì¶œë ¥. ë”°ì˜´í‘œ, ë¶€ì—°ì„¤ëª…, ë²ˆí˜¸, ë§ˆí¬ë‹¤ìš´ ê¸ˆì§€.
</output_format>
</event_title_prompt>"""

def build_title_prompt(params: Dict[str, Any]) -> str:
    # No try/except blocking logic here. Let it propagate.
    content_preview = params.get('contentPreview', '')
    background_text = params.get('backgroundText', '')
    topic = params.get('topic', '')
    full_name = params.get('fullName', '')
    keywords = params.get('keywords', [])
    user_keywords = params.get('userKeywords', [])
    category = params.get('category', '')
    status = params.get('status', '')
    title_scope = params.get('titleScope', {})
    forced_type = params.get('_forcedType')
    stance_text = params.get('stanceText', '')  # ğŸ”‘ [NEW] ì…ì¥ë¬¸
    title_purpose = resolve_title_purpose(topic, params)
    if title_purpose == 'event_announcement':
        return _build_event_title_prompt(params)
    event_title_policy = build_event_title_policy_instruction(params) if title_purpose == 'event_announcement' else ''
    
    avoid_local_in_title = bool(title_scope and title_scope.get('avoidLocalInTitle'))
    detected_type_id = None
    
    if forced_type and forced_type in TITLE_TYPES:
        detected_type_id = forced_type
    else:
        detected_type_id = detect_content_type(content_preview, category)
        if avoid_local_in_title and detected_type_id == 'LOCAL_FOCUSED':
            detected_type_id = 'ISSUE_ANALYSIS' # avoidLocalInTitle ì •ì±… ì ìš©
            
    primary_type = TITLE_TYPES.get(detected_type_id) or TITLE_TYPES['DATA_BASED']
    # If default was chosen but really we want Viral Hook for general cases:
    if detected_type_id == 'VIRAL_HOOK':
         primary_type = TITLE_TYPES['VIRAL_HOOK']
    
    number_validation = extract_numbers_from_content(content_preview)
    election_compliance = get_election_compliance_instruction(status)
    keyword_strategy = get_keyword_strategy_instruction(user_keywords, keywords)
    user_few_shot = build_user_provided_few_shot_instruction(primary_type['id'], params)
    
    region_scope_instruction = ""
    if avoid_local_in_title:
        region_scope_instruction = f"""
[TITLE REGION SCOPE]
- Target position: {title_scope.get('position', 'metro-level') if title_scope else 'metro-level'}
- Do NOT use district/town names (gu/gun/dong/eup/myeon) in the title.
- Use the metro-wide region like "{title_scope.get('regionMetro', 'the city/province') if title_scope else 'the city/province'}".
"""

    good_lines = []
    for i, ex in enumerate(primary_type['good']):
        good_lines.append(f"{i+1}. \"{ex['title']}\" ({ex.get('chars', 0)}ì)\n   â†’ {ex.get('analysis', '')}")
    good_examples = "\n".join(good_lines)

    bad_lines = []
    for i, ex in enumerate(primary_type['bad']):
        bad_lines.append(f"{i+1}. âŒ \"{ex['title']}\"\n   ë¬¸ì œ: {ex.get('problem', '')}\n   âœ… ìˆ˜ì •: \"{ex.get('fix', '')}\"")
    bad_examples = "\n\n".join(bad_lines)

    primary_kw_str = user_keywords[0] if user_keywords else '(ì—†ìŒ)'
    objective_block = """
<objective>
ì•„ë˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬, **ë…ìê°€ í´ë¦­í•˜ì§€ ì•Šê³ ëŠ” ëª» ë°°ê¸°ëŠ” ì„œì‚¬ì  ê¸´ì¥ê°ì´ ìˆëŠ” ë¸”ë¡œê·¸ ì œëª©**ì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

ã€í•µì‹¬ ì›ì¹™: ì •ë³´ ê²©ì°¨(Information Gap)ã€‘
ì¢‹ì€ ì œëª©ì€ "ë‹µ"ì´ ì•„ë‹ˆë¼ "ì§ˆë¬¸"ì„ ë‚¨ê¹ë‹ˆë‹¤.
- âŒ ì„ ì–¸í˜• (ê¸´ì¥ê° ì—†ìŒ): "ì´ì¬ì„±ì´ ê²½ì œ 0.7%ë¥¼ ë°”ê¾¼ë‹¤" â†’ ë‹µì„ ë‹¤ ì•Œë ¤ì¤˜ì„œ í´ë¦­ ë¶ˆí•„ìš”
- âŒ í‚¤ì›Œë“œ ë‚˜ì—´ (ì˜ë¯¸ ì—†ìŒ): "ì´ì¬ì„± ë¶€ì‚° AI 3ëŒ€ ê°•êµ­?" â†’ ë¬¸ì¥ì´ ì•„ë‹˜
- âœ… ì„œì‚¬ì  ê¸´ì¥ê°: "ë¶€ì‚° ê²½ì œ 0.7%, ì™œ ì´ ë‚¨ìê°€ ë›°ì–´ë“¤ì—ˆë‚˜" â†’ êµ¬ì²´ì  íŒ©íŠ¸ + ë¯¸í•´ê²° ì§ˆë¬¸

ã€ê¸ˆì§€ã€‘
- ì§€ë£¨í•œ ê³µë¬´ì› ìŠ¤íƒ€ì¼("~ê°œìµœ", "~ì°¸ì„", "~ë°œí‘œ")
- ì„ ì–¸í˜• ê²°ë¡ ("~ë°”ê¾¼ë‹¤", "~ì´ëˆë‹¤", "~ì™„ì„±í•œë‹¤")
- í‚¤ì›Œë“œë§Œ ë‚˜ì—´í•˜ê³  ë¬¸ì¥ì„ ì™„ì„±í•˜ì§€ ì•ŠëŠ” ê²ƒ
- ê³¼ë„í•œ ìê·¹("ì¶©ê²©", "ê²½ì•…", "ê²°êµ­ í„°ì¡Œë‹¤")
</objective>
""".strip()
    style_ban_rule = '"ë°œí‘œ", "ê°œìµœ", "ì°¸ì„" ë“± ë³´ë„ìë£Œ ìŠ¤íƒ€ì¼ ê¸ˆì§€'
    keyword_position_rule = (
        f'í•µì‹¬ í‚¤ì›Œë“œ "{primary_kw_str}" ë°˜ë“œì‹œ í¬í•¨. í‚¤ì›Œë“œ ì§í›„ì— ë°˜ë“œì‹œ êµ¬ë¶„ì(ì‰¼í‘œ, ë¬¼ìŒí‘œ, ì¡°ì‚¬+ì‰¼í‘œ)ë¥¼ ë„£ì–´ë¼. '
        'âœ… "ë¶€ì‚° ì§€ë°©ì„ ê±°, ì™œ~" âœ… "ë¶€ì‚° ì§€ë°©ì„ ê±°ì— ë›°ì–´ë“ ~" âŒ "ë¶€ì‚° ì§€ë°©ì„ ê±° ì´ì¬ì„±" '
        '(ë„¤ì´ë²„ê°€ í•˜ë‚˜ì˜ í‚¤ì›Œë“œë¡œ ì¸ì‹)'
    )

    if title_purpose == 'event_announcement':
        event_label = _detect_event_label(topic)
        objective_block = f"""
<objective>
ì•„ë˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬, **í–‰ì‚¬ ì•ˆë‚´ ëª©ì ì´ ë¶„ëª…í•˜ë©´ì„œë„ í´ë¦­í•˜ê³  ì‹¶ì–´ì§€ëŠ” ì œëª©**ì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

ã€í•µì‹¬ ì›ì¹™: ëª©ì  ì í•©ì„± ìš°ì„ ã€‘
- ì œëª©ì€ ë¨¼ì € "ì´ ê¸€ì´ í–‰ì‚¬ ì•ˆë‚´/ì´ˆëŒ€"ë¼ëŠ” ì ì´ ì¦‰ì‹œ ë“œëŸ¬ë‚˜ì•¼ í•©ë‹ˆë‹¤.
- ê·¸ ë‹¤ìŒì— í›„í‚¹ì„ ì–¹ìŠµë‹ˆë‹¤. ì•ˆë‚´ ëª©ì ì„ íë¦¬ë©´ ì‹¤íŒ¨ì…ë‹ˆë‹¤.

ã€í—ˆìš©ã€‘
- ì•ˆë‚´í˜• í‘œí˜„: "ì•ˆë‚´", "ì´ˆëŒ€", "ê°œìµœ", "ì—´ë¦½ë‹ˆë‹¤", "í–‰ì‚¬ëª…"
- ë‚ ì§œ/ì¥ì†Œ/í–‰ì‚¬ëª…ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•œ ì œëª©
- ì•ˆì „í•œ í›„í‚¹ ë‹¨ì–´: "í˜„ì¥", "ì§ì ‘", "ì¼ì •", "ì•ˆë‚´", "ì´ˆëŒ€", "ë§Œë‚¨", "ì°¸ì„"
- ì¶”ìƒ ë¬¸êµ¬("í•µì‹¬ ëŒ€í™” ê³µê°œ", "í•µì‹¬ ë©”ì‹œì§€ ê³µê°œ") ë‹¨ë… ì‚¬ìš© ê¸ˆì§€

ã€ê¶Œì¥ ê³µì‹ã€‘
- [ë©”ì¸ SEO í‚¤ì›Œë“œ] + [ë‚ ì§œ/ì¥ì†Œ] + [í›„í‚¹ ë‹¨ì–´] + [[í–‰ì‚¬ëª…]/ì•ˆë‚´]
- ì˜ˆ: "[ì¥ì†Œëª…], [ë‚ ì§œ] [{event_label}] ì•ˆë‚´"

ã€ê¸ˆì§€ã€‘
- ì¶”ì¸¡í˜•/ë…¼ìŸí˜•/ê³µê²©í˜• í‘œí˜„: "ì§„ì§œ ì†ë‚´", "ì™œ ì™”ëƒ", "ë‹µí• ê¹Œ"
- ë¬¼ìŒí‘œ(?) ê¸°ë°˜ ë„ë°œí˜• ì œëª©
- ê³¼ë„í•œ ìê·¹("ì¶©ê²©", "ê²½ì•…", "ê²°êµ­ í„°ì¡Œë‹¤")
</objective>
""".strip()
        style_ban_rule = 'í–‰ì‚¬ ì•ˆë‚´ ëª©ì ì„ íë¦¬ëŠ” ë…¼ìŸí˜•/ë„ë°œí˜• ì¹´í”¼ ê¸ˆì§€ (ì¶”ì¸¡Â·ê³µê²©Â·ì„ ë™ ì–´íˆ¬ ê¸ˆì§€)'
        keyword_position_rule = (
            f'í•µì‹¬ í‚¤ì›Œë“œ "{primary_kw_str}" ë°˜ë“œì‹œ í¬í•¨. í‚¤ì›Œë“œ ì§í›„ì—ëŠ” ì‰¼í‘œ(,) ë˜ëŠ” ì¡°ì‚¬(ì—/ì˜/ì—ì„œ ë“±)ë¥¼ ì‚¬ìš©í•´ ë¶„ë¦¬í•˜ì„¸ìš”. '
            'âœ… "[ì¥ì†Œëª…], [ë‚ ì§œ] [í–‰ì‚¬ëª…] ì•ˆë‚´" âœ… "[ì¥ì†Œëª…]ì—ì„œ ì—´ë¦¬ëŠ” [í–‰ì‚¬ëª…] ì•ˆë‚´" '
            'âŒ "[ì¥ì†Œëª…] [ì¸ë¬¼ëª…] [í–‰ì‚¬ëª…]"'
        )
    
    return f"""<title_generation_prompt>

<role>ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì œëª© ì „ë¬¸ê°€ (í´ë¦­ë¥  1ìœ„ ì¹´í”¼ë¼ì´í„°)</role>

{objective_block}

<content_type detected="{primary_type['id']}">
  <name>{primary_type['name']}</name>
  <when>{primary_type['when']}</when>
  <pattern>{primary_type['pattern']}</pattern>
  <naver_tip>{primary_type['naverTip']}</naver_tip>
</content_type>

{('<narrative_principle>' + primary_type['principle'] + '</narrative_principle>') if primary_type.get('principle') else ''}

<input>
  <topic>{topic}</topic>
  <author>{full_name}</author>
  <stance_summary priority="Highest">{stance_text[:500] if stance_text else '(ì—†ìŒ) - ì…ì¥ë¬¸ì´ ì—†ìœ¼ë©´ ë³¸ë¬¸ ë‚´ìš© ë°”íƒ•ìœ¼ë¡œ ì‘ì„±'}</stance_summary>
  <content_preview>{(content_preview or '')[:800]}</content_preview>
  <background>{background_text[:300] if background_text else '(ì—†ìŒ)'}</background>
</input>

<examples type="good">
{good_examples}
</examples>

<examples type="bad">
{bad_examples}
</examples>

{user_few_shot}

<rules priority="critical">
  <rule id="length_max">ğŸš¨ {TITLE_LENGTH_HARD_MAX}ì ì´ë‚´ (ë„¤ì´ë²„ ê²€ìƒ‰ê²°ê³¼ ì˜ë¦¼ ë°©ì§€) - ì ˆëŒ€ ì´ˆê³¼ ê¸ˆì§€!</rule>
  <rule id="length_optimal">{TITLE_LENGTH_OPTIMAL_MIN}-{TITLE_LENGTH_OPTIMAL_MAX}ì ê¶Œì¥ (í´ë¦­ë¥  ìµœê³  êµ¬ê°„)</rule>
  <rule id="no_slot_placeholder">ìŠ¬ë¡¯ í”Œë ˆì´ìŠ¤í™€ë”([í–‰ì‚¬ëª…], [ì§€ì—­ëª…], [ì •ì±…ëª…] ë“±)ë¥¼ ì œëª©ì— ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.</rule>
  <rule id="no_ellipsis">ë§ì¤„ì„í‘œ("...") ì ˆëŒ€ ê¸ˆì§€</rule>
  <rule id="keyword_position">{keyword_position_rule}</rule>
  <rule id="no_greeting">ì¸ì‚¬ë§("ì•ˆë…•í•˜ì„¸ìš”"), ì„œìˆ í˜• ì–´ë¯¸("~ì…ë‹ˆë‹¤") ì ˆëŒ€ ê¸ˆì§€</rule>
  <rule id="style_ban">{style_ban_rule}</rule>
  <rule id="narrative_tension">ì½ì€ ë’¤ "ê·¸ë˜ì„œ?" "ì™œ?"ê°€ ë– ì˜¤ë¥´ëŠ” ì œëª©ì´ ì¢‹ë‹¤. ê¸°ë²•ì„ ì–µì§€ë¡œ ë„£ì§€ ë§ê³  ìì—°ìŠ¤ëŸ¬ìš´ í˜¸ê¸°ì‹¬ì„ ë§Œë“¤ì–´ë¼. ì„ ì–¸í˜• ì¢…ê²°("~ë°”ê¾¼ë‹¤") ê¸ˆì§€. ì •ë³´ ìš”ì†Œ 3ê°œ ì´í•˜.</rule>
  <rule id="info_density">ì œëª©ì— ë‹´ëŠ” ì •ë³´ ìš”ì†ŒëŠ” ìµœëŒ€ 3ê°œ. SEO í‚¤ì›Œë“œëŠ” 1ê°œë¡œ ì¹´ìš´íŠ¸. ìš”ì†Œ: SEOí‚¤ì›Œë“œ, ì¸ëª…, ìˆ˜ì¹˜, ì •ì±…ëª…, ìˆ˜ì‹ì–´. "ë¶€ì‚° ì§€ë°©ì„ ê±°, ì™œ ì´ ë‚¨ìê°€ ë›°ì–´ë“¤ì—ˆë‚˜" = 2ê°œ OK. "ë¶€ì‚° ì§€ë°©ì„ ê±° ì´ì¬ëª… 2í˜¸ ì´ì¬ì„± ì›ì¹™ ì„ íƒ" = 5ê°œ NG.</rule>
</rules>

{event_title_policy}
{election_compliance}
{keyword_strategy}
{number_validation['instruction']}
{region_scope_instruction}

<topic_priority priority="highest">
  <instruction>ğŸš¨ ì œëª©ì˜ ë°©í–¥ì€ ë°˜ë“œì‹œ ì£¼ì œ(topic)ë¥¼ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤. ë³¸ë¬¸ ë‚´ìš©ì´ ì•„ë¬´ë¦¬ ë§ì•„ë„ topicì´ ì ˆëŒ€ ìš°ì„ .</instruction>
  <rules>
    <rule>ì£¼ì œê°€ "í›„ì›"ì´ë©´ ì œëª©ë„ í›„ì›/ì‘ì›/í•¨ê»˜ì— ê´€í•œ ê²ƒì´ì–´ì•¼ í•¨ â€” ê²½ì œ/AI/ì •ì±…ìœ¼ë¡œ ë¹ ì§€ë©´ ì•ˆ ë¨</rule>
    <rule>ì£¼ì œê°€ "ì›ì¹™"ì´ë©´ ì œëª©ë„ ì›ì¹™/í’ˆê²©ì— ê´€í•œ ê²ƒì´ì–´ì•¼ í•¨</rule>
    <rule>ë³¸ë¬¸(content_preview)ì€ ë°°ê²½ ì •ë³´ì¼ ë¿, ì œëª© ë°©í–¥ì„ ê²°ì •í•˜ì§€ ì•ŠìŒ</rule>
    <rule>ì£¼ì œ í‚¤ì›Œë“œë¥¼ ì „ë¶€ ë„£ì„ í•„ìš”ëŠ” ì—†ì§€ë§Œ, ì£¼ì œì˜ í•µì‹¬ í–‰ë™/ìš”ì²­ì€ ë°˜ë“œì‹œ ë°˜ì˜</rule>
  </rules>
  <example>
    <topic>ì›ì¹™ê³¼ í’ˆê²©, ë¶€ì‚°ì‹œì¥ ì˜ˆë¹„í›„ë³´ ì´ì¬ì„± í›„ì›</topic>
    <good>ë¶€ì‚° ì§€ë°©ì„ ê±°, ì´ì¬ì„±ì—ê²Œ í˜ì„ ë³´íƒœëŠ” ë°©ë²•</good>
    <bad reason="ì£¼ì œ ì´íƒˆ â€” í›„ì›ì´ ì£¼ì œì¸ë° ê²½ì œë¡œ ë¹ ì§">ë¶€ì‚° ì§€ë°©ì„ ê±°, ê²½ì œ 0.7% ëŠªì—ì„œ ì´ì¬ì„±ì´ êº¼ë‚¸ ë¹„ì±…ì€</bad>
  </example>
</topic_priority>

<output_rules>
  <rule>ğŸš¨ {TITLE_LENGTH_HARD_MAX}ì ì´ë‚´ í•„ìˆ˜</rule>
  <rule>{TITLE_LENGTH_OPTIMAL_MIN}-{TITLE_LENGTH_OPTIMAL_MAX}ì ê¶Œì¥</rule>
  <rule>ìŠ¬ë¡¯ í”Œë ˆì´ìŠ¤í™€ë”([í–‰ì‚¬ëª…] ë“±) ì¶œë ¥ ê¸ˆì§€</rule>
  <rule>ë§ì¤„ì„í‘œ ê¸ˆì§€</rule>
  <rule>í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨</rule>
  <rule>ë³¸ë¬¸ì— ì‹¤ì œ ë“±ì¥í•˜ëŠ” ìˆ«ìë§Œ ì‚¬ìš©</rule>
  <rule>ì§€ë£¨í•œ í‘œí˜„ ê¸ˆì§€</rule>
</output_rules>

<output_format>ìˆœìˆ˜í•œ ì œëª© í…ìŠ¤íŠ¸ë§Œ. ë”°ì˜´í‘œ ì œì™¸.</output_format>

</title_generation_prompt>
"""

def extract_topic_keywords(topic: str) -> List[str]:
    keywords = []
    try:
        # Names (simple heuristic for Korean names)
        name_matches = re.findall(r'[ê°€-í£]{2,4}(?=\s*(?:ì˜ì›|ì‹œì¥|êµ¬ì²­ì¥|ëŒ€í†µë ¹|ì´ë¦¬|ì¥ê´€|ëŒ€í‘œ)?(?:$|\s))', topic)
        if name_matches:
            keywords.extend(name_matches[:3])
            
        action_keywords = ['ì¹­ì°¬', 'ì§ˆíƒ€', 'ë¹„íŒ', 'ë…¼í‰', 'ë°œì–¸', 'ì†Œì‹ ', 'ì¹¨ë¬µ', 'ì‚¬í˜•', 'êµ¬í˜•', 'í˜‘ë ¥', 'ëŒ€ë¦½']
        for action in action_keywords:
            if action in topic:
                keywords.append(action)
                
        number_matches = re.findall(r'\d+(?:ì–µ|ë§Œì›|%|ëª…|ê±´)?', topic)
        if number_matches:
            keywords.extend(number_matches[:2])
    except:
        pass
        
    return list(set(keywords))

def validate_theme_and_content(topic: str, content: str, title: str = '') -> Dict[str, Any]:
    try:
        if not topic or not content:
            return {
                'isValid': False,
                'mismatchReasons': ['ì£¼ì œ ë˜ëŠ” ë³¸ë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤'],
                'topicKeywords': [],
                'contentKeywords': [],
                'overlapScore': 0
            }
            
        topic_keywords = extract_topic_keywords(topic)
        content_lower = content.lower()
        matched_keywords = []
        missing_keywords = []
        
        for kw in topic_keywords:
            if kw.lower() in content_lower:
                matched_keywords.append(kw)
            else:
                missing_keywords.append(kw)
                
        overlap_score = round(len(matched_keywords) / len(topic_keywords) * 100) if topic_keywords else 0
        mismatch_reasons = []
        
        if overlap_score < 50:
             mismatch_reasons.append(f"ì£¼ì œ í•µì‹¬ì–´ ì¤‘ {len(missing_keywords)}ê°œê°€ ë³¸ë¬¸ì— ì—†ìŒ: {', '.join(missing_keywords)}")
             
        if title:
            title_lower = title.lower()
            title_missing = [kw for kw in topic_keywords if kw.lower() not in title_lower]
            if len(title_missing) > len(topic_keywords) * 0.5:
                 mismatch_reasons.append(f"ì œëª©ì— ì£¼ì œ í•µì‹¬ì–´ ë¶€ì¡±: {', '.join(title_missing[:3])}")
                 
        return {
            'isValid': overlap_score >= 50 and not mismatch_reasons,
            'mismatchReasons': mismatch_reasons,
            'topicKeywords': topic_keywords,
            'matchedKeywords': matched_keywords,
            'missingKeywords': missing_keywords,
            'overlapScore': overlap_score
        }
    except:
        return {'isValid': True, 'overlapScore': 100, 'mismatchReasons': []}

def calculate_title_quality_score(title: str, params: Dict[str, Any]) -> Dict[str, Any]:
    # No try/except blocking logic here. Let it propagate.
    topic = params.get('topic', '')
    content = params.get('contentPreview', '')
    user_keywords = params.get('userKeywords', [])
    author_name = params.get('fullName', '')
    
    if not title:
        return {'score': 0, 'breakdown': {}, 'passed': False, 'suggestions': ['ì œëª©ì´ ì—†ìŠµë‹ˆë‹¤']}
        
    # 0. Critical Failure Checks
    has_html_tag = bool(re.search(r'<\s*/?\s*[a-zA-Z][^>]*>', title))
    has_slot_placeholder = any(f'[{name}]' in title for name in SLOT_PLACEHOLDER_NAMES)
    looks_like_content = (
        'ì—¬ëŸ¬ë¶„' in title or
        has_html_tag or
        has_slot_placeholder or
        title.endswith('ì…ë‹ˆë‹¤') or
        title.endswith('ìŠµë‹ˆë‹¤') or
        title.endswith('ìŠµë‹ˆê¹Œ') or
        title.endswith('ë‹ˆë‹¤') or
        len(title) > 50
    )
    
    if looks_like_content:
        reason = (
            'í˜¸ì¹­("ì—¬ëŸ¬ë¶„") í¬í•¨' if 'ì—¬ëŸ¬ë¶„' in title else
            ('HTML íƒœê·¸ í¬í•¨' if has_html_tag else
             ('ìŠ¬ë¡¯ í”Œë ˆì´ìŠ¤í™€ë” í¬í•¨' if has_slot_placeholder else
              ('50ì ì´ˆê³¼' if len(title) > 50 else 'ì„œìˆ í˜• ì¢…ê²°ì–´ë¯¸')))
        )
        return {
            'score': 0,
            'breakdown': {'contentPattern': {'score': 0, 'max': 100, 'status': 'ì‹¤íŒ¨', 'reason': reason}},
            'passed': False,
            'suggestions': [f'ì œëª©ì´ ë³¸ë¬¸ì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤ ({reason}). ê²€ìƒ‰ì–´ ì¤‘ì‹¬ì˜ ê°„ê²°í•œ ì œëª©ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”.']
        }
        
    if '...' in title or title.endswith('..'):
             return {
            'score': 0,
            'breakdown': {'ellipsis': {'score': 0, 'max': 100, 'status': 'ì‹¤íŒ¨', 'reason': 'ë§ì¤„ì„í‘œ í¬í•¨'}},
            'passed': False,
            'suggestions': ['ë§ì¤„ì„í‘œ("...") ì‚¬ìš© ê¸ˆì§€. ë‚´ìš©ì„ ìë¥´ì§€ ë§ê³  ì™„ê²°ëœ ì œëª©ì„ ì‘ì„±í•˜ì„¸ìš”.']
        }

    title_purpose = resolve_title_purpose(topic, params)
    if title_purpose == 'event_announcement':
        event_validation = validate_event_announcement_title(title, params)
        if not event_validation.get('passed'):
            return {
                'score': 0,
                'breakdown': {
                    'eventPurpose': {
                        'score': 0,
                        'max': 100,
                        'status': 'ì‹¤íŒ¨',
                        'reason': str(event_validation.get('reason') or 'í–‰ì‚¬ ì•ˆë‚´ ëª©ì  ë¶ˆì¼ì¹˜')
                    }
                },
                'passed': False,
                'suggestions': [str(event_validation.get('reason') or 'í–‰ì‚¬ ì•ˆë‚´ ëª©ì ì— ë§ê²Œ ì œëª©ì„ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”.')]
            }

    event_anchor_context: Dict[str, Any] = {
        'dateHint': '',
        'bookTitle': '',
        'authorName': '',
    }
    if title_purpose == 'event_announcement':
        context_analysis = params.get('contextAnalysis') if isinstance(params.get('contextAnalysis'), dict) else {}
        must_preserve = context_analysis.get('mustPreserve') if isinstance(context_analysis.get('mustPreserve'), dict) else {}
        event_date = str(must_preserve.get('eventDate') or '').strip()
        event_anchor_context = {
            'dateHint': _extract_date_hint(event_date) or _extract_date_hint(topic),
            'bookTitle': _extract_book_title(topic, params),
            'authorName': str(author_name or '').strip(),
        }
        
    breakdown = {}
    suggestions = []
    title_length = len(title)
    
    # Hard fail length check
    if title_length < TITLE_LENGTH_HARD_MIN or title_length > TITLE_LENGTH_HARD_MAX:
             return {
            'score': 0,
            'breakdown': {'length': {'score': 0, 'max': 100, 'status': 'ì‹¤íŒ¨', 'reason': f'{title_length}ì ({TITLE_LENGTH_HARD_MIN}-{TITLE_LENGTH_HARD_MAX}ì í•„ìš”)'}},
            'passed': False,
            'suggestions': [f'ì œëª©ì´ {title_length}ìì…ë‹ˆë‹¤. {TITLE_LENGTH_HARD_MIN}-{TITLE_LENGTH_HARD_MAX}ì ë²”ìœ„ë¡œ ì‘ì„±í•˜ì„¸ìš”.']
        }

    # 1. Length Score (Max 20)
    if TITLE_LENGTH_OPTIMAL_MIN <= title_length <= TITLE_LENGTH_OPTIMAL_MAX:
        breakdown['length'] = {'score': 20, 'max': 20, 'status': 'ìµœì '}
    elif TITLE_LENGTH_HARD_MIN <= title_length < TITLE_LENGTH_OPTIMAL_MIN:
        breakdown['length'] = {'score': 12, 'max': 20, 'status': 'ì§§ìŒ'}
        suggestions.append(f'ì œëª©ì´ {title_length}ìì…ë‹ˆë‹¤. {TITLE_LENGTH_OPTIMAL_MIN}ì ì´ìƒ ê¶Œì¥.')
    elif TITLE_LENGTH_OPTIMAL_MAX < title_length <= TITLE_LENGTH_HARD_MAX:
        breakdown['length'] = {'score': 12, 'max': 20, 'status': 'ê²½ê³„'}
        suggestions.append(f'ì œëª©ì´ {title_length}ìì…ë‹ˆë‹¤. {TITLE_LENGTH_OPTIMAL_MAX}ì ì´í•˜ê°€ í´ë¦­ë¥  ìµœê³ .')
    else:
        breakdown['length'] = {'score': 0, 'max': 20, 'status': 'ë¶€ì ì •'}
        suggestions.append(f'ì œëª©ì´ {title_length}ìì…ë‹ˆë‹¤. {TITLE_LENGTH_OPTIMAL_MIN}-{TITLE_LENGTH_OPTIMAL_MAX}ì ë²”ìœ„ë¡œ ì‘ì„±í•˜ì„¸ìš”.')
        
    # 2. Keyword Position (Max 20)
    if user_keywords:
        # Check positions
        keyword_infos = []
        for kw in user_keywords:
            idx = title.find(kw)
            keyword_infos.append({
                'keyword': kw,
                'index': idx,
                'inFront10': 0 <= idx <= 10
            })
            
        any_in_front10 = any(k['inFront10'] for k in keyword_infos)
        any_in_title = any(k['index'] >= 0 for k in keyword_infos)
        front_keyword = next((k['keyword'] for k in keyword_infos if k['inFront10']), '')
        any_keyword = next((k['keyword'] for k in keyword_infos if k['index'] >= 0), '')
        
        # í‚¤ì›Œë“œ ë’¤ êµ¬ë¶„ì ê²€ì¦: ì‰¼í‘œ, ë¬¼ìŒí‘œ, ì¡°ì‚¬ ë“±ìœ¼ë¡œ ë¶„ë¦¬ë˜ì–´ì•¼ í•¨
        # ë‹¨, ìœ ì‚¬ í‚¤ì›Œë“œê°€ ì¤‘ì²©ë˜ëŠ” ê²½ìš°(ì˜ˆ: "ë¶€ì‚° ë””ì¦ˆë‹ˆëœë“œ ìœ ì¹˜" / "ë¶€ì‚° ë””ì¦ˆë‹ˆëœë“œ")
        # ì§§ì€ í‚¤ì›Œë“œì˜ ì¤‘ê°„ ë§¤ì¹­ì€ êµ¬ë¶„ì ê²€ì¦ì—ì„œ ì œì™¸í•œë‹¤.
        matched_spans = []
        for info in keyword_infos:
            idx = int(info.get('index', -1))
            keyword = str(info.get('keyword') or '')
            if idx < 0 or not keyword:
                continue
            matched_spans.append({
                'keyword': keyword,
                'start': idx,
                'end': idx + len(keyword),
            })

        kw_delimiter_ok = True
        delimiters = (',', '?', '!', '.', 'ì—', 'ì˜', 'ì„', 'ë¥¼', 'ì€', 'ëŠ”', 'ì´', 'ê°€', ':', ' ')
        for span in matched_spans:
            is_shadowed = any(
                other['start'] == span['start'] and other['end'] > span['end']
                for other in matched_spans
            )
            if is_shadowed:
                continue

            end_pos = span['end']
            if end_pos >= len(title):
                continue

            next_char = title[end_pos]
            if next_char not in delimiters:
                kw_delimiter_ok = False
                continue

            if next_char == ' ':
                # ê³µë°± ë’¤ì— ë°”ë¡œ í•œê¸€(ì´ë¦„ ë“±)ì´ ì˜¤ë©´ êµ¬ë¶„ì ë¶€ì¡±
                if end_pos + 1 < len(title) and '\uac00' <= title[end_pos + 1] <= '\ud7a3':
                    kw_delimiter_ok = False

        # ë“€ì–¼ í‚¤ì›Œë“œ ë°°ì¹˜ ê²€ì¦: 1ë²ˆ í‚¤ì›Œë“œê°€ ì œëª© ì‹œì‘ì— ìˆëŠ”ì§€
        dual_kw_bonus = 0
        dual_kw_penalty = 0
        if len(user_keywords) >= 2:
            kw1 = user_keywords[0]
            kw1_idx = title.find(kw1)
            kw1_starts_title = 0 <= kw1_idx <= 2  # ì œëª© ë§¨ ì•(0~2ì ë‚´)
            if kw1_starts_title:
                dual_kw_bonus = 3
            elif kw1_idx < 0:
                dual_kw_penalty = 5
                suggestions.append(f'1ìˆœìœ„ í‚¤ì›Œë“œ "{kw1}"ê°€ ì œëª©ì— ì—†ìŠµë‹ˆë‹¤. ì œëª© ì‹œì‘ ë¶€ë¶„ì— ë°°ì¹˜í•˜ì„¸ìš”.')

            # 2ë²ˆ í‚¤ì›Œë“œ: ìœ ì‚¬ë©´ ì–´ì ˆ í•´ì²´ ì¶©ì¡±, ë…ë¦½ì´ë©´ í¬í•¨ ì—¬ë¶€
            kw2 = user_keywords[1]
            similar = are_keywords_similar(kw1, kw2)
            if similar:
                kw2_words = kw2.split()
                kw1_words = kw1.split()
                unique_words = [w for w in kw2_words if w not in kw1_words and len(w) >= 2]
                has_unique = len(unique_words) == 0 or any(w in title for w in unique_words)
                if not has_unique:
                    dual_kw_penalty += 3
                    suggestions.append(f'2ìˆœìœ„ í‚¤ì›Œë“œ "{kw2}"ì˜ ê³ ìœ  ì–´ì ˆ({", ".join(unique_words)})ì´ ì œëª©ì— ì—†ìŠµë‹ˆë‹¤.')
            else:
                if kw2 not in title:
                    dual_kw_penalty += 3
                    suggestions.append(f'2ìˆœìœ„ í‚¤ì›Œë“œ "{kw2}"ê°€ ì œëª©ì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')

        if any_in_front10:
            score = min(20, max(0, (20 if kw_delimiter_ok else 15) + dual_kw_bonus - dual_kw_penalty))
            status = 'ìµœì ' if kw_delimiter_ok else 'ìµœì (êµ¬ë¶„ì ë¶€ì¡±)'
            breakdown['keywordPosition'] = {'score': score, 'max': 20, 'status': status, 'keyword': front_keyword}
            if not kw_delimiter_ok:
                suggestions.append(f'í‚¤ì›Œë“œ "{front_keyword}" ë’¤ì— ì‰¼í‘œë‚˜ ì¡°ì‚¬ë¥¼ ë„£ì–´ ë‹¤ìŒ ë‹¨ì–´ì™€ ë¶„ë¦¬í•˜ì„¸ìš”. (ì˜ˆ: "ë¶€ì‚° ì§€ë°©ì„ ê±°, ~")')
        elif any_in_title:
            score = max(0, 12 - dual_kw_penalty)
            breakdown['keywordPosition'] = {'score': score, 'max': 20, 'status': 'í¬í•¨ë¨', 'keyword': any_keyword}
            suggestions.append(f'í‚¤ì›Œë“œ "{any_keyword}"ë¥¼ ì œëª© ì•ìª½(10ì ë‚´)ìœ¼ë¡œ ì´ë™í•˜ë©´ SEO íš¨ê³¼ ì¦ê°€.')
        else:
            breakdown['keywordPosition'] = {'score': 0, 'max': 20, 'status': 'ì—†ìŒ'}
            suggestions.append(f'í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ ì œëª©ì— í¬í•¨í•˜ì„¸ìš”: {", ".join(user_keywords[:2])}')
    else:
        breakdown['keywordPosition'] = {'score': 10, 'max': 20, 'status': 'í‚¤ì›Œë“œì—†ìŒ'}
             
    # 3. Numbers Score (Max 15)
    has_numbers = bool(re.search(r'\d+(?:ì–µ|ë§Œì›|%|ëª…|ê±´|ê°€êµ¬|ê³³)?', title))
    if has_numbers:
        content_numbers_res = extract_numbers_from_content(content)
        safe_content_numbers = content_numbers_res.get('numbers', [])
        content_number_tokens = [_normalize_digit_token(c_num) for c_num in safe_content_numbers]

        allowed_event_tokens: set[str] = set()
        if title_purpose == 'event_announcement':
            allowed_event_tokens.update(_extract_digit_tokens(topic))
            allowed_event_tokens.update(_extract_digit_tokens(event_anchor_context.get('dateHint', '')))

        title_numbers = re.findall(r'\d+(?:ì–µ|ë§Œì›|%|ëª…|ê±´|ê°€êµ¬|ê³³)?', title)

        # Check if all title numbers exist in content (fuzzy match)
        all_valid = True
        for t_num in title_numbers:
            t_val = _normalize_digit_token(t_num)
            if not t_val:
                continue

            # Check if t_val exists inside any content number OR any content number exists inside t_val
            in_content = any(
                t_val in c_token or c_token in t_val
                for c_token in content_number_tokens
                if c_token
            )
            in_event_hint = t_val in allowed_event_tokens
            if not in_content and not in_event_hint:
                all_valid = False
                break

        if all_valid:
                breakdown['numbers'] = {'score': 15, 'max': 15, 'status': 'ê²€ì¦ë¨'}
        else:
                breakdown['numbers'] = {'score': 5, 'max': 15, 'status': 'ë¯¸ê²€ì¦'}
                suggestions.append('ì œëª©ì˜ ìˆ«ìê°€ ë³¸ë¬¸ì—ì„œ í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
    else:
        breakdown['numbers'] = {'score': 8, 'max': 15, 'status': 'ì—†ìŒ'}
        
    # 4. Topic Match (Max 25)
    if topic:
        theme_val = validate_theme_and_content(topic, content, title)
        if theme_val['overlapScore'] >= 80:
            breakdown['topicMatch'] = {'score': 25, 'max': 25, 'status': 'ë†’ìŒ', 'overlap': theme_val['overlapScore']}
        elif theme_val['overlapScore'] >= 50:
            breakdown['topicMatch'] = {'score': 15, 'max': 25, 'status': 'ë³´í†µ', 'overlap': theme_val['overlapScore']}
            if theme_val['mismatchReasons']:
                suggestions.append(theme_val['mismatchReasons'][0])
        else:
            breakdown['topicMatch'] = {'score': 5, 'max': 25, 'status': 'ë‚®ìŒ', 'overlap': theme_val['overlapScore']}
            suggestions.append('ì œëª©ì´ ì£¼ì œì™€ ë§ì´ ë‹¤ë¦…ë‹ˆë‹¤. ì£¼ì œ í•µì‹¬ì–´ë¥¼ ë°˜ì˜í•˜ì„¸ìš”.')
    else:
        breakdown['topicMatch'] = {'score': 15, 'max': 25, 'status': 'ì£¼ì œì—†ìŒ'}
        
    # 5. Author Inclusion (Max 10)
    if author_name:
        category_text = str(params.get('category') or '').strip().lower()
        commentary_purposes = {'commentary', 'issue_analysis', 'current_affairs'}
        commentary_categories = {'current-affairs', 'bipartisan-cooperation'}
        prefers_relationship_style = (
            title_purpose in commentary_purposes or category_text in commentary_categories
        )

        if author_name in title:
            speaker_patterns = [
                f"{author_name}ì´ ë³¸", f"{author_name}ê°€ ë³¸", f"{author_name}ì˜ í‰ê°€", f"{author_name}ì˜ ì‹œê°",
                f"ì¹­ì°¬í•œ {author_name}", f"ì§ˆíƒ€í•œ {author_name}", f"{author_name} [\"'`]"
            ]
            has_pattern = any(re.search(p, title) for p in speaker_patterns)

            if prefers_relationship_style:
                if has_pattern:
                    breakdown['authorIncluded'] = {'score': 10, 'max': 10, 'status': 'íŒ¨í„´ ì ìš©'}
                else:
                    breakdown['authorIncluded'] = {'score': 6, 'max': 10, 'status': 'ë‹¨ìˆœ í¬í•¨'}
                    suggestions.append(f'"{author_name}ì´ ë³¸", "ì¹­ì°¬í•œ {author_name}" ë“± ê´€ê³„í˜• í‘œí˜„ ê¶Œì¥.')
            else:
                breakdown['authorIncluded'] = {'score': 10, 'max': 10, 'status': 'í¬í•¨'}
        else:
            # í–‰ì‚¬ ì•ˆë‚´í˜• ì œëª©ì€ ì¸ë¬¼ëª… ëˆ„ë½ì„ ì¹˜ëª… ê°ì ìœ¼ë¡œ ë³´ì§€ ì•ŠëŠ”ë‹¤.
            if title_purpose == 'event_announcement':
                breakdown['authorIncluded'] = {'score': 6, 'max': 10, 'status': 'í–‰ì‚¬í˜• ì˜ˆì™¸'}
            elif prefers_relationship_style:
                breakdown['authorIncluded'] = {'score': 0, 'max': 10, 'status': 'ë¯¸í¬í•¨'}
                suggestions.append(f'í™”ì "{author_name}"ë¥¼ ì œëª©ì— í¬í•¨í•˜ë©´ ë¸Œëœë”©ì— ë„ì›€ë©ë‹ˆë‹¤.')
            else:
                breakdown['authorIncluded'] = {'score': 5, 'max': 10, 'status': 'ì„ íƒ'}
    else:
        breakdown['authorIncluded'] = {'score': 5, 'max': 10, 'status': 'í•´ë‹¹ì—†ìŒ'}

    # í–‰ì‚¬í˜• ì œëª©ì€ ê³ ìœ  ì•µì»¤(ë‚ ì§œ/ì¸ë¬¼ëª…/ë„ì„œëª…)ë¥¼ ê°€ì‚°í•´
    # ì‚¬ìš©ì few-shot ê¸°ë°˜ì˜ êµ¬ì²´ ì œëª©ì´ ì ìˆ˜ì—ì„œ ë¶ˆë¦¬í•˜ì§€ ì•Šë„ë¡ ë³´ì •í•œë‹¤.
    if title_purpose == 'event_announcement':
        anchor_score = 0
        matched_anchors: List[str] = []
        date_hint = str(event_anchor_context.get('dateHint') or '')
        if date_hint and _contains_date_hint(title, date_hint):
            anchor_score += 4
            matched_anchors.append('date')
        book_title = str(event_anchor_context.get('bookTitle') or '').strip()
        if book_title:
            book_tokens = _split_hint_tokens(book_title)
            if book_tokens and any(token in title for token in book_tokens):
                anchor_score += 3
                matched_anchors.append('book')
        author_hint = str(event_anchor_context.get('authorName') or '').strip()
        if author_hint and author_hint in title:
            anchor_score += 3
            matched_anchors.append('author')

        breakdown['eventAnchors'] = {
            'score': min(anchor_score, 10),
            'max': 10,
            'status': 'ì¶©ë¶„' if anchor_score >= 6 else ('ë³´í†µ' if anchor_score >= 3 else 'ë¶€ì¡±'),
            'matched': matched_anchors,
        }
        if anchor_score == 0:
            suggestions.append('í–‰ì‚¬ ê³ ìœ  ì •ë³´(ë‚ ì§œ/ì¸ë¬¼ëª…/ë„ì„œëª…)ë¥¼ 1ê°œ ì´ìƒ ë„£ìœ¼ë©´ í’ˆì§ˆ ì ìˆ˜ê°€ ìƒìŠ¹í•©ë‹ˆë‹¤.')

    # 6. Impact (Max 10) - ì„œì‚¬ì  ê¸´ì¥ê° íŒ¨í„´ í¬í•¨
    impact_score = 0
    impact_features = []

    if '?' in title or title.endswith('ë‚˜') or title.endswith('ê¹Œ'):
        impact_score += 3
        impact_features.append('ì§ˆë¬¸/ë¯¸ì™„ê²°')
    if re.search(r"'.*'|\".*\"", title):
        impact_score += 3
        impact_features.append('ì¸ìš©ë¬¸')
    if re.search(r"vs|\bvs\b|â†’|ëŒ€ë¹„", title):
        impact_score += 2
        impact_features.append('ëŒ€ë¹„êµ¬ì¡°')
    if re.search(r"ì´ ë³¸|ê°€ ë³¸", title):
        impact_score += 2
        impact_features.append('ê´€ì í‘œí˜„')
    # ì„œì‚¬ì  ê¸´ì¥ê° íŒ¨í„´
    if re.search(r'(ì€|ëŠ”|ì¹´ë“œëŠ”|ë‹µì€|ì„ íƒ|í•œ ìˆ˜|ì´ìœ )$', title):
        impact_score += 2
        impact_features.append('ë¯¸ì™„ê²°ì„œì‚¬')
    if re.search(r'ì—ì„œ.*ê¹Œì§€', title):
        impact_score += 2
        impact_features.append('ì„œì‚¬ì•„í¬')
    if re.search(r'ì™œ\s|ì–´ë–»ê²Œ\s', title):
        impact_score += 2
        impact_features.append('ì›ì¸ì§ˆë¬¸')
    # ì •ë³´ ê³¼ë°€ íŒ¨ë„í‹°: ì‹¤ì§ˆ ìš”ì†Œ(2ê¸€ì ì´ìƒ ë‹¨ì–´)ê°€ 7ê°œ ì´ìƒì´ë©´ ê°ì 
    substantive_elements = [e for e in re.findall(r'[ê°€-í£A-Za-z0-9]{2,}', title)]
    if len(substantive_elements) >= 7:
        impact_score -= 2
        impact_features.append('ì •ë³´ê³¼ë°€(-2)')
    if title_purpose == 'event_announcement':
        if any(token in title for token in ('í˜„ì¥', 'ì§ì ‘', 'ì¼ì •', 'ì•ˆë‚´', 'ì´ˆëŒ€', 'ë§Œë‚¨', 'ì°¸ì„')):
            impact_score += 3
            impact_features.append('í–‰ì‚¬í˜•í›„í‚¹')
        
    breakdown['impact'] = {
        'score': min(impact_score, 10),
        'max': 10,
        'status': 'ìˆìŒ' if impact_score > 0 else 'ì—†ìŒ',
        'features': impact_features
    }
    
    # Total Score
    total_score = sum(item.get('score', 0) for item in breakdown.values())
    max_possible = sum(item.get('max', 0) for item in breakdown.values())
    
    # Normalize to 100
    normalized_score = round(total_score / max_possible * 100) if max_possible > 0 else 0
    
    return {
        'score': normalized_score,
        'rawScore': total_score,
        'maxScore': max_possible,
        'breakdown': breakdown,
        'passed': normalized_score >= 70,
        'suggestions': suggestions[:3]
    }

async def generate_and_validate_title(generate_fn, params: Dict[str, Any], options: Dict[str, Any] = {}) -> Dict[str, Any]:
    min_score = int(options.get('minScore', 70))
    max_attempts = int(options.get('maxAttempts', 3))
    candidate_count = max(1, int(options.get('candidateCount', 5)))
    soft_accept_min_score = max(0, int(options.get('softAcceptMinScore', 60)))
    similarity_threshold = float(options.get('similarityThreshold', 0.78))
    similarity_threshold = min(max(similarity_threshold, 0.50), 0.95)
    max_similarity_penalty = max(0, int(options.get('maxSimilarityPenalty', 18)))
    on_progress = options.get('onProgress')

    option_recent_titles = options.get('recentTitles') if isinstance(options.get('recentTitles'), list) else []
    param_recent_titles = params.get('recentTitles') if isinstance(params.get('recentTitles'), list) else []
    recent_titles: List[str] = []
    seen_recent_titles = set()
    for value in option_recent_titles + param_recent_titles:
        title = str(value or '').strip()
        if not title or title in seen_recent_titles:
            continue
        seen_recent_titles.add(title)
        recent_titles.append(title)

    history = []
    best_title = ''
    best_score = -1
    best_result = None
    title_purpose = resolve_title_purpose(str(params.get('topic') or ''), params)

    for attempt in range(1, max_attempts + 1):
        if on_progress:
            on_progress({
                'attempt': attempt,
                'maxAttempts': max_attempts,
                'status': 'generating',
                'candidateCount': candidate_count
            })

        # 1. Prompt generation
        prompt = ""
        if attempt == 1 or not history:
            prompt = build_title_prompt(params)
        else:
            last_attempt = history[-1]
            feedback_prompt = f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ ì´ì „ ì œëª© í”¼ë“œë°± (ì ìˆ˜: {last_attempt.get('score', 0)}/100)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì´ì „ ì œëª©: "{last_attempt.get('title', '')}"
ë¬¸ì œì :
{chr(10).join([f'â€¢ {s}' for s in last_attempt.get('suggestions', [])])}

ìœ„ ë¬¸ì œë¥¼ í•´ê²°í•œ ìƒˆë¡œìš´ ì œëª©ì„ ì‘ì„±í•˜ì„¸ìš”.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""
            prompt = feedback_prompt + build_title_prompt(params)

        disallow_titles = list(recent_titles)
        disallow_titles.extend([
            str(item.get('title') or '').strip()
            for item in history
            if isinstance(item, dict) and str(item.get('title') or '').strip()
        ])

        candidate_prompts = [
            _build_title_candidate_prompt(
                prompt,
                attempt,
                idx + 1,
                candidate_count,
                disallow_titles,
                title_purpose,
            )
            for idx in range(candidate_count)
        ]

        # 2. Multi-candidate generation
        if candidate_count == 1:
            responses = [await generate_fn(candidate_prompts[0])]
        else:
            responses = await asyncio.gather(
                *[generate_fn(candidate_prompt) for candidate_prompt in candidate_prompts],
                return_exceptions=True,
            )

        generation_errors: List[str] = []
        candidate_results: List[Dict[str, Any]] = []
        for idx, response in enumerate(responses, start=1):
            if isinstance(response, Exception):
                err = str(response)
                generation_errors.append(err)
                logger.warning("[TitleGen] í›„ë³´ %s ìƒì„± ì‹¤íŒ¨ (attempt=%s): %s", idx, attempt, err)
                continue

            raw_generated_title = str(response or '').strip().strip('"\'')
            generated_title = _normalize_generated_title(raw_generated_title, params)
            if raw_generated_title != generated_title:
                logger.info(
                    "[TitleGen] ì œëª© ì •ê·œí™” ì ìš©(í›„ë³´ %s): raw=\"%s\" -> normalized=\"%s\"",
                    idx,
                    raw_generated_title,
                    generated_title,
                )

            if not generated_title:
                continue

            score_result = calculate_title_quality_score(generated_title, params)
            similarity_meta = _compute_similarity_penalty(
                generated_title,
                disallow_titles,
                threshold=similarity_threshold,
                max_penalty=max_similarity_penalty,
            )
            adjusted_score = max(0, int(score_result.get('score', 0)) - int(similarity_meta.get('penalty', 0)))

            candidate_results.append({
                'candidateIndex': idx,
                'title': generated_title,
                'rawTitle': raw_generated_title,
                'baseScore': int(score_result.get('score', 0)),
                'adjustedScore': adjusted_score,
                'scoreResult': score_result,
                'similarityMeta': similarity_meta,
            })

        if not candidate_results:
            if generation_errors and len(generation_errors) == len(candidate_prompts):
                raise RuntimeError(
                    f"[TitleGen] ì œëª© ìƒì„± ì‹¤íŒ¨: attempt {attempt}ì—ì„œ í›„ë³´ {candidate_count}ê°œ ìƒì„±ì´ ëª¨ë‘ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. "
                    f"ì²« ì˜¤ë¥˜: {generation_errors[0]}"
                )

            history.append({
                'attempt': attempt,
                'title': '',
                'score': 0,
                'suggestions': ['í›„ë³´ ì œëª©ì´ ëª¨ë‘ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ë˜ëŠ” ëª¨ë¸ ì‘ë‹µì„ í™•ì¸í•˜ì„¸ìš”.'],
                'breakdown': {'empty': {'score': 0, 'max': 100, 'status': 'ì‹¤íŒ¨'}},
                'candidateCount': candidate_count,
            })
            continue

        selected = max(
            candidate_results,
            key=lambda item: (item.get('adjustedScore', 0), item.get('baseScore', 0)),
        )
        selected_score_result = selected.get('scoreResult', {})
        selected_similarity = selected.get('similarityMeta', {})
        selected_suggestions = list(selected_score_result.get('suggestions', []))
        if int(selected_similarity.get('penalty', 0)) > 0:
            selected_suggestions.append(
                f"ì´ì „ ì œëª©ê³¼ ìœ ì‚¬ë„ {selected_similarity.get('maxSimilarity', 0)}ë¡œ "
                f"{selected_similarity.get('penalty', 0)}ì  ê°ì "
            )

        selected_breakdown = dict(selected_score_result.get('breakdown', {}))
        selected_breakdown['diversityPenalty'] = {
            'score': int(selected_similarity.get('penalty', 0)),
            'max': max_similarity_penalty,
            'status': 'ì ìš©' if int(selected_similarity.get('penalty', 0)) > 0 else 'ì—†ìŒ',
            'similarity': selected_similarity.get('maxSimilarity', 0),
            'against': selected_similarity.get('against', ''),
        }

        history_item = {
            'attempt': attempt,
            'title': selected.get('title', ''),
            'score': selected.get('adjustedScore', 0),
            'baseScore': selected.get('baseScore', 0),
            'candidateCount': candidate_count,
            'selectedCandidate': selected.get('candidateIndex', 1),
            'similarityPenalty': int(selected_similarity.get('penalty', 0)),
            'similarity': selected_similarity.get('maxSimilarity', 0),
            'suggestions': selected_suggestions[:4],
            'breakdown': selected_breakdown,
        }
        if selected.get('rawTitle') != selected.get('title'):
            history_item['rawTitle'] = selected.get('rawTitle', '')
        history.append(history_item)

        current_score = int(selected.get('adjustedScore', 0))
        if best_result is None or current_score > best_score:
            best_score = current_score
            best_title = str(selected.get('title') or '')
            best_result = history_item

        if current_score >= min_score:
            if on_progress:
                on_progress({
                    'attempt': attempt,
                    'maxAttempts': max_attempts,
                    'status': 'passed',
                    'score': current_score,
                    'baseScore': selected.get('baseScore', 0),
                    'candidateCount': candidate_count
                })

            return {
                'title': selected.get('title', ''),
                'score': current_score,
                'baseScore': selected.get('baseScore', 0),
                'similarityPenalty': int(selected_similarity.get('penalty', 0)),
                'attempts': attempt,
                'passed': True,
                'history': history,
                'breakdown': selected_breakdown,
            }

    if on_progress:
        on_progress({
            'attempt': max_attempts,
            'maxAttempts': max_attempts,
            'status': 'failed',
            'score': max(best_score, 0),
            'candidateCount': candidate_count
        })

    if best_result is None:
        raise RuntimeError(
            f"[TitleGen] ì œëª© ìƒì„± ì‹¤íŒ¨: {max_attempts}íšŒ ì‹œë„ ëª¨ë‘ ìœ íš¨í•œ ì œëª©ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        )

    if title_purpose == 'event_announcement':
        logger.warning(
            "[TitleGen] event_announcement soft-accept: score=%s < minScore=%s, title=%s",
            best_score,
            min_score,
            best_title,
        )
        return {
            'title': best_title,
            'score': max(best_score, 0),
            'baseScore': int(best_result.get('baseScore', max(best_score, 0))),
            'similarityPenalty': int(best_result.get('similarityPenalty', 0)),
            'attempts': max_attempts,
            'passed': False,
            'softAccepted': True,
            'history': history,
            'breakdown': dict(best_result.get('breakdown', {})),
        }

    # ì¼ë°˜ ëª©ì  ì œëª©ë„ í’ˆì§ˆ í•˜í•œì„ ë§Œì¡±í•˜ë©´ ì‹¤íŒ¨ ëŒ€ì‹  soft-accept ì²˜ë¦¬
    # (íŒŒì´í”„ë¼ì¸ ì „ì²´ ì‹¤íŒ¨ë¥¼ ë§‰ê³ , í›„ì† ê°€ë“œ/ì—ë””í„° ë‹¨ê³„ì—ì„œ ì¶”ê°€ ë³´ì •)
    if best_score >= soft_accept_min_score:
        logger.warning(
            "[TitleGen] generic soft-accept: purpose=%s score=%s < minScore=%s (softAcceptMinScore=%s), title=%s",
            title_purpose or "(none)",
            best_score,
            min_score,
            soft_accept_min_score,
            best_title,
        )
        return {
            'title': best_title,
            'score': max(best_score, 0),
            'baseScore': int(best_result.get('baseScore', max(best_score, 0))),
            'similarityPenalty': int(best_result.get('similarityPenalty', 0)),
            'attempts': max_attempts,
            'passed': False,
            'softAccepted': True,
            'history': history,
            'breakdown': dict(best_result.get('breakdown', {})),
        }

    best_suggestions = best_result.get('suggestions', []) if isinstance(best_result, dict) else []
    suggestion_text = ', '.join(best_suggestions) if best_suggestions else 'ì—†ìŒ'
    raise RuntimeError(
        f"[TitleGen] ì œëª© ìƒì„± ì‹¤íŒ¨: ìµœì†Œ ì ìˆ˜ {min_score}ì  ë¯¸ë‹¬ "
        f"(ìµœê³  {best_score}ì , ì œëª©: \"{best_title}\"). ê°œì„  íŒíŠ¸: {suggestion_text}"
    )
