
import re
import json
import logging
import time
import random
from html import escape as _xml_escape_raw
from typing import Dict, Any, Optional, List, Tuple

# API Call Timeout (seconds)
LLM_CALL_TIMEOUT = 120  # 2ë¶„ íƒ€ì„ì•„ì›ƒ
CONTEXT_ANALYZER_TIMEOUT = 60  # 1ë¶„ íƒ€ì„ì•„ì›ƒ

# Local imports
from ..common.classifier import classify_topic
from ..common.warnings import generate_non_lawmaker_warning, generate_family_status_warning
from ..common.theminjoo import get_party_stance
from ..common.election_rules import get_election_stage, get_prompt_instruction
from ..common.seo import build_seo_instruction
from ..common.constants import resolve_writing_method

# Template Builders
from ..templates.daily_communication import build_daily_communication_prompt
from ..templates.activity_report import build_activity_report_prompt
from ..templates.policy_proposal import build_policy_proposal_prompt
from ..templates.current_affairs import build_critical_writing_prompt, build_diagnosis_writing_prompt
from ..templates.local_issues import build_local_issues_prompt

# Template Builders Mapping
TEMPLATE_BUILDERS = {
    'emotional_writing': build_daily_communication_prompt,
    'logical_writing': build_policy_proposal_prompt, # Mapped buildLogicalWritingPrompt to policy proposal
    'direct_writing': build_activity_report_prompt,
    'critical_writing': build_critical_writing_prompt,
    'diagnostic_writing': build_diagnosis_writing_prompt,
    'analytical_writing': build_local_issues_prompt
}

logger = logging.getLogger(__name__)

def strip_html(text: str) -> str:
    if not text:
        return ''
    text = re.sub(r'<[^>]*>', '', text)
    return re.sub(r'\s+', '', text).strip()

def normalize_artifacts(text: str) -> str:
    if not text:
        return ''
    cleaned = text.strip()
    # ì½”ë“œíœìŠ¤ê°€ ê°ì‹¸ì ¸ ì˜¨ ê²½ìš° ë³¸ë¬¸ì„ ë³´ì¡´í•œ ì±„ íœìŠ¤ë§Œ ì œê±°
    cleaned = re.sub(
        r'```(?:[\w.+-]+)?\s*([\s\S]*?)\s*```',
        lambda m: m.group(1).strip(),
        cleaned,
    ).strip()
    cleaned = re.sub(r'^\s*\\"', '', cleaned)
    cleaned = re.sub(r'\\"?\s*$', '', cleaned)
    cleaned = re.sub(r'^\s*["â€œ]', '', cleaned)
    cleaned = re.sub(r'["â€]\s*$', '', cleaned)
    
    # Remove trailing metadata block only when marker appears near the tail as a standalone line.
    # (Do not truncate normal body text that happens to include "ì¹´í…Œê³ ë¦¬:" in a sentence.)
    lines = cleaned.splitlines()
    metadata_line_re = re.compile(
        r'^\s*\*{0,2}(ì¹´í…Œê³ ë¦¬|ê²€ìƒ‰ì–´ ì‚½ì… íšŸìˆ˜|ìƒì„± ì‹œê°„)\*{0,2}\s*:\s*',
        re.IGNORECASE,
    )
    tail_cut_index = None
    if lines:
        tail_window_start = max(0, len(lines) - 8)
        for i in range(tail_window_start, len(lines)):
            line = lines[i].strip()
            if not line:
                continue
            # HTML lineì€ ë³¸ë¬¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ metadata ì‹œì‘ì ìœ¼ë¡œ ë³´ì§€ ì•ŠìŒ
            if '<' in line and '>' in line:
                continue
            if metadata_line_re.match(line):
                tail_cut_index = i
                break
    if tail_cut_index is not None:
        cleaned = "\n".join(lines[:tail_cut_index]).strip()
    
    cleaned = re.sub(r'"content"\s*:\s*', '', cleaned)
    
    return cleaned.strip()


def normalize_html_structure_tags(text: str) -> str:
    """ê¸°ë³¸ êµ¬ì¡° íƒœê·¸ë¥¼ í‘œì¤€ í˜•íƒœë¡œ ì •ê·œí™”í•œë‹¤.

    ì—„ê²© ê¸°ì¤€ì€ ìœ ì§€í•˜ë˜, ëª¨ë¸ì´ ìƒì„±í•œ ë¶€ê°€ ì†ì„±/ëŒ€ì†Œë¬¸ì ì°¨ì´ë¡œ
    êµ¬ì¡° ê²€ì¦ì´ ì˜¤íƒìœ¼ë¡œ ì‹¤íŒ¨í•˜ëŠ” ìƒí™©ì„ ì¤„ì¸ë‹¤.
    """
    if not text:
        return ''
    normalized = text
    normalized = re.sub(r'<\s*h2\b[^>]*>', '<h2>', normalized, flags=re.IGNORECASE)
    normalized = re.sub(r'<\s*/\s*h2\s*>', '</h2>', normalized, flags=re.IGNORECASE)
    normalized = re.sub(r'<\s*p\b[^>]*>', '<p>', normalized, flags=re.IGNORECASE)
    normalized = re.sub(r'<\s*/\s*p\s*>', '</p>', normalized, flags=re.IGNORECASE)
    return normalized


def is_example_like_block(text: str) -> bool:
    """ì˜ˆì‹œ/í…œí”Œë¦¿ ë¸”ë¡ ì—¬ë¶€ë¥¼ íŒì •í•œë‹¤.

    ê·œì¹™ ì™„í™”ê°€ ì•„ë‹ˆë¼, ëª¨ë¸ì´ í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œë¥¼ ê·¸ëŒ€ë¡œ ì¬ì¶œë ¥í•œ ë¸”ë¡ì„
    ë³¸ë¬¸ í›„ë³´ì—ì„œ ì œì™¸í•˜ê¸° ìœ„í•œ ë°©ì–´ ë¡œì§ì´ë‹¤.
    """
    if not text:
        return True
    lowered = text.lower()
    placeholder_count = len(re.findall(r'\[[^\]\n]{1,40}\]', text))
    marker_count = sum(
        1
        for marker in (
            'sample_output',
            'reference_example',
            'placeholder',
            'ì˜ˆì‹œ',
            'ìƒ˜í”Œ',
            'ì—¬ê¸°ì—',
            'ì‘ì„±',
        )
        if marker in lowered
    )
    plain_len = len(strip_html(text))
    return (
        placeholder_count >= 2
        or ('<![cdata[' in lowered and plain_len < 900)
        or (marker_count >= 1 and plain_len < 900)
    )


def normalize_context_text(value: Any, *, sep: str = "\n\n") -> str:
    """list/tuple ì¤‘ì²© ì…ë ¥ê¹Œì§€ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ì •ê·œí™”."""
    if value is None:
        return ""
    if isinstance(value, str):
        return value.strip()
    if isinstance(value, (list, tuple, set)):
        parts: List[str] = []
        for item in value:
            normalized = normalize_context_text(item, sep=sep)
            if normalized:
                parts.append(normalized)
        return sep.join(parts)
    return str(value).strip()


def _xml_text(value: Any) -> str:
    return _xml_escape_raw(str(value or ""), quote=True)


def _xml_cdata(value: Any) -> str:
    text = str(value or "")
    safe = text.replace("]]>", "]]]]><![CDATA[>")
    return f"<![CDATA[{safe}]]>"

from ..base_agent import Agent

class StructureAgent(Agent):
    def __init__(self, name: str = 'StructureAgent', options: Optional[Dict[str, Any]] = None):
        super().__init__(name, options)

        # ê³µí†µ Gemini í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© (ìƒˆ google-genai SDK)
        from ..common.gemini_client import get_client, DEFAULT_MODEL
        self.model_name = DEFAULT_MODEL

        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” í™•ì¸
        client = get_client()
        if client:
            print(f"ğŸ¤– [StructureAgent] ëª¨ë¸: {self.model_name}")
        else:
            print(f"âš ï¸ [StructureAgent] Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")

    def _sanitize_target_word_count(self, target_word_count: Any) -> int:
        try:
            parsed = int(float(target_word_count))
        except (TypeError, ValueError):
            return 2000
        return max(1600, min(parsed, 3200))

    def _build_length_spec(self, target_word_count: Any, stance_count: int = 0) -> Dict[str, int]:
        target_chars = self._sanitize_target_word_count(target_word_count)

        # ì„¹ì…˜ë‹¹ 400ì ë‚´ì™¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 5~7ì„¹ì…˜ ê³„íš
        total_sections = round(target_chars / 400)
        total_sections = max(5, min(7, total_sections))
        if stance_count > 0:
            total_sections = max(total_sections, min(7, stance_count + 2))

        body_sections = total_sections - 2
        per_section_recommended = max(360, min(420, round(target_chars / total_sections)))
        per_section_min = max(320, per_section_recommended - 50)
        per_section_max = min(460, per_section_recommended + 50)

        min_chars = max(int(target_chars * 0.88), total_sections * per_section_min)
        # ìƒí•œì€ ê¸°ë³¸ ë¶„ëŸ‰(2000ì ê¸°ì¤€)ì—ì„œ 3000ìê¹Œì§€ í—ˆìš©í•˜ë„ë¡ ê³ ì • ìº¡ì„ ë‘”ë‹¤.
        # - ê¸°ì¡´: 2000ì ê¸°ì¤€ ì•½ 2250ì
        # - ë³€ê²½: ìµœëŒ€ 3000ì
        if target_chars >= 2000:
            max_chars = 3000
        else:
            max_chars = min(int(target_chars * 1.18), total_sections * per_section_max)
        if max_chars <= min_chars:
            max_chars = min_chars + 180

        return {
            'target_chars': target_chars,
            'body_sections': body_sections,
            'total_sections': total_sections,
            'per_section_min': per_section_min,
            'per_section_max': per_section_max,
            'per_section_recommended': per_section_recommended,
            'min_chars': min_chars,
            'max_chars': max_chars,
            'expected_h2': total_sections - 1
        }

    def _is_low_context_input(
        self,
        *,
        stance_text: str,
        instructions: str,
        news_data_text: str,
        news_context: str,
    ) -> bool:
        stance_len = len(strip_html(stance_text or ""))
        instruction_len = len(strip_html(instructions or ""))
        news_data_len = len(strip_html(news_data_text or ""))
        news_ctx_len = len(strip_html(news_context or ""))
        primary_len = stance_len + instruction_len + max(news_data_len, news_ctx_len)
        source_count = sum(
            1
            for length in (stance_len, instruction_len, news_data_len, news_ctx_len)
            if length > 0
        )

        if primary_len < 550:
            return True
        if source_count <= 1 and primary_len < 900:
            return True
        if max(stance_len, instruction_len) < 320 and max(news_data_len, news_ctx_len) < 220:
            return True
        return False

    def _build_profile_support_context(self, user_profile: Dict[str, Any], *, max_chars: int = 1800) -> str:
        if not isinstance(user_profile, dict):
            return ""

        facts: List[str] = []
        seen: set[str] = set()

        def add_fact(raw: Any, *, prefix: str = "") -> None:
            text = normalize_context_text(raw, sep="\n")
            if not text:
                return

            chunks: List[str] = []
            for line in re.split(r'[\r\n]+', text):
                line = line.strip(" \t-â€¢")
                if not line:
                    continue
                sentence_parts = re.split(r'[;Â·â€¢]+|[.!?ã€‚]\s+|ë‹¤\.\s+', line)
                for part in sentence_parts:
                    cleaned = re.sub(r'\s+', ' ', part).strip(" \t-â€¢")
                    if len(cleaned) < 8:
                        continue
                    chunks.append(f"{prefix}{cleaned}" if prefix else cleaned)

            for chunk in chunks:
                key = re.sub(r'\s+', ' ', chunk).strip().lower()
                if not key or key in seen:
                    continue
                seen.add(key)
                facts.append(chunk)
                if len(facts) >= 14:
                    return

        name = str(user_profile.get('name') or '').strip()
        party_name = str(user_profile.get('partyName') or '').strip()
        title = str(user_profile.get('customTitle') or user_profile.get('position') or '').strip()
        identity = " ".join(part for part in (party_name, title, name) if part)
        if identity:
            add_fact(f"í™”ì ì •ë³´: {identity}")

        add_fact(user_profile.get('careerSummary'))
        add_fact(user_profile.get('bio'))
        add_fact(user_profile.get('politicalExperience'), prefix='ì •ì¹˜ ì´ë ¥: ')

        core_values = user_profile.get('coreValues')
        if isinstance(core_values, list):
            core_values_text = ", ".join(str(v).strip() for v in core_values if str(v).strip())
            if core_values_text:
                add_fact(core_values_text, prefix='í•µì‹¬ ê°€ì¹˜: ')
        else:
            add_fact(core_values, prefix='í•µì‹¬ ê°€ì¹˜: ')

        bio_entries = user_profile.get('bioEntries')
        if isinstance(bio_entries, list):
            for entry in bio_entries[:8]:
                if isinstance(entry, dict):
                    entry_parts = []
                    for key in ('title', 'summary', 'content', 'description', 'value', 'text'):
                        value = normalize_context_text(entry.get(key))
                        if value:
                            entry_parts.append(value)
                    if entry_parts:
                        add_fact(" - ".join(entry_parts))
                else:
                    add_fact(entry)

        region_metro = str(user_profile.get('regionMetro') or '').strip()
        region_district = str(user_profile.get('regionDistrict') or '').strip()
        if region_metro or region_district:
            add_fact(f"í™œë™ ì§€ì—­: {' '.join(part for part in (region_metro, region_district) if part)}")

        if not facts:
            return ""

        lines: List[str] = []
        total_chars = 0
        for fact in facts:
            line = f"- {fact}"
            line_len = len(line) + 1
            if total_chars + line_len > max_chars:
                break
            lines.append(line)
            total_chars += line_len

        return "\n".join(lines).strip()

    def _split_into_context_items(self, text: str, *, min_len: int = 14, max_items: int = 40) -> List[str]:
        if not text:
            return []

        normalized = normalize_context_text(text, sep="\n")
        if not normalized:
            return []

        raw_parts = re.split(r'[\r\n]+|[;Â·â€¢]+|(?<=[.!?ã€‚])\s+|ë‹¤\.\s+', normalized)
        items: List[str] = []
        seen: set[str] = set()

        for part in raw_parts:
            cleaned = re.sub(r'\s+', ' ', str(part or "")).strip(" \t-â€¢")
            if not cleaned:
                continue
            if len(strip_html(cleaned)) < min_len:
                continue
            if len(cleaned) > 220:
                cleaned = cleaned[:220].rstrip() + "..."
            key = cleaned.lower()
            if key in seen:
                continue
            seen.add(key)
            items.append(cleaned)
            if len(items) >= max_items:
                break

        return items

    def _material_key(self, value: Any) -> str:
        text = normalize_context_text(value)
        if not text:
            return ""
        text = strip_html(text).lower()
        text = re.sub(r'["\'`â€œâ€â€˜â€™\[\]\(\)<>]', '', text)
        text = re.sub(r'[\s\.,!?;:Â·~\-_\\/]+', '', text)
        return text

    def _normalize_context_analysis_materials(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        if not isinstance(analysis, dict):
            return {}

        normalized_analysis = dict(analysis)

        stance_items: List[Dict[str, str]] = []
        stance_seen: set[str] = set()
        raw_stance = normalized_analysis.get('mustIncludeFromStance')
        if isinstance(raw_stance, list):
            for item in raw_stance:
                if isinstance(item, dict):
                    topic = normalize_context_text(item.get('topic'))
                    why_txt = normalize_context_text(item.get('expansion_why'))
                    how_txt = normalize_context_text(item.get('expansion_how'))
                    effect_txt = normalize_context_text(item.get('expansion_effect'))
                else:
                    topic = normalize_context_text(item)
                    why_txt = ""
                    how_txt = ""
                    effect_txt = ""

                if len(strip_html(topic)) < 5:
                    continue

                key = self._material_key(topic)
                if not key or key in stance_seen:
                    continue

                stance_seen.add(key)
                stance_items.append(
                    {
                        'topic': topic,
                        'expansion_why': why_txt,
                        'expansion_how': how_txt,
                        'expansion_effect': effect_txt,
                    }
                )
                if len(stance_items) >= 6:
                    break

        normalized_analysis['mustIncludeFromStance'] = stance_items

        def dedupe_text_list(
            raw_values: Any,
            *,
            blocked_keys: Optional[set[str]] = None,
            max_items: int = 8,
        ) -> Tuple[List[str], set[str]]:
            blocked = blocked_keys or set()
            results: List[str] = []
            keys: set[str] = set()
            if not isinstance(raw_values, list):
                return results, keys

            for raw in raw_values:
                text = normalize_context_text(raw)
                if len(strip_html(text)) < 8:
                    continue
                key = self._material_key(text)
                if not key or key in blocked or key in keys:
                    continue
                keys.add(key)
                results.append(text)
                if len(results) >= max_items:
                    break
            return results, keys

        stance_keys = {self._material_key(item.get('topic')) for item in stance_items if isinstance(item, dict)}
        stance_keys.discard("")

        facts, fact_keys = dedupe_text_list(
            normalized_analysis.get('mustIncludeFacts'),
            blocked_keys=stance_keys,
            max_items=8,
        )
        normalized_analysis['mustIncludeFacts'] = facts

        quotes, _quote_keys = dedupe_text_list(
            normalized_analysis.get('newsQuotes'),
            blocked_keys=stance_keys.union(fact_keys),
            max_items=8,
        )
        normalized_analysis['newsQuotes'] = quotes

        return normalized_analysis

    def _build_material_uniqueness_guard(
        self,
        context_analysis: Optional[Dict[str, Any]],
        *,
        body_sections: int,
    ) -> str:
        if not isinstance(context_analysis, dict):
            return ""

        cards: List[Dict[str, str]] = []
        seen: set[str] = set()

        def add_card(card_type: str, raw_text: Any) -> None:
            text = normalize_context_text(raw_text)
            if len(strip_html(text)) < 8:
                return
            key = self._material_key(text)
            if not key or key in seen:
                return
            seen.add(key)
            cards.append({"type": card_type, "text": text})

        for item in context_analysis.get('mustIncludeFromStance') or []:
            if isinstance(item, dict):
                add_card("stance", item.get('topic'))
            else:
                add_card("stance", item)
        for item in context_analysis.get('mustIncludeFacts') or []:
            add_card("fact", item)
        for item in context_analysis.get('newsQuotes') or []:
            add_card("quote", item)

        if not cards:
            return ""

        body_count = max(1, int(body_sections or 1))
        max_cards = max(4, min(len(cards), body_count + 3))
        selected = cards[:max_cards]
        lines: List[str] = []
        for idx, card in enumerate(selected):
            section_slot = (idx % body_count) + 1
            lines.append(
                f'    <material id="M{idx + 1}" type="{card["type"]}" '
                f'section_hint="body_{section_slot}">{_xml_text(card["text"])}</material>'
            )

        allocated_count = min(body_count, len(selected))
        allocation_lines: List[str] = []
        for idx in range(allocated_count):
            allocation_lines.append(
                f'    <section index="{idx + 1}" use="M{idx + 1}" mode="exclusive_once"/>'
            )

        if body_count > allocated_count:
            banned_ids = ",".join(f"M{idx + 1}" for idx in range(allocated_count))
            for idx in range(allocated_count, body_count):
                allocation_lines.append(
                    f'    <section index="{idx + 1}" use="DERIVED" mode="new_evidence_only" '
                    f'ban_ids="{banned_ids}"/>'
                )

        lines_text = "\n".join(lines)
        allocation_text = "\n".join(allocation_lines)
        return f"""
<material_uniqueness_guard priority="critical">
  <rule id="one_material_one_use">ì†Œì¬ ì¹´ë“œëŠ” ë³¸ë¬¸ ì „ì²´ì—ì„œ 1íšŒë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.</rule>
  <rule id="follow_section_allocation">section_allocation ì§€ì‹œë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¥´ê³ , ì´ë¯¸ ì‚¬ìš©í•œ material idëŠ” ì¬ì‚¬ìš© ê¸ˆì§€í•©ë‹ˆë‹¤.</rule>
  <rule id="no_recycled_quote">ë™ì¼ ì¸ìš©/ì¼í™”/ê·¼ê±° ë¬¸ì¥ì„ ë‹¤ë¥¸ ì„¹ì…˜ì—ì„œ ë‹¤ì‹œ ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤.</rule>
  <rule id="body_diversity">ê° ë³¸ë¡  ì„¹ì…˜ì€ ì„œë¡œ ë‹¤ë¥¸ ê·¼ê±°ë¥¼ ì‚¬ìš©í•´ ë…¼ì§€ë¥¼ ì „ê°œí•©ë‹ˆë‹¤.</rule>
  <materials>
{lines_text}
  </materials>
  <section_allocation>
{allocation_text}
  </section_allocation>
</material_uniqueness_guard>
""".strip()

    def _detect_material_reuse_issues(
        self,
        content: str,
        context_analysis: Optional[Dict[str, Any]],
        *,
        max_mentions: int = 1,
    ) -> List[Dict[str, Any]]:
        if not content or not isinstance(context_analysis, dict):
            return []

        normalized_body = self._material_key(strip_html(content))
        if not normalized_body:
            return []

        candidates: List[Tuple[str, str]] = []
        for item in context_analysis.get('mustIncludeFromStance') or []:
            if isinstance(item, dict):
                candidates.append(("stance", normalize_context_text(item.get('topic'))))
            else:
                candidates.append(("stance", normalize_context_text(item)))
        for item in context_analysis.get('mustIncludeFacts') or []:
            candidates.append(("fact", normalize_context_text(item)))
        for item in context_analysis.get('newsQuotes') or []:
            candidates.append(("quote", normalize_context_text(item)))

        issues: List[Dict[str, Any]] = []
        seen_keys: set[str] = set()
        for material_type, text in candidates:
            key = self._material_key(text)
            if not key or key in seen_keys:
                continue
            seen_keys.add(key)
            if len(key) < 16:
                continue
            count = normalized_body.count(key)
            if count > max_mentions:
                issues.append(
                    {
                        "type": material_type,
                        "text": text,
                        "count": count,
                    }
                )
        return issues

    def _extract_profile_additional_items(self, user_profile: Dict[str, Any], *, max_items: int = 24) -> List[str]:
        if not isinstance(user_profile, dict):
            return []

        items: List[str] = []
        seen: set[str] = set()

        def add_unique(text: str) -> None:
            cleaned = re.sub(r'\s+', ' ', normalize_context_text(text)).strip(" \t-â€¢")
            if not cleaned:
                return
            if len(strip_html(cleaned)) < 12:
                return
            key = cleaned.lower()
            if key in seen:
                return
            seen.add(key)
            items.append(cleaned)

        def flatten_value(value: Any) -> str:
            if value is None:
                return ""
            if isinstance(value, str):
                return value.strip()
            if isinstance(value, dict):
                parts: List[str] = []
                for k, v in value.items():
                    nested = flatten_value(v)
                    if not nested:
                        continue
                    if isinstance(v, (dict, list, tuple, set)):
                        parts.append(nested)
                    else:
                        parts.append(f"{k}: {nested}")
                return "\n".join(parts)
            if isinstance(value, (list, tuple, set)):
                parts = [flatten_value(v) for v in value]
                return "\n".join(p for p in parts if p)
            return str(value).strip()

        # 1) bioEntries ê¸°ë°˜ ì¶”ê°€ì •ë³´ ìš°ì„  ì¶”ì¶œ (ì •ì±…/ë²•ì•ˆ/ì„±ê³¼ ìš°ì„ )
        type_priority = {
            'policy': 0,
            'legislation': 1,
            'achievement': 2,
            'vision': 3,
            'experience': 4,
            'reference': 5,
        }
        typed_candidates: List[Tuple[int, str]] = []
        bio_entries = user_profile.get('bioEntries')
        if isinstance(bio_entries, list):
            for entry in bio_entries:
                if not isinstance(entry, dict):
                    continue
                entry_type = str(entry.get('type') or '').strip().lower()
                priority = type_priority.get(entry_type, 9)
                if priority >= 9:
                    continue
                title = normalize_context_text(entry.get('title'))
                content = normalize_context_text(
                    entry.get('content') or entry.get('summary') or entry.get('description') or entry.get('text')
                )
                if not content:
                    continue
                label = entry_type or 'profile'
                if title:
                    typed_candidates.append((priority, f"[{label}] {title} - {content}"))
                else:
                    typed_candidates.append((priority, f"[{label}] {content}"))

        for _, text in sorted(typed_candidates, key=lambda x: x[0]):
            add_unique(text)
            if len(items) >= max_items:
                return items[:max_items]

        # 2) userProfileì˜ êµ¬ì¡°í™” í•„ë“œì—ì„œ ê³µì•½/ë²•ì•ˆ/ì„±ê³¼ì„± í‚¤ ì¶”ì¶œ
        interesting_key_pattern = re.compile(
            r'(policy|pledge|promise|manifesto|bill|legislation|ordinance|achievement|performance|track|'
            r'ê³µì•½|ì •ì±…|ë²•ì•ˆ|ì¡°ë¡€|ì„±ê³¼|ì‹¤ì |ì—…ì )',
            re.IGNORECASE,
        )
        skip_keys = {
            'name', 'partyName', 'customTitle', 'position', 'status', 'role',
            'regionMetro', 'regionDistrict', 'regionLocal', 'electoralDistrict',
            'bio', 'careerSummary', 'bioEntries', 'styleGuide', 'styleFingerprint',
            'slogan', 'sloganEnabled', 'donationInfo', 'donationEnabled',
            'targetElection', 'familyStatus', 'age', 'ageDecade', 'gender',
            'committees', 'customCommittees', 'localConnection', 'politicalExperience',
            'constituencyType', 'isAdmin', 'isTester',
        }
        for key, value in user_profile.items():
            key_text = str(key or '').strip()
            if not key_text or key_text in skip_keys:
                continue
            if not interesting_key_pattern.search(key_text):
                continue
            flattened = flatten_value(value)
            for snippet in self._split_into_context_items(flattened, min_len=14, max_items=8):
                add_unique(f"[{key_text}] {snippet}")
                if len(items) >= max_items:
                    return items[:max_items]

        return items[:max_items]

    def _build_profile_substitute_context(self, user_profile: Dict[str, Any], *, target_items: int = 3) -> Dict[str, Any]:
        target = max(1, int(target_items or 3))
        additional_pool = self._extract_profile_additional_items(user_profile, max_items=24)

        rng = random.SystemRandom()
        selected_additional: List[str] = []
        if additional_pool:
            selected_additional = rng.sample(additional_pool, min(target, len(additional_pool)))

        selected_items: List[str] = list(selected_additional)
        needed = max(0, target - len(selected_items))

        if needed > 0:
            bio_text = normalize_context_text(
                [user_profile.get('careerSummary'), user_profile.get('bio')],
                sep="\n",
            )
            bio_pool = [
                item for item in self._split_into_context_items(bio_text, min_len=12, max_items=24)
                if item not in selected_items
            ]
            if bio_pool:
                bio_selected = rng.sample(bio_pool, min(needed, len(bio_pool)))
                selected_items.extend(bio_selected)
                needed = max(0, target - len(selected_items))

        if needed > 0:
            support_text = self._build_profile_support_context(user_profile, max_chars=1800)
            support_pool = [
                item for item in self._split_into_context_items(support_text, min_len=10, max_items=24)
                if item not in selected_items
            ]
            if support_pool:
                support_selected = rng.sample(support_pool, min(needed, len(support_pool)))
                selected_items.extend(support_selected)

        if len(selected_items) > 1:
            rng.shuffle(selected_items)

        context_text = "\n".join(f"- {item}" for item in selected_items)
        return {
            'selectedItems': selected_items,
            'contextText': context_text,
            'additionalPoolCount': len(additional_pool),
            'usedAdditionalCount': len(selected_additional),
            'usedBioCount': max(0, len(selected_items) - len(selected_additional)),
        }

    def _build_retry_directive(self, validation: Dict[str, Any], length_spec: Dict[str, int]) -> str:
        code = validation.get('code')
        total_sections = length_spec['total_sections']
        body_sections = length_spec['body_sections']
        min_chars = length_spec['min_chars']
        max_chars = length_spec['max_chars']
        per_section_recommended = length_spec['per_section_recommended']
        expected_h2 = length_spec['expected_h2']

        if code == 'LENGTH_SHORT':
            return (
                f"ì¬ì‘ì„± ì‹œ ì´ ë¶„ëŸ‰ì„ {min_chars}~{max_chars}ìë¡œ ë§ì¶”ì‹­ì‹œì˜¤. "
                f"ì´ ì„¹ì…˜ì€ ë„ì… 1 + ë³¸ë¡  {body_sections} + ê²°ë¡  1(ì´ {total_sections})ë¡œ ìœ ì§€í•˜ê³ , "
                f"ì„¹ì…˜ë‹¹ {per_section_recommended}ì ë‚´ì™¸ë¡œ ë³´ê°•í•˜ì‹­ì‹œì˜¤."
            )

        if code == 'LENGTH_LONG':
            return (
                f"ì¬ì‘ì„± ì‹œ ì´ ë¶„ëŸ‰ì„ {max_chars}ì ì´í•˜ë¡œ ì••ì¶•í•˜ì‹­ì‹œì˜¤(ì ˆëŒ€ ì´ˆê³¼ ê¸ˆì§€). "
                f"ì¤‘ë³µ ë¬¸ì¥, ìˆ˜ì‹ì–´, ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ì œê±°í•˜ê³  ì„¹ì…˜ë‹¹ {per_section_recommended}ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì‹­ì‹œì˜¤."
            )

        if code in {'H2_SHORT', 'H2_LONG'}:
            return (
                f"ì„¹ì…˜ êµ¬ì¡°ë¥¼ ì •í™•íˆ ë§ì¶”ì‹­ì‹œì˜¤: ë„ì… 1 + ë³¸ë¡  {body_sections} + ê²°ë¡  1. "
                f"<h2>ëŠ” ë³¸ë¡ ê³¼ ê²°ë¡ ì—ë§Œ ì‚¬ìš©í•˜ì—¬ ì´ {expected_h2}ê°œë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤. "
                f"ì†Œì œëª© íƒœê·¸ëŠ” ì†ì„± ì—†ì´ ë°˜ë“œì‹œ <h2>í…ìŠ¤íŠ¸</h2> í˜•ì‹ë§Œ í—ˆìš©ë©ë‹ˆë‹¤."
            )

        if code in {'P_SHORT', 'P_LONG'}:
            return (
                f"ë¬¸ë‹¨ ìˆ˜ë¥¼ ì¡°ì •í•˜ì‹­ì‹œì˜¤. ì´ {total_sections}ê°œ ì„¹ì…˜ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ì€ 2~3ê°œì”© ìœ ì§€í•˜ê³ , "
                f"êµ°ë”ë”ê¸° ì—†ëŠ” ë¬¸ì¥ìœ¼ë¡œ ê¸¸ì´ ë²”ìœ„({min_chars}~{max_chars}ì)ë¥¼ ì§€í‚¤ì‹­ì‹œì˜¤."
            )

        if code == 'EVENT_INVITE_REDUNDANT':
            return (
                "í–‰ì‚¬ ì•ˆë‚´ ë¬¸êµ¬ ë°˜ë³µì„ ì¤„ì´ì‹­ì‹œì˜¤. \"ì§ì ‘ ë§Œë‚˜\", \"ì§„ì†”í•œ ì†Œí†µ\", \"ê¸°ë‹¤ë¦¬ê² ìŠµë‹ˆë‹¤\" ë¥˜ í‘œí˜„ì€ "
                "ê° 2íšŒ ì´í•˜ë¡œ ì œí•œí•˜ê³ , ì¤‘ë³µëœ ë¬¸ì¥ì€ í–‰ì‚¬ í•µì‹¬ ì •ë³´(ì¼ì‹œ/ì¥ì†Œ/ì°¸ì—¬ ë°©ë²•)ë‚˜ ìƒˆë¡œìš´ ê·¼ê±°ë¡œ ì¹˜í™˜í•˜ì‹­ì‹œì˜¤."
            )

        if code == 'EVENT_FACT_REPEAT':
            return (
                "í–‰ì‚¬ ì¼ì‹œ+ì¥ì†Œê°€ ê²°í•©ëœ ì•ˆë‚´ ë¬¸ì¥ì€ ë„ì… 1íšŒ, ê²°ë¡  1íšŒê¹Œì§€ë§Œ í—ˆìš©ë©ë‹ˆë‹¤. "
                "3íšŒì§¸ë¶€í„°ëŠ” \"ì´ë²ˆ í–‰ì‚¬ í˜„ì¥\"ì²˜ëŸ¼ ë³€í˜•í•˜ì—¬ ë°˜ë³µ êµ¬ë¬¸ì„ í•´ì†Œí•˜ì‹­ì‹œì˜¤."
            )

        if code == 'META_PROMPT_LEAK':
            return (
                "í”„ë¡¬í”„íŠ¸ ê·œì¹™ ì„¤ëª… ë¬¸ì¥ì„ ë³¸ë¬¸ì— ì“°ì§€ ë§ˆì‹­ì‹œì˜¤. "
                "\"ë¬¸ì œëŠ”~ì ê²€\"ë¥˜ ë©”íƒ€ ë¬¸ì¥ì„ ì œê±°í•˜ê³  ì‹¤ì œ ì‚¬ì‹¤/ê·¼ê±° ë¬¸ì¥ìœ¼ë¡œ ë°”ê¿” ì‘ì„±í•˜ì‹­ì‹œì˜¤."
            )

        if code == 'PHRASE_REPEAT_CAP':
            return (
                "ìƒíˆ¬ êµ¬ë¬¸ ë°˜ë³µì´ ê³¼ë‹¤í•©ë‹ˆë‹¤. ë™ì¼ ì–´êµ¬ëŠ” ìµœëŒ€ 2íšŒë¡œ ì œí•œí•˜ê³ , "
                "ì´ˆê³¼ êµ¬ê°„ì€ ìƒˆë¡œìš´ ê·¼ê±°Â·ìˆ˜ì¹˜Â·ì‚¬ë¡€ ì¤‘ì‹¬ ë¬¸ì¥ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì‹­ì‹œì˜¤."
            )

        if code == 'MATERIAL_REUSE':
            return (
                "ê°™ì€ ì†Œì¬(ì¸ìš©/ì¼í™”/ê·¼ê±°)ë¥¼ ì—¬ëŸ¬ ë²ˆ ì¬ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. "
                "ë³¸ë¡  ì„¹ì…˜ë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ ì†Œì¬ ì¹´ë“œë¥¼ ë°°ì •í•˜ê³ , ê° ì¹´ë“œëŠ” 1íšŒë§Œ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤."
            )

        return (
            f"ì´ {total_sections}ê°œ ì„¹ì…˜ êµ¬ì¡°ì™€ ë¶„ëŸ‰ ë²”ìœ„({min_chars}~{max_chars}ì)ë¥¼ ì •í™•íˆ ì¤€ìˆ˜í•˜ì—¬ ì¬ì‘ì„±í•˜ì‹­ì‹œì˜¤."
        )

    async def process(self, context: Dict[str, Any]) -> Dict[str, Any]:
        topic = context.get('topic', '')
        user_profile = context.get('userProfile', {})
        # ë°©ì–´ ì½”ë“œ - listë¡œ ì „ë‹¬ë˜ëŠ” ê²½ìš° ë°©ì–´
        if not isinstance(user_profile, dict):
            user_profile = {}
        category = context.get('category', '')
        sub_category = context.get('subCategory', '')
        instructions = normalize_context_text(context.get('instructions', ''))
        news_context = normalize_context_text(context.get('newsContext', ''))
        # ğŸ”‘ [NEW] ì…ì¥ë¬¸ê³¼ ë‰´ìŠ¤/ë°ì´í„° ë¶„ë¦¬
        stance_text = normalize_context_text(context.get('stanceText', ''))
        news_data_text = normalize_context_text(context.get('newsDataText', ''))
        source_instructions = normalize_context_text([stance_text, instructions], sep="\n\n")
        # stanceTextê°€ ë¹„ì–´ë„ ìµœì†Œ ì•µì»¤ë¥¼ ìƒì§€ ì•Šë„ë¡ topicì„ ë¶„ì„ ì‹œë“œë¡œ ë³´ê°•í•œë‹¤.
        if not strip_html(source_instructions):
            source_instructions = normalize_context_text([topic, instructions], sep="\n\n")
        effective_news_context = news_data_text or news_context
        target_word_count = context.get('targetWordCount', 2000)
        user_keywords = context.get('userKeywords', [])
        personalized_hints = normalize_context_text(context.get('personalizedHints', ''), sep="\n")
        memory_context = normalize_context_text(context.get('memoryContext', ''), sep="\n")
        personalization_context = normalize_context_text([personalized_hints, memory_context], sep="\n")
        profile_support_context = self._build_profile_support_context(user_profile)
        has_news_source = bool(strip_html(effective_news_context))
        profile_substitute = self._build_profile_substitute_context(user_profile, target_items=3) if not has_news_source else {}
        analyzer_news_context = effective_news_context
        news_source_mode = 'news'
        if not has_news_source:
            news_source_mode = 'profile_fallback'
            substitute_text = normalize_context_text(profile_substitute.get('contextText'))
            if not substitute_text and profile_support_context:
                fallback_items = self._split_into_context_items(
                    profile_support_context,
                    min_len=12,
                    max_items=3,
                )
                if fallback_items:
                    substitute_text = "\n".join(f"- {item}" for item in fallback_items)
                    if isinstance(profile_substitute, dict):
                        profile_substitute["selectedItems"] = fallback_items
                        profile_substitute["contextText"] = substitute_text
                        profile_substitute["usedBioCount"] = max(
                            int(profile_substitute.get("usedBioCount") or 0),
                            len(fallback_items),
                        )
                    print(
                        "âš ï¸ [StructureAgent] í”„ë¡œí•„ ì¶”ê°€ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ Bio ë³´ê°• 1ì°¨ ë¬¸ë§¥ì„ ëŒ€ì²´ìë£Œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."
                    )
            analyzer_news_context = f"[ì‚¬ìš©ì ì¶”ê°€ì •ë³´ ëŒ€ì²´ìë£Œ]\n{substitute_text}" if substitute_text else ""

        print(f"ğŸš€ [StructureAgent] ì‹œì‘ - ì¹´í…Œê³ ë¦¬: {category or '(ìë™)'}, ì£¼ì œ: {topic}")
        print(f"ğŸ“Š [StructureAgent] ì…ì¥ë¬¸: {len(stance_text)}ì, ë‰´ìŠ¤/ë°ì´í„°: {len(news_data_text)}ì")
        if news_source_mode == 'news':
            print(f"ğŸ§­ [StructureAgent] ContextAnalyzer ì†ŒìŠ¤: ë‰´ìŠ¤/ë°ì´í„° ì‚¬ìš© ({len(strip_html(effective_news_context))}ì)")
        else:
            print(
                "ğŸ§­ [StructureAgent] ContextAnalyzer ì†ŒìŠ¤: í”„ë¡œí•„ ëŒ€ì²´ "
                f"(ì¶”ê°€ì •ë³´ í’€ {profile_substitute.get('additionalPoolCount', 0)}ê°œ, "
                f"ì‚¬ìš© ì¶”ê°€ì •ë³´ {profile_substitute.get('usedAdditionalCount', 0)}ê°œ, "
                f"bio ë³´ì¶© {profile_substitute.get('usedBioCount', 0)}ê°œ)"
            )

        # 1. Determine Writing Method
        writing_method = ''
        if category and category != 'auto':
            writing_method = resolve_writing_method(category, sub_category)
            print(f"âœï¸ [StructureAgent] ì‘ë²• ì„ íƒ (ì¹´í…Œê³ ë¦¬ ê¸°ë°˜): {writing_method}")
        else:
            classification = await classify_topic(topic)
            writing_method = classification['writingMethod']
            print(f"ğŸ¤– [StructureAgent] ì‘ë²• ìë™ ì¶”ë¡ : {writing_method} (ì‹ ë¢°ë„: {classification.get('confidence')}, ì†ŒìŠ¤: {classification.get('source')})")

        # 2. Build Author Bio
        author_bio, author_name = self.build_author_bio(user_profile)

        # 3. Get Party Stance
        party_stance_guide = None
        try:
             party_stance_guide = get_party_stance(topic)
        except Exception as e:
             print(f"âš ï¸ [StructureAgent] ë‹¹ë¡  ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

        # 4. ContextAnalyzer (ì…ì¥ë¬¸/ë‰´ìŠ¤ ë¶„ë¦¬ ì²˜ë¦¬)
        analyzer_stance_text = source_instructions
        if len(strip_html(analyzer_stance_text)) < 24:
            analyzer_stance_text = normalize_context_text([analyzer_stance_text, topic], sep="\n\n")
        analyzer_news_text = analyzer_news_context

        context_analysis = await self.run_context_analyzer(
            analyzer_stance_text,
            analyzer_news_text,
            author_name
        )
        if isinstance(context_analysis, dict):
            context_analysis = self._normalize_context_analysis_materials(context_analysis)
        # validate_output í˜¸ì¶œì— ì‚¬ìš©í•˜ëŠ” ì´ë²¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ íŒíŠ¸ëŠ”
        # process ìŠ¤ì½”í”„ì—ì„œ í•­ìƒ ì´ˆê¸°í™”ë˜ì–´ì•¼ í•œë‹¤.
        is_event_announcement = False
        event_date_hint = ''
        event_location_hint = ''
        if isinstance(context_analysis, dict):
            analysis_intent = str(context_analysis.get('intent') or '').strip().lower()
            must_preserve = context_analysis.get('mustPreserve')
            if analysis_intent == 'event_announcement':
                is_event_announcement = True
                if isinstance(must_preserve, dict):
                    event_date_hint = str(must_preserve.get('eventDate') or '').strip()
                    event_location_hint = str(must_preserve.get('eventLocation') or '').strip()

        stance_count = len(context_analysis.get('mustIncludeFromStance', [])) if context_analysis else 0
        length_spec = self._build_length_spec(target_word_count, stance_count)
        print(
            f"ğŸ“ [StructureAgent] ë¶„ëŸ‰ ê³„íš: {length_spec['total_sections']}ì„¹ì…˜, "
            f"{length_spec['min_chars']}~{length_spec['max_chars']}ì "
            f"(ì„¹ì…˜ë‹¹ {length_spec['per_section_recommended']}ì)"
        )

        # 5. Build Prompt
        prompt = self.build_prompt({
            'topic': topic,
            'category': category,
            'writingMethod': writing_method,
            'authorName': author_name,
            'authorBio': author_bio,
            'instructions': source_instructions,
            'newsContext': effective_news_context,
            'targetWordCount': target_word_count,
            'partyStanceGuide': party_stance_guide,
            'contextAnalysis': context_analysis,
            'userProfile': user_profile,
            'personalizationContext': personalization_context,
            'memoryContext': memory_context,
            'profileSupportContext': profile_support_context,
            'profileSubstituteContext': profile_substitute.get('contextText') if isinstance(profile_substitute, dict) else '',
            'newsSourceMode': news_source_mode,
            'userKeywords': user_keywords,
            'lengthSpec': length_spec
        })

        print(f"ğŸ“ [StructureAgent] í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ ({len(prompt)}ì)")

        # 6. Retry Loop
        max_retries = 3
        attempt = 0
        feedback = ''
        retry_directive = ''
        validation: Dict[str, Any] = {}
        last_error = None
        best_candidate: Dict[str, Any] = {}
        structural_recoverable_codes = {
            'H2_SHORT',
            'H2_LONG',
            'P_SHORT',
            'P_LONG',
            'H2_MALFORMED',
            'P_MALFORMED',
            'TAG_DISALLOWED',
            'PHRASE_REPEAT_CAP',
            'MATERIAL_REUSE',
            'LOCATION_ORPHAN_REPEAT',
            'META_PROMPT_LEAK',
            'EVENT_FACT_REPEAT',
            'EVENT_INVITE_REDUNDANT',
        }

        def _candidate_rank(candidate_validation: Dict[str, Any], candidate_content: str) -> tuple:
            plain_len = len(strip_html(candidate_content or ''))
            code = str(candidate_validation.get('code') or '')
            penalties = {
                'LENGTH_SHORT': 8,
                'LENGTH_LONG': 7,
                'H2_SHORT': 4,
                'H2_LONG': 4,
                'P_SHORT': 5,
                'P_LONG': 5,
                'H2_MALFORMED': 6,
                'P_MALFORMED': 6,
                'TAG_DISALLOWED': 6,
            }
            penalty = penalties.get(code, 5)
            return (
                1 if bool(candidate_validation.get('passed')) else 0,
                1 if plain_len >= int(length_spec.get('min_chars') or 0) else 0,
                1 if plain_len <= int(length_spec.get('max_chars') or 999999) else 0,
                -penalty,
                -abs(plain_len - int(length_spec.get('min_chars') or 0)),
                plain_len,
            )

        def _remember_best(
            candidate_content: str,
            candidate_title: str,
            candidate_validation: Dict[str, Any],
            source: str,
            source_attempt: int,
        ) -> None:
            nonlocal best_candidate
            if not candidate_content:
                return
            rank = _candidate_rank(candidate_validation, candidate_content)
            if (not best_candidate) or rank > tuple(best_candidate.get('rank') or ()):
                best_candidate = {
                    'content': candidate_content,
                    'title': candidate_title,
                    'validation': dict(candidate_validation or {}),
                    'rank': rank,
                    'plain_len': len(strip_html(candidate_content or '')),
                    'source': source,
                    'attempt': source_attempt,
                }

        while attempt <= max_retries:
            attempt += 1
            print(f"ğŸ”„ [StructureAgent] ìƒì„± ì‹œë„ {attempt}/{max_retries + 1}")

            current_prompt = prompt
            if feedback:
                retry_block = f"\n\n{retry_directive}" if retry_directive else ""
                current_prompt += (
                    f"\n\nğŸš¨ [ì¤‘ìš” - ì¬ì‘ì„± ì§€ì‹œ] ì´ì „ ì‘ì„±ë³¸ì´ ë‹¤ìŒ ì´ìœ ë¡œ ë°˜ë ¤ë˜ì—ˆìŠµë‹ˆë‹¤:\n"
                    f"\"{feedback}\"{retry_block}"
                )

            try:
                response = await self.call_llm(current_prompt)
                print(f"ğŸ“¥ [StructureAgent] LLM ì›ë³¸ ì‘ë‹µ ({len(response)}ì)")

                structured = self.parse_response(response)
                content = normalize_artifacts(structured['content'])
                content = normalize_html_structure_tags(content)
                title = normalize_artifacts(structured['title'])

                # íŒŒì‹±/ì •ë¦¬ ê³¼ì •ì—ì„œ ë³¸ë¬¸ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ì¶•ì•½ëœ ê²½ìš° ì¬ì‹œë„ ìœ ë„.
                plain_len = len(strip_html(content))
                response_text = str(response or "")
                response_plain_len = len(strip_html(response_text))
                print(
                    f"ğŸ“ [StructureAgent] ì‹œë„ {attempt} ê¸¸ì´: "
                    f"raw={len(response_text)}ì, parsed={len(content)}ì, plain={plain_len}ì"
                )
                if plain_len < 400 and (
                    len(response_text) > 1000
                    or response_plain_len > max(700, plain_len * 4)
                ):
                    raise Exception(f"íŒŒì‹± ë¹„ì •ìƒ ì¶•ì•½ ê°ì§€ ({plain_len}ì)")

                validation = self.validate_output(
                    content,
                    length_spec,
                    context_analysis=context_analysis,
                    is_event_announcement=is_event_announcement,
                    event_date_hint=event_date_hint,
                    event_location_hint=event_location_hint,
                )
                _remember_best(content, title, validation, source='draft', source_attempt=attempt)

                if validation['passed']:
                    print(f"âœ… [StructureAgent] ê²€ì¦ í†µê³¼: {len(strip_html(content))}ì")
                    if not title.strip():
                        title = topic[:20] if topic else 'ìƒˆ ì›ê³ '
                    return {
                        'content': content,
                        'title': title,
                        'writingMethod': writing_method,
                        'contextAnalysis': context_analysis
                    }

                print(
                    f"âš ï¸ [StructureAgent] ê²€ì¦ ì‹¤íŒ¨: code={validation.get('code')} "
                    f"reason={validation['reason']}"
                )

                recovery_code = str(validation.get('code') or '')
                recovery_content = content
                recovery_title = title
                recovery_validation = dict(validation or {})
                max_recovery_rounds = 3 if (
                    recovery_code == 'LENGTH_SHORT' or recovery_code in structural_recoverable_codes
                ) else 1

                for recovery_round in range(1, max_recovery_rounds + 1):
                    current_code = str(recovery_validation.get('code') or '')
                    recovery_result: Optional[Tuple[str, str]] = None

                    if current_code == 'LENGTH_SHORT':
                        recovery_result = await self.recover_length_shortfall(
                            content=recovery_content,
                            title=recovery_title,
                            topic=topic,
                            length_spec=length_spec,
                            author_bio=author_bio,
                        )
                    elif current_code in structural_recoverable_codes:
                        recovery_result = await self.recover_structural_shortfall(
                            content=recovery_content,
                            title=recovery_title,
                            topic=topic,
                            length_spec=length_spec,
                            author_bio=author_bio,
                            failed_code=current_code,
                            failed_reason=str(recovery_validation.get('reason') or ''),
                            failed_feedback=str(recovery_validation.get('feedback') or ''),
                        )

                    if not recovery_result:
                        break

                    recovered_content, recovered_title = recovery_result
                    recovered_validation = self.validate_output(
                        recovered_content,
                        length_spec,
                        context_analysis=context_analysis,
                        is_event_announcement=is_event_announcement,
                        event_date_hint=event_date_hint,
                        event_location_hint=event_location_hint,
                    )
                    _remember_best(
                        recovered_content,
                        recovered_title,
                        recovered_validation,
                        source='repair',
                        source_attempt=attempt,
                    )
                    if recovered_validation.get('passed'):
                        print(
                            f"âœ… [StructureAgent] ë³µêµ¬ ê²€ì¦ í†µê³¼: "
                            f"{len(strip_html(recovered_content))}ì"
                        )
                        if not recovered_title.strip():
                            recovered_title = topic[:20] if topic else 'ìƒˆ ì›ê³ '
                        return {
                            'content': recovered_content,
                            'title': recovered_title,
                            'writingMethod': writing_method,
                            'contextAnalysis': context_analysis
                        }

                    next_code = str(recovered_validation.get('code') or '')
                    print(
                        f"âš ï¸ [StructureAgent] ë³µêµ¬ ì‹œë„ {recovery_round}/{max_recovery_rounds} ì‹¤íŒ¨: "
                        f"code={next_code} reason={recovered_validation.get('reason')}"
                    )

                    same_code = next_code == current_code
                    unchanged_text = strip_html(recovered_content) == strip_html(recovery_content)
                    recovery_content = recovered_content
                    recovery_title = recovered_title
                    recovery_validation = dict(recovered_validation or {})

                    if same_code and unchanged_text:
                        print(
                            "âš ï¸ [StructureAgent] ë³µêµ¬ ê²°ê³¼ê°€ ë™ì¼í•´ ì¶”ê°€ ë³µêµ¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤."
                        )
                        break

                content = recovery_content
                title = recovery_title
                validation = recovery_validation

                feedback = str(validation.get('feedback') or validation.get('reason') or '')
                retry_directive = self._build_retry_directive(validation, length_spec)
                last_error = None

            except Exception as e:
                error_msg = str(e)
                print(f"âŒ [StructureAgent] ì—ëŸ¬ ë°œìƒ: {error_msg}")
                feedback = error_msg
                retry_directive = ''
                last_error = error_msg

            if attempt > max_retries:
                if best_candidate:
                    best_validation = best_candidate.get('validation') or {}
                    best_reason = str(best_validation.get('reason') or '').strip()
                    best_code = str(best_validation.get('code') or '').strip()
                    best_len = int(best_candidate.get('plain_len') or 0)
                    final_reason = best_reason or last_error or validation.get('reason', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                    raise Exception(
                        f"StructureAgent ì‹¤íŒ¨ ({max_retries}íšŒ ì¬ì‹œë„ í›„): {final_reason} "
                        f"[bestCode={best_code}, bestLen={best_len}, source={best_candidate.get('source')}]"
                    )
                final_reason = last_error or validation.get('reason', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                raise Exception(f"StructureAgent ì‹¤íŒ¨ ({max_retries}íšŒ ì¬ì‹œë„ í›„): {final_reason}")

    async def call_llm(self, prompt: str) -> str:
        from ..common.gemini_client import generate_content_async

        print(f"ğŸ“¤ [StructureAgent] LLM í˜¸ì¶œ ì‹œì‘")
        start_time = time.time()

        try:
            response_text = await generate_content_async(
                prompt,
                model_name=self.model_name,
                temperature=0.1,  # êµ¬ì¡° ì¤€ìˆ˜ìœ¨ì„ ë†’ì´ê¸° ìœ„í•´ ë³€ë™ì„± ì¶•ì†Œ
                max_output_tokens=4096
            )

            elapsed = time.time() - start_time
            print(f"âœ… [StructureAgent] LLM ì‘ë‹µ ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")

            return response_text

        except Exception as e:
            elapsed = time.time() - start_time
            error_msg = str(e)
            print(f"âŒ [StructureAgent] LLM í˜¸ì¶œ ì‹¤íŒ¨ ({elapsed:.1f}ì´ˆ): {error_msg}")

            # íƒ€ì„ì•„ì›ƒ ê´€ë ¨ ì—ëŸ¬ ë©”ì‹œì§€ ê°œì„ 
            if 'timeout' in error_msg.lower() or 'deadline' in error_msg.lower():
                raise Exception(f"LLM í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ ({elapsed:.1f}ì´ˆ). Gemini APIê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            raise

    async def recover_length_shortfall(
        self,
        *,
        content: str,
        title: str,
        topic: str,
        length_spec: Dict[str, int],
        author_bio: str = '',
    ) -> Optional[Tuple[str, str]]:
        current_len = len(strip_html(content))
        min_len = int(length_spec.get('min_chars', 0))
        max_len = int(length_spec.get('max_chars', 0))
        expected_h2 = int(length_spec.get('expected_h2', 0))

        if min_len <= 0:
            return None

        from ..common.gemini_client import generate_content_async

        best_content = content
        best_title = title
        best_len = current_len
        max_recovery_attempts = 3

        for recovery_attempt in range(1, max_recovery_attempts + 1):
            gap = max(0, min_len - best_len)
            rewrite_mode = best_len < int(min_len * 0.6)

            if rewrite_mode:
                prompt = f"""
<length_recovery_prompt version="xml-v1" mode="full_rewrite">
  <role>ë‹¹ì‹ ì€ ì—„ê²©í•œ í•œêµ­ì–´ ì •ì¹˜ ì—ë””í„°ì…ë‹ˆë‹¤. í˜„ì¬ ì´ˆì•ˆì´ ì§€ë‚˜ì¹˜ê²Œ ì§§ìœ¼ë¯€ë¡œ ì™„ì „ ì¬ì‘ì„±í•©ë‹ˆë‹¤.</role>
  <goal>
    <current_chars>{best_len}</current_chars>
    <min_chars>{min_len}</min_chars>
    <max_chars>{max_len}</max_chars>
    <expected_h2>{expected_h2}</expected_h2>
  </goal>
  <rules>
    <rule order="1">ìµœì¢… ê²°ê³¼ëŠ” ì™„ì„±í˜• ë³¸ë¬¸ì´ì–´ì•¼ í•˜ë©°, ê°œìš”/ìš”ì•½/ì˜ˆì‹œ ê¸ˆì§€.</rule>
    <rule order="2">íƒœê·¸ëŠ” &lt;h2&gt;, &lt;p&gt;ë§Œ ì‚¬ìš©.</rule>
    <rule order="3">ë„ì… 1 + ë³¸ë¡ /ê²°ë¡  êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê³  ë¶„ëŸ‰ì„ ì¶©ì¡±.</rule>
    <rule order="4">ì¶œë ¥ì—ëŠ” title/content XML ì™¸ ì„¤ëª…ë¬¸ ê¸ˆì§€.</rule>
  </rules>
  <topic>{_xml_cdata(topic)}</topic>
  <author_bio>{_xml_cdata((author_bio or '(ì—†ìŒ)')[:1800])}</author_bio>
  <draft>
    <draft_title>{_xml_cdata(best_title)}</draft_title>
    <draft_content>{_xml_cdata(best_content)}</draft_content>
  </draft>
  <output_contract>
    <format>XML</format>
    <allowed_tags>title, content</allowed_tags>
    <example>{_xml_cdata('<title>...</title>\n<content>...</content>')}</example>
  </output_contract>
</length_recovery_prompt>
""".strip()
            else:
                prompt = f"""
<length_recovery_prompt version="xml-v1" mode="expand_only">
  <role>ë‹¹ì‹ ì€ ì—„ê²©í•œ í•œêµ­ì–´ ì •ì¹˜ ì—ë””í„°ì…ë‹ˆë‹¤. ê¸°ì¡´ íë¦„ì„ ìœ ì§€í•˜ë©° ë¶„ëŸ‰ë§Œ ë³´ê°•í•©ë‹ˆë‹¤.</role>
  <goal>
    <current_chars>{best_len}</current_chars>
    <min_chars>{min_len}</min_chars>
    <max_chars>{max_len}</max_chars>
    <required_additional_chars>{gap}</required_additional_chars>
    <expected_h2>{expected_h2}</expected_h2>
  </goal>
  <rules>
    <rule order="1">ê¸°ì¡´ &lt;h2&gt; ì œëª© ì‚­ì œ/ë³€ê²½ ê¸ˆì§€.</rule>
    <rule order="2">&lt;h2&gt; ê°œìˆ˜ëŠ” ì •í™•íˆ {expected_h2}ê°œ ìœ ì§€.</rule>
    <rule order="3">ë¬¸ë‹¨ì€ &lt;p&gt;...&lt;/p&gt;ë§Œ ì‚¬ìš©í•˜ê³  íƒœê·¸ë¥¼ ì •í™•íˆ ë‹«ì„ ê²ƒ.</rule>
    <rule order="4">ê¸°ì¡´ ì‚¬ì‹¤/ì£¼ì¥ì„ ì‚­ì œí•˜ê±°ë‚˜ ì™œê³¡í•˜ì§€ ë§ ê²ƒ.</rule>
    <rule order="5">ì¤‘ë³µ/ë°˜ë³µ ê¸ˆì§€. ê° ë‹¨ë½ì€ ìƒˆë¡œìš´ ê·¼ê±°/ì„¤ëª…ìœ¼ë¡œ ë³´ê°•.</rule>
    <rule order="6">ìµœì¢… ë¶„ëŸ‰ì€ {min_len}~{max_len}ì ë²”ìœ„ë¥¼ ë°˜ë“œì‹œ ì¶©ì¡±.</rule>
  </rules>
  <topic>{_xml_cdata(topic)}</topic>
  <author_bio>{_xml_cdata((author_bio or '(ì—†ìŒ)')[:1800])}</author_bio>
  <draft>
    <draft_title>{_xml_cdata(best_title)}</draft_title>
    <draft_content>{_xml_cdata(best_content)}</draft_content>
  </draft>
  <output_contract>
    <format>XML</format>
    <allowed_tags>title, content</allowed_tags>
    <example>{_xml_cdata('<title>...</title>\n<content>...</content>')}</example>
  </output_contract>
</length_recovery_prompt>
""".strip()

            try:
                response_text = await generate_content_async(
                    prompt,
                    model_name=self.model_name,
                    temperature=0.0,
                    max_output_tokens=8192,
                )
                parsed = self.parse_response(response_text)
                recovered_content = normalize_html_structure_tags(normalize_artifacts(parsed.get('content', '')))
                recovered_title = normalize_artifacts(parsed.get('title', '')) or best_title
                if not recovered_content:
                    continue

                recovered_len = len(strip_html(recovered_content))
                print(
                    f"ğŸ”§ [StructureAgent] ë¶„ëŸ‰ ë³´ê°• ì‹œë„ {recovery_attempt}/{max_recovery_attempts}: "
                    f"{best_len}ì -> {recovered_len}ì"
                )
                if recovered_len > best_len:
                    best_content = recovered_content
                    best_title = recovered_title
                    best_len = recovered_len

                if recovered_len >= min_len:
                    return recovered_content, recovered_title
            except Exception as e:
                print(f"âš ï¸ [StructureAgent] ë¶„ëŸ‰ ë³´ê°• ë³µêµ¬ ì‹¤íŒ¨: {str(e)}")

        if best_len > current_len:
            print(
                f"âš ï¸ [StructureAgent] ë¶„ëŸ‰ ê¸°ì¤€ ë¯¸ë‹¬ì´ì§€ë§Œ ë³´ê°• ê°œì„ : "
                f"{current_len}ì -> {best_len}ì"
            )
            return best_content, best_title
        return None

    async def recover_structural_shortfall(
        self,
        *,
        content: str,
        title: str,
        topic: str,
        length_spec: Dict[str, int],
        author_bio: str = '',
        failed_code: str,
        failed_reason: str,
        failed_feedback: str,
    ) -> Optional[Tuple[str, str]]:
        from ..common.gemini_client import generate_content_async

        current_len = len(strip_html(content))
        min_len = int(length_spec.get('min_chars', 0))
        max_len = int(length_spec.get('max_chars', 0))
        expected_h2 = int(length_spec.get('expected_h2', 0))
        total_sections = int(length_spec.get('total_sections', 5))
        min_p = total_sections * 2
        max_p = total_sections * 4

        prompt = f"""
<structural_recovery_prompt version="xml-v1">
  <role>ë‹¹ì‹ ì€ ì—„ê²©í•œ í¸ì§‘ìì…ë‹ˆë‹¤. ì•„ë˜ ì›ê³ ëŠ” êµ¬ì¡°/í˜•ì‹ ê²€ì¦ì— ì‹¤íŒ¨í–ˆìœ¼ë¯€ë¡œ ì™„ì „ êµì •í•©ë‹ˆë‹¤.</role>
  <failure>
    <code>{_xml_text(failed_code)}</code>
    <reason>{_xml_cdata(failed_reason)}</reason>
    <feedback>{_xml_cdata(failed_feedback)}</feedback>
  </failure>
  <goal>
    <current_chars>{current_len}</current_chars>
    <target_chars>{min_len}~{max_len}</target_chars>
    <expected_h2>{expected_h2}</expected_h2>
    <expected_p>{min_p}~{max_p}</expected_p>
  </goal>
  <rules>
    <rule order="1">í—ˆìš© íƒœê·¸ëŠ” &lt;h2&gt;, &lt;p&gt;ë§Œ ì‚¬ìš©.</rule>
    <rule order="2">ëª¨ë“  &lt;h2&gt;, &lt;p&gt; íƒœê·¸ë¥¼ ì •í™•íˆ ì—´ê³  ë‹«ì„ ê²ƒ.</rule>
    <rule order="3">ë³¸ë¬¸ì— ì˜ˆì‹œ í”Œë ˆì´ìŠ¤í™€ë”([ì œëª©], [ë‚´ìš©], [êµ¬ì²´ì  ëŒ€ì•ˆ] ë“±)ë¥¼ ë‚¨ê¸°ì§€ ë§ ê²ƒ.</rule>
    <rule order="4">ê¸°ì¡´ í•µì‹¬ ì˜ë¯¸/ì‚¬ì‹¤ì€ ìœ ì§€í•˜ë˜ í˜•ì‹ê³¼ êµ¬ì¡°ë¥¼ ì™„ì „ êµì •í•  ê²ƒ.</rule>
    <rule order="5">ë¶„ëŸ‰ ë¶€ì¡±ì´ë©´ êµ¬ì²´ ê·¼ê±°ë¥¼ ë³´ê°•í•˜ê³ , ë¶„ëŸ‰ ì´ˆê³¼ë©´ ì¤‘ë³µì„ ì••ì¶•í•  ê²ƒ.</rule>
    <rule order="6">ìµœì¢… ì‘ë‹µì€ title/content XML íƒœê·¸ë§Œ ì¶œë ¥í•  ê²ƒ.</rule>
    <rule order="7">ì‹¤íŒ¨ ì½”ë“œ({ _xml_text(failed_code) })ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ í•´ê²°í•˜ê³ , ë™ì¼ ì‹¤íŒ¨ ì½”ë“œê°€ ì¬ë°œí•˜ì§€ ì•Šê²Œ ì¬ì‘ì„±í•  ê²ƒ.</rule>
    <rule order="8">ë°˜ë³µ ê´€ë ¨ ì‹¤íŒ¨ ì½”ë“œë¼ë©´ ë™ì¼ ì–´êµ¬ ë°˜ë³µì„ ì¤„ì´ê³ , ì´ˆê³¼ ë¶€ë¶„ì€ ìƒˆë¡œìš´ ì‚¬ì‹¤/ê·¼ê±°/í–‰ë™ ë¬¸ì¥ìœ¼ë¡œ ì¹˜í™˜í•  ê²ƒ(ì˜ë¯¸ ë³´ì¡´).</rule>
    <rule order="9">ê²€ì¦ ê·œì¹™ ì„¤ëª…ë¬¸ì´ë‚˜ ë©”íƒ€ ë¬¸ì¥ì„ ë³¸ë¬¸ìœ¼ë¡œ ì¶œë ¥í•˜ì§€ ë§ ê²ƒ.</rule>
  </rules>
  <topic>{_xml_cdata(topic)}</topic>
  <author_bio>{_xml_cdata((author_bio or '(ì—†ìŒ)')[:1800])}</author_bio>
  <draft>
    <draft_title>{_xml_cdata(title)}</draft_title>
    <draft_content>{_xml_cdata(content)}</draft_content>
  </draft>
  <output_contract>
    <format>XML</format>
    <allowed_tags>title, content</allowed_tags>
    <example>{_xml_cdata('<title>...</title>\n<content>...</content>')}</example>
  </output_contract>
</structural_recovery_prompt>
""".strip()

        try:
            response_text = await generate_content_async(
                prompt,
                model_name=self.model_name,
                temperature=0.1,
                max_output_tokens=8192,
            )
            parsed = self.parse_response(response_text)
            recovered_content = normalize_html_structure_tags(normalize_artifacts(parsed.get('content', '')))
            recovered_title = normalize_artifacts(parsed.get('title', '')) or title
            if not recovered_content:
                return None
            return recovered_content, recovered_title
        except Exception as e:
            print(f"âš ï¸ [StructureAgent] êµ¬ì¡°/ë¶„ëŸ‰ ë³´ê°• ë³µêµ¬ ì‹¤íŒ¨: {str(e)}")
            return None

    async def run_context_analyzer(self, stance_text: str, news_data_text: str, author_name: str) -> Optional[Dict]:
        from ..common.gemini_client import generate_content_async

        stance_len = len(strip_html(stance_text or ""))
        news_len = len(strip_html(news_data_text or ""))
        # ì…ì¥ë¬¸ì´ ì§§ì•„ë„ ë‰´ìŠ¤/ëŒ€ì²´ìë£Œê°€ ì¶©ë¶„í•˜ë©´ ë¶„ì„ ì§„í–‰
        if stance_len < 50:
            if news_len >= 80:
                print(
                    f"âš ï¸ [StructureAgent] ì…ì¥ë¬¸ì´ ì§§ìŒ ({stance_len}ì) - "
                    f"ë‰´ìŠ¤/ëŒ€ì²´ìë£Œ({news_len}ì) ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„ ì§„í–‰"
                )
                stance_text = normalize_context_text([stance_text, news_data_text], sep="\n\n")
            else:
                print(
                    f"âš ï¸ [StructureAgent] ì…ì¥ë¬¸/ë‰´ìŠ¤ ëª¨ë‘ ì§§ìŒ "
                    f"(stance={stance_len}ì, news={news_len}ì) - ContextAnalyzer ìŠ¤í‚µ"
                )
                return None

        print(f'ğŸ” [StructureAgent] ContextAnalyzer ì‹¤í–‰... (ì…ì¥ë¬¸: {len(stance_text)}ì, ë‰´ìŠ¤: {len(news_data_text)}ì)')
        start_time = time.time()

        if not news_data_text:
            # ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì…ì¥ë¬¸ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„
            print(f"âš ï¸ [StructureAgent] ë‰´ìŠ¤ ë°ì´í„° ì—†ìŒ - ì…ì¥ë¬¸ë§Œìœ¼ë¡œ ë¶„ì„ ì§„í–‰")

        news_preview = news_data_text[:2000] if news_data_text else '(ì—†ìŒ)'
        context_json_example = """{
  "intent": "donation_request",
  "contentStrategy": {
    "tone": "ê°ì„± í˜¸ì†Œ",
    "structure": "ìŠ¤í† ë¦¬í…”ë§ â†’ ë¹„ì „ â†’ CTA",
    "emphasis": ["í›„ì› ë™ì°¸ ìœ ë„", "ì§„ì •ì„± ì „ë‹¬"]
  },
  "mustIncludeFromStance": [
    {
      "topic": "í•µì‹¬ ì£¼ì¥ 1",
      "expansion_why": "ë°°ê²½...",
      "expansion_how": "ë°©ì•ˆ...",
      "expansion_effect": "íš¨ê³¼..."
    }
  ],
  "mustIncludeFacts": [],
  "mustPreserve": {
    "bankName": "ì‹ í•œì€í–‰",
    "accountNumber": "140016005619",
    "accountHolder": "ì´ì¬ì„± í›„ì›íšŒ",
    "contactNumber": "01097262663",
    "instruction": "ì…ê¸ˆ í›„ ì„±í•¨ ë¬¸ì â†’ ì˜ìˆ˜ì¦ ë°œê¸‰",
    "eventDate": null,
    "eventLocation": null,
    "ctaPhrase": "ì§€ê¸ˆ ë°”ë¡œ í•¨ê»˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
  }
}"""

        context_prompt = f"""
<context_analyzer_prompt version="xml-v1">
  <role>ë‹¹ì‹ ì€ ì •ì¹˜ ì½˜í…ì¸  ì „ëµê°€ì…ë‹ˆë‹¤. ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•´ ë¸”ë¡œê·¸ ì½˜í…ì¸  ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.</role>
  <inputs>
    <stance_text>{_xml_cdata(stance_text[:2500])}</stance_text>
    <news_or_data>{_xml_cdata(news_preview)}</news_or_data>
    <author_name>{_xml_text(author_name)}</author_name>
  </inputs>
  <analysis_tasks>
    <intent_selection>
      <description>ì•„ë˜ ì¤‘ ê°€ì¥ ì í•©í•œ ì˜ë„ í•˜ë‚˜ë§Œ ì„ íƒ</description>
      <option key="donation_request">í›„ì› ìš”ì²­ (ê³„ì¢Œ/ì—°ë½ì²˜ í¬í•¨)</option>
      <option key="policy_promotion">ì •ì±…/ë¹„ì „ í™ë³´</option>
      <option key="event_announcement">ì¼ì •/í–‰ì‚¬ ì•ˆë‚´</option>
      <option key="activity_report">í™œë™ ë³´ê³ </option>
      <option key="personal_message">ê°œì¸ ì†Œí†µ/ì¸ì‚¬</option>
    </intent_selection>
    <content_strategy>
      <field name="tone">í†¤ì•¤ë§¤ë„ˆ (ì˜ˆ: ê°ì„± í˜¸ì†Œ, ë…¼ë¦¬ì  ì„¤ë“, ì •ë³´ ì „ë‹¬, ì¹œê·¼í•œ ì†Œí†µ)</field>
      <field name="structure">ì „ê°œ êµ¬ì¡° (ì˜ˆ: ìŠ¤í† ë¦¬í…”ë§â†’ë¹„ì „â†’CTA / ë¬¸ì œâ†’í•´ë²•â†’íš¨ê³¼ / ì¼ì •â†’ë‚´ìš©â†’ì°¸ì—¬ë°©ë²•)</field>
      <field name="emphasis">ê°•ì¡° í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸</field>
    </content_strategy>
    <must_include_from_stance max_items="3">
      <description>ê¸€ì“´ì´({_xml_text(author_name)})ì˜ í•µì‹¬ ì£¼ì¥ ì¶”ì¶œ</description>
      <field name="topic">í•µì‹¬ ì£¼ì¥ (ê°„ê²°í•œ ë¬¸ì¥)</field>
      <field name="expansion_why">ì´ ì£¼ì¥ì´ í•„ìš”í•œ ë°°ê²½</field>
      <field name="expansion_how">êµ¬ì²´ì  ì‹¤í˜„ ë°©ì•ˆ</field>
      <field name="expansion_effect">ê¸°ëŒ€ë˜ëŠ” íš¨ê³¼</field>
    </must_include_from_stance>
    <must_preserve critical="true">
      <description>ì›ë¬¸ì—ì„œ ì ˆëŒ€ ëˆ„ë½ë˜ë©´ ì•ˆ ë˜ëŠ” êµ¬ì²´ ì •ë³´ë§Œ ì¶”ì¶œ</description>
      <field name="bankName">ì€í–‰ëª… (ì—†ìœ¼ë©´ null)</field>
      <field name="accountNumber">ê³„ì¢Œë²ˆí˜¸ (ì—†ìœ¼ë©´ null)</field>
      <field name="accountHolder">ì˜ˆê¸ˆì£¼ (ì—†ìœ¼ë©´ null)</field>
      <field name="contactNumber">ì—°ë½ì²˜ (ì—†ìœ¼ë©´ null)</field>
      <field name="instruction">ì•ˆë‚´ ë¬¸êµ¬ (ì—†ìœ¼ë©´ null)</field>
      <field name="eventDate">ì¼ì‹œ (ì—†ìœ¼ë©´ null)</field>
      <field name="eventLocation">ì¥ì†Œ (ì—†ìœ¼ë©´ null)</field>
      <field name="ctaPhrase">CTA ë¬¸êµ¬ (ì—†ìœ¼ë©´ null)</field>
    </must_preserve>
  </analysis_tasks>
  <output_contract>
    <format>JSON only</format>
    <rules>
      <rule order="1">ë°˜ë“œì‹œ JSON ê°ì²´ í•˜ë‚˜ë§Œ ì¶œë ¥</rule>
      <rule order="2">ì½”ë“œë¸”ë¡, XML, ë¶€ê°€ ì„¤ëª…ë¬¸ ì¶œë ¥ ê¸ˆì§€</rule>
      <rule order="3">í‚¤ ëˆ„ë½ ì‹œ null ë˜ëŠ” ë¹ˆ ë°°ì—´ì„ ì‚¬ìš©</rule>
    </rules>
    <json_example>{_xml_cdata(context_json_example)}</json_example>
  </output_contract>
</context_analyzer_prompt>
""".strip()

        try:
            response_text = await generate_content_async(
                context_prompt,
                model_name=self.model_name,
                temperature=0.0,  # ë¶„ì„ì€ ì •í™•ë„ ìš°ì„ 
                response_mime_type='application/json'
            )

            analysis = json.loads(response_text)

            elapsed = time.time() - start_time
            print(f"âœ… [StructureAgent] ContextAnalyzer ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")

            # Filter phrases
            if 'mustIncludeFromStance' in analysis and isinstance(analysis['mustIncludeFromStance'], list):
                filtered_list = []
                for item in analysis['mustIncludeFromStance']:
                    # ê¸°ì¡´ ë¬¸ìì—´ í˜¸í™˜ì„± (stringì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
                    if isinstance(item, str) and len(item.strip()) >= 5:
                         filtered_list.append({'topic': item, 'expansion_why': '', 'expansion_how': '', 'expansion_effect': ''})
                    # ë”•ì…”ë„ˆë¦¬ êµ¬ì¡° í•„í„°ë§
                    elif isinstance(item, dict) and item.get('topic'):
                        topic = item.get('topic', '').strip()
                        if len(topic) >= 2 and not topic.startswith('âš ï¸'):
                            filtered_list.append(item)
                analysis['mustIncludeFromStance'] = filtered_list

            analysis = self._normalize_context_analysis_materials(analysis)
            return analysis
        except Exception as e:
            elapsed = time.time() - start_time
            error_msg = str(e)
            print(f"âš ï¸ [StructureAgent] ContextAnalyzer ì‹¤íŒ¨ ({elapsed:.1f}ì´ˆ): {error_msg} - ê±´ë„ˆëœ€")
            return None

    def build_prompt(self, params: Dict[str, Any]) -> str:
        # Extract params
        writing_method = params.get('writingMethod')
        template_builder = TEMPLATE_BUILDERS.get(writing_method, build_daily_communication_prompt)

        # userProfile ë°©ì–´ ì½”ë“œ - listë¡œ ì „ë‹¬ë˜ëŠ” ê²½ìš° ë°©ì–´
        user_profile = params.get('userProfile', {})
        if not isinstance(user_profile, dict):
            user_profile = {}
        news_source_mode = str(params.get('newsSourceMode') or 'news').strip().lower()
        profile_support_context = normalize_context_text(params.get('profileSupportContext'))
        profile_substitute_context = normalize_context_text(params.get('profileSubstituteContext'))
        personalization_context = normalize_context_text(
            params.get('personalizationContext') or params.get('memoryContext'),
            sep="\n",
        )

        # Build base template prompt
        template_prompt = template_builder({
            'topic': params.get('topic'),
            'authorBio': params.get('authorBio'),
            'authorName': params.get('authorName'),
            'instructions': params.get('instructions'),
            'keywords': params.get('userKeywords'),
            'targetWordCount': params.get('targetWordCount'),
            'personalizedHints': personalization_context,
            'newsContext': params.get('newsContext'),
            'isCurrentLawmaker': self.is_current_lawmaker(user_profile),
            'politicalExperience': user_profile.get('politicalExperience', 'ì •ì¹˜ ì‹ ì¸'),
            'familyStatus': user_profile.get('familyStatus', '')
        })

        # Reference Materials Section
        instructions_text = normalize_context_text(params.get('instructions'))
        news_context_text = normalize_context_text(params.get('newsContext'))
        source_blocks = [instructions_text]
        if news_context_text:
            source_blocks.append(news_context_text)
        bio_source_line = ""
        bio_source_rule = "ë³´ì¡° ìë£Œ: ì‚¬ìš©ì í”„ë¡œí•„(Bio)ì€ í™”ì ì •ì²´ì„±ê³¼ ì–´ì¡° ì°¸ê³ ìš©ì´ë©°, ë¶„ëŸ‰ì´ ë¶€ì¡±í•  ë•Œë§Œ í™œìš©í•˜ì„¸ìš”."
        if news_source_mode == 'profile_fallback' and profile_substitute_context:
            source_blocks.append(f"[ë‰´ìŠ¤/ë°ì´í„° ëŒ€ì²´ìë£Œ]\n{profile_substitute_context}")
            bio_source_line = "- ëŒ€ì²´ ìë£Œ: ì‚¬ìš©ì ì¶”ê°€ì •ë³´(ê³µì•½/ë²•ì•ˆ/ì„±ê³¼) ë¬´ì‘ìœ„ 3ê°œ + Bio ë³´ê°•"
            bio_source_rule = (
                "ëŒ€ì²´ìë£Œ í™œìš©: ë‰´ìŠ¤/ë°ì´í„°ê°€ ë¹„ì–´ ìˆìœ¼ë¯€ë¡œ ì‚¬ìš©ì ì¶”ê°€ì •ë³´(ê³µì•½/ë²•ì•ˆ/ì„±ê³¼)ì™€ "
                "Bio ë³´ê°• ë§¥ë½ì—ì„œ íŒ©íŠ¸ë¥¼ ì¶”ì¶œí•´ ì‚¬ìš©í•˜ì„¸ìš”. ëŒ€ì²´ìë£Œ 3ê°œëŠ” ë§¤ ìš”ì²­ë§ˆë‹¤ ë¬´ì‘ìœ„ ì„ ì •ë©ë‹ˆë‹¤."
            )
        elif not news_context_text and profile_support_context:
            source_blocks.append(f"[ì‘ì„±ì BIO ë³´ê°• ë§¥ë½]\n{profile_support_context}")
            bio_source_line = "- ë³´ê°• ìë£Œ: ì‚¬ìš©ì Bio (ê²½ë ¥/ì´ë ¥/ê°€ì¹˜)"
            bio_source_rule = (
                "Bio ë³´ê°• í™œìš©: ë‰´ìŠ¤/ë°ì´í„°ì™€ êµ¬ì¡°í™” ì¶”ê°€ì •ë³´ê°€ ëª¨ë‘ ë¶€ì¡±í•˜ë¯€ë¡œ "
                "ì‚¬ìš©ì Bioì—ì„œ í™•ì¸ ê°€ëŠ¥í•œ ê²½ë ¥/ì„±ê³¼/í•µì‹¬ê°€ì¹˜ë¥¼ ì‚¬ì‹¤ ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”."
            )

        source_text = "\n\n---\n\n".join(block for block in source_blocks if block)
        ref_section = ""
        if source_text.strip():
            ref_section = f"""
<reference_materials priority="critical">
  <overview>ì•„ë˜ ì°¸ê³ ìë£Œê°€ ì´ ì›ê³ ì˜ 1ì°¨ ìë£Œ(Primary Source)ì…ë‹ˆë‹¤.</overview>
  <source_order>
    <item order="1">ì²« ë²ˆì§¸ ìë£Œ: ì‘ì„±ìì˜ ì…ì¥ë¬¸/í˜ì´ìŠ¤ë¶ ê¸€ (í•µì‹¬ ë…¼ì¡°ì™€ ì£¼ì¥)</item>
    <item order="2">ì´í›„ ìë£Œ: ë‰´ìŠ¤/ë°ì´í„° (ê·¼ê±°, íŒ©íŠ¸, ë°°ê²½ ì •ë³´)</item>
    {'<item order="3">' + _xml_text(bio_source_line) + '</item>' if bio_source_line else ''}
  </source_order>
  <source_body>{_xml_cdata(source_text[:6000])}</source_body>
  <processing_rules>
    <rule order="1">ì •ë³´ ì¶”ì¶œ: í•µì‹¬ íŒ©íŠ¸, ìˆ˜ì¹˜, ë…¼ì ë§Œ ì‚¬ìš©</rule>
    <rule order="2">ì¬ì‘ì„± í•„ìˆ˜: ì°¸ê³ ìë£Œ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ì•ŠìŒ</rule>
    <rule order="3">êµ¬ì–´ì²´ë¥¼ ë¬¸ì–´ì²´ë¡œ ë³€í™˜</rule>
    <rule order="4">ì°½ì‘ ê¸ˆì§€: ì°¸ê³ ìë£Œì— ì—†ëŠ” íŒ©íŠ¸/ìˆ˜ì¹˜ ìƒì„± ê¸ˆì§€</rule>
    <rule order="5">ì£¼ì œ ìœ ì§€: ì°¸ê³ ìë£Œ í•µì‹¬ ì£¼ì œ ì´íƒˆ ê¸ˆì§€</rule>
    <rule order="6">{_xml_text(bio_source_rule)}</rule>
  </processing_rules>
  <forbidden_examples>
    <example type="source">{_xml_cdata('ì •í™•í•˜ê²Œ ì–˜ê¸°ë¥¼ í•˜ë©´ ê·¸ë˜ì„œ ì°½ì˜ì ì´ê³  ì •ë§ ì••ë„ì ì¸...')}</example>
    <example type="bad">{_xml_cdata('ì •í™•í•˜ê²Œ ì–˜ê¸°ë¥¼ í•˜ë©´ ê·¸ë˜ì„œ ì°½ì˜ì ì´ê³ ...')}</example>
    <example type="good">{_xml_cdata('ì°½ì˜ì ì´ê³  ì••ë„ì ì¸ ì½˜í…ì¸  ê¸°ë°˜ ì „ëµì´ í•µì‹¬ì…ë‹ˆë‹¤.')}</example>
  </forbidden_examples>
</reference_materials>
"""
            print(f"ğŸ“š [StructureAgent] ì°¸ê³ ìë£Œ ì£¼ì… ì™„ë£Œ: {len(source_text)}ì")
        else:
            print("âš ï¸ [StructureAgent] ì°¸ê³ ìë£Œ ì—†ìŒ - ì‚¬ìš©ì í”„ë¡œí•„ë§Œìœ¼ë¡œ ìƒì„±")

        # Context Injection
        context_injection = ""
        is_event_announcement = False
        event_date_hint = ""
        event_location_hint = ""
        event_contact_hint = ""
        event_cta_hint = ""
        intro_anchor_topic = ""
        intro_anchor_why = ""
        intro_anchor_effect = ""
        intro_seed = ""

        intro_seed_candidates = self._split_into_context_items(instructions_text, min_len=10, max_items=6)
        if not intro_seed_candidates and profile_substitute_context:
            intro_seed_candidates = self._split_into_context_items(profile_substitute_context, min_len=10, max_items=6)
        if not intro_seed_candidates and news_context_text:
            intro_seed_candidates = self._split_into_context_items(news_context_text, min_len=10, max_items=6)
        if not intro_seed_candidates:
            intro_seed_candidates = self._split_into_context_items(
                normalize_context_text(params.get('topic')),
                min_len=6,
                max_items=2,
            )
        if intro_seed_candidates:
            intro_seed = intro_seed_candidates[0]

        raw_context_analysis = params.get('contextAnalysis')
        context_analysis = (
            self._normalize_context_analysis_materials(raw_context_analysis)
            if isinstance(raw_context_analysis, dict)
            else {}
        )
        if context_analysis:
            stance_list = context_analysis.get('mustIncludeFromStance', [])
            
            # êµ¬ì¡°í™”ëœ stance ì²˜ë¦¬
            formatted_stances = []
            for i, p in enumerate(stance_list):
                if isinstance(p, dict):
                    topic = p.get('topic', '')
                    why_txt = p.get('expansion_why', '')
                    how_txt = p.get('expansion_how', '')
                    eff_txt = p.get('expansion_effect', '')
                    
                    block = f"""
<stance index="{i+1}" section_hint="ë³¸ë¡  {i+1}">
  <topic>{_xml_text(topic)}</topic>
  <why>{_xml_text(why_txt)}</why>
  <how>{_xml_text(how_txt)}</how>
  <effect>{_xml_text(eff_txt)}</effect>
</stance>"""
                    formatted_stances.append(block.strip())
                else:
                    # Fallback for string (legacy)
                    formatted_stances.append(
                        f"<stance index=\"{i+1}\" section_hint=\"ë³¸ë¡  {i+1}\"><topic>{_xml_text(p)}</topic></stance>"
                    )

            stance_phrases = "\n\n".join(formatted_stances)
            stance_count = len(stance_list)
            if stance_list:
                first = stance_list[0]
                if isinstance(first, dict):
                    intro_anchor_topic = normalize_context_text(first.get('topic'))
                    intro_anchor_why = normalize_context_text(first.get('expansion_why'))
                    intro_anchor_effect = normalize_context_text(first.get('expansion_effect'))
                else:
                    intro_anchor_topic = normalize_context_text(first)
             
            if stance_count > 0:
                context_injection = f"""
<body_expansion mandatory="true">
  <description>ì•„ë˜ {stance_count}ê°œ ì„¤ê³„ë„ì— ë”°ë¼ ë³¸ë¡  ì„¹ì…˜ì„ í™•ì¥í•©ë‹ˆë‹¤.</description>
  <stance_count>{stance_count}</stance_count>
  <stance_blueprints>
{stance_phrases}
  </stance_blueprints>
  <instructions>
    <instruction order="1">ê° ì£¼ì œë¥¼ ë³„ë„ì˜ ë³¸ë¡  ì„¹ì…˜(H2)ìœ¼ë¡œ êµ¬ì„±</instruction>
    <instruction order="2">ê° ì„¹ì…˜ì— Why/How/Effect ë…¼ë¦¬ë¥¼ í•µì‹¬ ìœ„ì£¼ë¡œ ë°˜ì˜</instruction>
    <instruction order="3">How ë‹¨ê³„ì—ì„œ Bio(ê²½ë ¥)ë¥¼ ê·¼ê±°ë¡œ ì „ë¬¸ì„±ì„ ì œì‹œ</instruction>
  </instructions>
</body_expansion>
"""

                intro_anchor_summary = " / ".join(
                    part for part in [intro_anchor_topic, intro_anchor_why, intro_anchor_effect] if part
                ).strip()
                if intro_anchor_summary:
                    context_injection += f"""
<intro_anchor mandatory="true">
  <description>ì„œë¡  1~2ë¬¸ë‹¨ì€ ì…ì¥ë¬¸ í•µì‹¬ ìš”ì§€ë¥¼ ì¬ì§„ìˆ í•˜ê³  ë³¸ë¡ ìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.</description>
  <anchor>{_xml_text(intro_anchor_summary)}</anchor>
</intro_anchor>
"""

            # ğŸ”´ [NEW] contentStrategy ì£¼ì…
            content_strategy = context_analysis.get('contentStrategy', {})
            if content_strategy:
                tone = content_strategy.get('tone', '')
                structure = content_strategy.get('structure', '')
                emphasis = content_strategy.get('emphasis', [])
                
                if tone or structure:
                    emphasis_str = ", ".join(emphasis) if emphasis else "ì—†ìŒ"
                    context_injection += f"""
<content_strategy>
  <tone>{_xml_text(tone)}</tone>
  <structure>{_xml_text(structure)}</structure>
  <emphasis>{_xml_text(emphasis_str)}</emphasis>
</content_strategy>
"""
                    print(f"ğŸ¯ [StructureAgent] ì½˜í…ì¸  ì „ëµ ì£¼ì…: {tone} / {structure}")

            # ğŸ”´ [NEW] mustPreserve ê¸°ë°˜ CTA ì •ë³´ ì£¼ì…
            must_preserve = context_analysis.get('mustPreserve', {})
            intent = context_analysis.get('intent', '')
            
            if must_preserve and intent == 'donation_request':
                # ìŠ¬ë¡œê±´/í›„ì› ì•ˆë‚´ëŠ” ìµœì¢… ì¶œë ¥ ì§ì „ì—ë§Œ ë¶€ì°©í•œë‹¤.
                # ë³¸ë¬¸ ìƒì„± ë‹¨ê³„ì—ì„œëŠ” ê³„ì¢Œ/ì—°ë½ì²˜/ì˜ìˆ˜ì¦ ë¬¸êµ¬ë¥¼ ì£¼ì…í•˜ì§€ ì•ŠëŠ”ë‹¤.
                print("ğŸ’¡ [StructureAgent] í›„ì› ì •ë³´ ë³¸ë¬¸ ì£¼ì… ìƒëµ (ìµœì¢… ì¶œë ¥ ë‹¨ê³„ì—ì„œë§Œ ë¶€ì°©)")

            # ğŸ”´ [NEW] í–‰ì‚¬ ì•ˆë‚´ ì •ë³´ ì£¼ì…
            elif must_preserve and intent == 'event_announcement':
                is_event_announcement = True
                event_date = must_preserve.get('eventDate')
                event_location = must_preserve.get('eventLocation')
                contact_number = must_preserve.get('contactNumber')
                cta_phrase = must_preserve.get('ctaPhrase')

                event_date_hint = str(event_date or '').strip()
                event_location_hint = str(event_location or '').strip()
                event_contact_hint = str(contact_number or '').strip()
                event_cta_hint = str(cta_phrase or '').strip()
                
                if event_date or event_location:
                    event_parts = []
                    if event_date:
                        event_parts.append(f"- ì¼ì‹œ: {event_date}")
                    if event_location:
                        event_parts.append(f"- ì¥ì†Œ: {event_location}")
                    if contact_number:
                        event_parts.append(f"- ë¬¸ì˜: {contact_number}")
                    
                    event_text = "\n".join(event_parts)
                    
                    context_injection += f"""
<event_context mandatory="true">
  <facts>{_xml_cdata(event_text)}</facts>
  <instructions>
    <instruction order="1">í–‰ì‚¬ ì •ë³´(ì¼ì‹œ/ì¥ì†Œ/ì°¸ì—¬ë°©ë²•)ë¥¼ ë„ì…ì—ì„œ ëª…í™•íˆ ì œì‹œ</instruction>
    <instruction order="2">ë™ì¼í•œ ì¼ì‹œ+ì¥ì†Œ ê²°í•© ë¬¸ì¥ì„ ë³¸ë¬¸ì—ì„œ ë°˜ë³µí•˜ì§€ ì•ŠìŒ</instruction>
    <instruction order="3">ê²°ë¡  CTAëŠ” í–‰ë™ ë™ì‚¬+êµ¬ì²´ ì¥ì†Œë¡œ 1íšŒë§Œ ì œì‹œ</instruction>
  </instructions>
</event_context>
"""
                    print(f"ğŸ“… [StructureAgent] í–‰ì‚¬ ì •ë³´ ì£¼ì…: {event_date} / {event_location}")

        if not intro_anchor_topic:
            intro_anchor_topic = intro_seed or normalize_context_text(params.get('topic'))

        # Warning Generation (XML)
        warning_blocks: List[str] = []
        non_lawmaker_warn = generate_non_lawmaker_warning(
            self.is_current_lawmaker(user_profile),
            user_profile.get('politicalExperience'),
            params.get('authorBio')
        )
        if non_lawmaker_warn:
            warning_blocks.append(
                f"<non_lawmaker_warning>{_xml_cdata(non_lawmaker_warn)}</non_lawmaker_warning>"
            )
        
        if params.get('authorBio') and '"' in params.get('authorBio', ''):
            warning_blocks.append(
                """
<bio_quote_rules priority="critical">
  <rule order="1">Bioì˜ í°ë”°ì˜´í‘œ(" ")ë¡œ ë¬¶ì¸ ë¬¸ì¥ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¸ìš©</rule>
  <rule order="2">ë”°ì˜´í‘œ ë¬¸ì¥ì˜ ë‹¨ì–´/ì¡°ì‚¬/ì–´ë¯¸ë¥¼ ì„ì˜ ìˆ˜ì •í•˜ì§€ ì•ŠìŒ</rule>
  <rule order="3">ì‚¬ëŒ ì´ë¦„ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ëŒ€ì²´í•˜ì§€ ì•ŠìŒ</rule>
  <examples>
    <bad><![CDATA["ë²Œì¨ êµ­íšŒì˜ì› í–ˆì„ í…ë°" -> "ë²Œì¨ í™ê¸¸ë™ í–ˆì„ í…ë°"]]></bad>
    <good><![CDATA["ë²Œì¨ êµ­íšŒì˜ì› í–ˆì„ í…ë°" (ì›ë¬¸ ê·¸ëŒ€ë¡œ)]]></good>
  </examples>
</bio_quote_rules>
""".strip()
            )

        bio_warning = ""
        if warning_blocks:
            bio_warning = "<warning_bundle>\n" + "\n".join(warning_blocks) + "\n</warning_bundle>"

        # Modified Structure Enforcement: Dynamic based on stance_count
        stance_count = 0
        if context_analysis:
            stance_count = len(context_analysis.get('mustIncludeFromStance', []))

        length_spec = params.get('lengthSpec') or self._build_length_spec(
            params.get('targetWordCount', 2000),
            stance_count
        )
        body_section_count = length_spec['body_sections']
        total_section_count = length_spec['total_sections']
        min_total_chars = length_spec['min_chars']
        max_total_chars = length_spec['max_chars']
        per_section_min = length_spec['per_section_min']
        per_section_max = length_spec['per_section_max']
        per_section_recommended = length_spec['per_section_recommended']
        material_uniqueness_guard = self._build_material_uniqueness_guard(
            context_analysis,
            body_sections=body_section_count,
        )

        intro_line_1 = '<p>1ë¬¸ë‹¨: í™”ì ì†Œê°œ + ì…ì¥ë¬¸ì—ì„œ ë“œëŸ¬ë‚œ ë¬¸ì œì˜ì‹ 1ê°€ì§€ë¥¼ ì¬ì§„ìˆ </p>'
        intro_line_2 = '<p>2ë¬¸ë‹¨: ì…ì¥ë¬¸ í•µì‹¬ ì£¼ì¥(ì›ë¬¸ ìš”ì§€)ì„ ì¬ì‘ì„±í•˜ì—¬ ê¸€ì˜ ëª©ì ì„ ëª…í™•íˆ ì œì‹œ</p>'
        intro_line_3 = '<p>3ë¬¸ë‹¨: ë³¸ë¡ ì—ì„œ ë‹¤ë£° í•´ê²° ë°©í–¥/í–‰ë™ ì œì•ˆì„ ì˜ˆê³ </p>'
        intro_stance_rules = f"""
  <intro_stance_binding priority="critical">
    <rule id="intro_must_anchor_stance">ì„œë¡  2ë¬¸ë‹¨ ì´ë‚´ì— ì…ì¥ë¬¸ í•µì‹¬ ì£¼ì¥ ë˜ëŠ” ë¬¸ì œì˜ì‹ì„ ë°˜ë“œì‹œ ì¬ì§„ìˆ í•  ê²ƒ.</rule>
    <rule id="intro_no_generic_opening">ë§¥ë½ ì—†ëŠ” ì¼ë°˜ ì¸ì‚¿ë§/ìƒíˆ¬ì  ë„ì…ìœ¼ë¡œ ì‹œì‘í•˜ì§€ ë§ ê²ƒ.</rule>
    <rule id="intro_paraphrase_required">ì…ì¥ë¬¸ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë³µë¶™í•˜ì§€ ë§ê³  ì˜ë¯¸ëŠ” ìœ ì§€í•œ ì±„ ì¬ì‘ì„±í•  ê²ƒ.</rule>
    <rule id="intro_to_body_bridge">ì„œë¡  ë§ˆì§€ë§‰ ë¬¸ì¥ì—ì„œ ë³¸ë¡  ì£¼ì œë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•  ê²ƒ.</rule>
    <stance_seed>{intro_seed or '(ì…ì¥ë¬¸ ìš”ì§€ ì—†ìŒ)'}</stance_seed>
    <stance_anchor_topic>{intro_anchor_topic or '(ë¯¸ì§€ì •)'}</stance_anchor_topic>
  </intro_stance_binding>
"""
        event_mode_rules = ''
        if is_event_announcement:
            intro_line_1 = '<p>1ë¬¸ë‹¨: í™”ì ì‹¤ëª… + í–‰ì‚¬ ëª©ì ì„ 2ë¬¸ì¥ ì´ë‚´ë¡œ ëª…í™•íˆ ì œì‹œ</p>'
            intro_line_2 = '<p>2ë¬¸ë‹¨: í–‰ì‚¬ í•µì‹¬ì •ë³´(ì¼ì‹œ/ì¥ì†Œ/ì°¸ì—¬ë°©ë²•/ë¬¸ì˜)ë¥¼ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ì••ì¶• ì œì‹œ</p>'
            intro_line_3 = '<p>3ë¬¸ë‹¨: ì…ì¥ë¬¸ì˜ ë¬¸ì œì˜ì‹/í•µì‹¬ ë©”ì‹œì§€ê°€ í–‰ì‚¬ì—ì„œ ì–´ë–»ê²Œ ë‹¤ë¤„ì§€ëŠ”ì§€ ì œì‹œ</p>'
            event_mode_rules = f"""
  <event_mode intent="event_announcement" priority="critical">
    <facts>
      <event_date>{event_date_hint or '(ë¯¸ìƒ)'}</event_date>
      <event_location>{event_location_hint or '(ë¯¸ìƒ)'}</event_location>
      <event_contact>{event_contact_hint or '(ë¯¸ìƒ)'}</event_contact>
      <event_cta>{event_cta_hint or '(ì—†ìŒ)'}</event_cta>
    </facts>
    <rule id="event_info_first">ë„ì…ë¶€ 2ë¬¸ë‹¨ ì´ë‚´ì— í–‰ì‚¬ ì¼ì‹œ/ì¥ì†Œ/ì°¸ì—¬ ë°©ë²•ì„ ëª¨ë‘ ì œì‹œí•  ê²ƒ.</rule>
    <rule id="speaker_name_required">ì²« ë¬¸ë‹¨ ì²« 2ë¬¸ì¥ ì•ˆì— í™”ì ì‹¤ëª…ì„ ë°˜ë“œì‹œ í¬í•¨í•  ê²ƒ.</rule>
    <rule id="bio_limit_before_event">í–‰ì‚¬ í•µì‹¬ì •ë³´ ì œì‹œ ì „, í™”ì ê²½ë ¥/ì„œì‚¬ ì„œìˆ ì€ ìµœëŒ€ 2ë¬¸ì¥ìœ¼ë¡œ ì œí•œí•  ê²ƒ.</rule>
    <rule id="no_invite_redundancy">"ì§ì ‘ ë§Œë‚˜", "ì§„ì†”í•œ ì†Œí†µ" ë¥˜ ë¬¸êµ¬ ë°˜ë³µ ê¸ˆì§€. ì›ê³  ì „ì²´ ìµœëŒ€ 2íšŒ.</rule>
    <rule id="event_fact_repeat_limit">í–‰ì‚¬ ì¼ì‹œ/ì¥ì†Œ/ì°¸ì—¬ ì•ˆë‚´ ë¬¸êµ¬ëŠ” ë„ì… 1íšŒ + ê²°ë¡  1íšŒê¹Œì§€ë§Œ í—ˆìš©í•  ê²ƒ.</rule>
    <rule id="event_fact_variation">ë™ì¼í•œ ì¼ì‹œ+ì¥ì†Œ ê²°í•© êµ¬ë¬¸ì„ ë³¸ë¬¸ ì„¹ì…˜ë§ˆë‹¤ ë°˜ë³µí•˜ì§€ ë§ ê²ƒ. ì¤‘ê°„ ì„¹ì…˜ì—ì„œëŠ” "ì´ë²ˆ í–‰ì‚¬ í˜„ì¥", "í–‰ì‚¬ ìë¦¬"ì²˜ëŸ¼ ë³€í˜•í•´ ì—°ê²°í•  ê²ƒ.</rule>
    <rule id="event_datetime_ngram_cap">"3ì›” 1ì¼(ì¼) ì˜¤í›„ 2ì‹œ, ì„œë©´..."ì²˜ëŸ¼ ì¼ì‹œ+ì¥ì†Œ ê²°í•© 5ë‹¨ì–´ ì´ìƒ êµ¬ë¬¸ì€ ì›ê³  ì „ì²´ ìµœëŒ€ 2íšŒ. 3íšŒì§¸ë¶€í„°ëŠ” "í–‰ì‚¬ ë‹¹ì¼", "ë‹¹ì¼ í˜„ì¥" ë“± ë³€í˜• í‘œí˜„ìœ¼ë¡œë§Œ ì‘ì„±í•  ê²ƒ.</rule>
    <rule id="event_seed_priority">ì„œë¡  1~2ë¬¸ë‹¨ì—ì„œ ì…ì¥ë¬¸ í•µì‹¬ ì‹œë“œ(stance_seed)ì˜ ì˜ë¯¸ë¥¼ ë°˜ë“œì‹œ ì¬ì§„ìˆ í•  ê²ƒ.</rule>
    <rule id="no_orphan_location_line">ì¥ì†Œ í‚¤ì›Œë“œ("ì„œë©´ ì˜ê´‘ë„ì„œ/ë¶€ì‚° ì˜ê´‘ë„ì„œ")ëŠ” ë‹¨ìˆœ ì•ˆë‚´ ë‹¨ë¬¸ìœ¼ë¡œ ë¶„ë¦¬í•˜ì§€ ë§ê³ , í•´ë‹¹ ë‹¨ë½ì˜ í–‰ì‚¬ ë§¥ë½(ì°¸ì—¬ ì •ë³´/ëŒ€í™” ì£¼ì œ/ë…ì íš¨ìµ)ê³¼ ê²°í•©í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ.</rule>
    <rule id="no_recap_echo">ê° ì„¹ì…˜ ëì˜ ìš”ì•½ ë‹¨ë¬¸ ë°˜ë³µ ê¸ˆì§€. íŠ¹íˆ "ì´ ë§Œë‚¨ì€ ~", "ì´ ìë¦¬ëŠ” ~", "ì´ ëœ»ê¹Šì€ ìë¦¬ëŠ” ~", "ì´ë²ˆ ë§Œë‚¨ì€ ~" íŒ¨í„´ì€ ì›ê³  ì „ì²´ 1íšŒë§Œ í—ˆìš©.</rule>
    <rule id="cta_once">ê²°ë¡ ë¶€ CTAëŠ” 1íšŒë§Œ ì‘ì„±í•˜ê³ , í–‰ë™ ë™ì‚¬+êµ¬ì²´ ì¥ì†Œë¥¼ í•¨ê»˜ ì œì‹œí•  ê²ƒ. ì˜ˆ: "ì£¼ì € ë§ê³  ì„œë©´ ì˜ê´‘ë„ì„œë¥¼ ì°¾ì•„ ì£¼ì‹­ì‹œì˜¤."</rule>
    <rule id="audience_intent">í–‰ì‚¬ ì•ˆë‚´ë¬¸ ë…ìê°€ ì¦‰ì‹œ í–‰ë™í•  ìˆ˜ ìˆë„ë¡ ì •ë³´ ìš°ì„ , ìê¸°ì„œì‚¬ ê³¼ì‰ ê¸ˆì§€.</rule>
    <rule id="event_intro_with_stance">í–‰ì‚¬ ì •ë³´ ì œì‹œ í›„, ì…ì¥ë¬¸ í•µì‹¬ ë©”ì‹œì§€ë¥¼ ì„œë¡ ì—ì„œ ë°”ë¡œ ì—°ê²°í•  ê²ƒ.</rule>
  </event_mode>
"""
        
        # ë™ì  ë³¸ë¡  êµ¬ì¡° ë¬¸ìì—´ ìƒì„±
        body_structure_lines = []
        for i in range(1, body_section_count + 1):
            body_structure_lines.append(
                f"<body_section order=\"{i+1}\" name=\"ë³¸ë¡  {i}\" paragraphs=\"2~3\" chars=\"{per_section_min}~{per_section_max}\" heading=\"h2 í•„ìˆ˜\"/>"
            )
        body_structure_str = "\n    ".join(body_structure_lines)
        
        # ì§€ì—­ ì •ë³´ ì¶”ì¶œ - ë²”ìš©ì„± í™•ë³´ ë° ë™ì  ë³€ìˆ˜
        region_metro = user_profile.get('regionMetro', '')
        region_district = user_profile.get('regionDistrict', '')
        user_region = f"{region_metro} {region_district}".strip()
        if not user_region:
            user_region = "ì§€ì—­ ì‚¬íšŒ"
            
        structure_enforcement = f"""
<structure_guide mode="strict">
  <strategy>E-A-T (ì „ë¬¸ì„±-ê¶Œìœ„-ì‹ ë¢°) ì „ëµìœ¼ë¡œ ì‘ì„±</strategy>

  <volume warning="ìœ„ë°˜ ì‹œ ì‹œìŠ¤í…œ ì˜¤ë¥˜">
    <per_section min="{per_section_min}" max="{per_section_max}" recommended="{per_section_recommended}"/>
    <paragraphs_per_section>2~3ê°œ ë¬¸ë‹¨, ë¬¸ë‹¨ë‹¹ 2~4ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ìœ„ì£¼ ì„œìˆ </paragraphs_per_section>
    <total sections="{total_section_count}" min="{min_total_chars}" max="{max_total_chars}"/>
    <caution>ì´ ë¶„ëŸ‰ ìƒí•œì„ ë„˜ê¸°ì§€ ì•Šë„ë¡ ì¤‘ë³µ ë¬¸ì¥ê³¼ ì¥í™©í•œ ìˆ˜ì‹ì–´ë¥¼ ì œê±°í•˜ê³ , ê·¼ê±° ì¤‘ì‹¬ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.</caution>
  </volume>

  <expansion_guide name="ì„¹ì…˜ë³„ ì‘ì„± 4ë‹¨ê³„">
    ê° ë³¸ë¡  ì„¹ì…˜ì„ ì•„ë˜ íë¦„ìœ¼ë¡œ ë°€ë„ ìˆê²Œ ì „ê°œí•˜ì‹­ì‹œì˜¤.
    <step name="Why" sentences="1~2">ì‹œë¯¼ë“¤ì´ ê²ªëŠ” ì‹¤ì œ ë¶ˆí¸í•¨ê³¼ í˜„ì¥ì˜ ê³ ì¶©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì§„ë‹¨</step>
    <step name="How+Expertise" sentences="2">ì‹¤í˜„ ê°€ëŠ¥í•œ í•´ê²°ì±… ì œì‹œ ë° ë³¸ì¸ì˜ Bio(ê²½ë ¥)ë¥¼ ì¸ìš©í•˜ì—¬ ì „ë¬¸ì„± ê°•ì¡°</step>
    <step name="Authority" sentences="1">ê³¼ê±° ì„±ê³¼ë‚˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤í–‰ ëŠ¥ë ¥ì„ ì¦ëª…</step>
    <step name="Effect+Trust" sentences="1~2">ë³€í™”ë  {user_region}ì˜ ë¯¸ë˜ ì²­ì‚¬ì§„ì„ ëª…í™•íˆ ì œì‹œ</step>
  </expansion_guide>

  <sections total="{total_section_count}">
    <intro paragraphs="2~3" chars="{per_section_recommended}" heading="ì—†ìŒ">
      {intro_line_1}
      {intro_line_2}
      {intro_line_3}
    </intro>
    {body_structure_str}
    <conclusion order="{total_section_count}" paragraphs="2~3" chars="{per_section_recommended}" heading="h2 í•„ìˆ˜"/>
  </sections>

  <h2_strategy name="ì†Œì œëª© ì‘ì„± ì „ëµ (AEO+SEO)">
    <type name="ì§ˆë¬¸í˜•" strength="AEO ìµœê°•">ì²­ë…„ ê¸°ë³¸ì†Œë“, ì‹ ì²­ ë°©ë²•ì€?</type>
    <type name="ëª…ì‚¬í˜•" strength="SEO ê¸°ë³¸">ë¶„ë‹¹êµ¬ ì •ìë™ ì£¼ì°¨ì¥ ì‹ ì„¤ ìœ„ì¹˜</type>
    <type name="ë°ì´í„°" strength="ì‹ ë¢°ì„±">2025ë…„ ìƒë°˜ê¸° 5ëŒ€ ì£¼ìš” ì„±ê³¼</type>
    <type name="ì ˆì°¨" strength="ì‹¤ìš©ì„±">ì²­ë…„ ê¸°ë³¸ì†Œë“ ì‹ ì²­ 3ë‹¨ê³„ ì ˆì°¨</type>
    <type name="ë¹„êµ" strength="ì°¨ë³„í™”">ê¸°ì¡´ ì •ì±… ëŒ€ë¹„ ê°œì„ ëœ 3ê°€ì§€</type>
    <banned>ì¶”ìƒì  í‘œí˜„("ë…¸ë ¥", "ì—´ì „", "ë§ˆìŒ"), ëª¨í˜¸í•œ ì œëª©("ì •ì±… ì•ˆë‚´", "ì†Œê°œ"), ì„œìˆ ì–´ í¬í•¨("~ì— ëŒ€í•œ ì„¤ëª…")</banned>
  </h2_strategy>

  <mandatory_rules>
    <rule id="html_tags">ì†Œì œëª©ì€ &lt;h2&gt;, ë¬¸ë‹¨ì€ &lt;p&gt; íƒœê·¸ë§Œ ì‚¬ìš© (ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ê¸ˆì§€)</rule>
    <rule id="defer_output_addons" severity="critical">ìŠ¬ë¡œê±´/í›„ì› ì•ˆë‚´(ê³„ì¢ŒÂ·ì˜ˆê¸ˆì£¼Â·ì—°ë½ì²˜Â·ì˜ìˆ˜ì¦ ì•ˆë‚´)ëŠ” ë³¸ë¬¸ì— ì“°ì§€ ë§ ê²ƒ. í•´ë‹¹ ì •ë³´ëŠ” ìµœì¢… ì¶œë ¥ ì§ì „ì— ì‹œìŠ¤í…œì´ ìë™ ë¶€ì°©.</rule>
    <rule id="no_slogan_repeat" severity="critical">ì…ì¥ë¬¸ì˜ ë§ºìŒë§/ìŠ¬ë¡œê±´ì„ ê° ì„¹ì…˜ ëë§ˆë‹¤ ë°˜ë³µ ê¸ˆì§€. ëª¨ë“  í˜¸ì†Œì™€ ë‹¤ì§ì€ ë§¨ ë§ˆì§€ë§‰ ê²°ë¡ ë¶€ì—ë§Œ.</rule>
    <rule id="sentence_completion">ë¬¸ì¥ì€ ì˜¬ë°”ë¥¸ ì¢…ê²° ì–´ë¯¸(~ì…ë‹ˆë‹¤, ~í•©ë‹ˆë‹¤, ~ì‹œì˜¤)ë¡œ ëë‚´ì•¼ í•¨. ê³ ì˜ì  ì˜¤íƒ€/ì˜ë¦° ë¬¸ì¥ ê¸ˆì§€.</rule>
    <rule id="keyword_per_section">ê° ì„¹ì…˜ë§ˆë‹¤ í‚¤ì›Œë“œ 1ê°œ ì´ìƒ í¬í•¨</rule>
    <rule id="separate_pledges">ê° ë³¸ë¡  ì„¹ì…˜ì€ ì„œë¡œ ë‹¤ë¥¸ ì£¼ì œ/ê³µì•½ì„ ë‹¤ë£° ê²ƒ</rule>
    <rule id="verb_diversity" severity="critical">ê°™ì€ ë™ì‚¬(ì˜ˆ: "ë˜ì§€ë©´ì„œ")ë¥¼ ì›ê³  ì „ì²´ì—ì„œ 3íšŒ ì´ìƒ ì‚¬ìš© ê¸ˆì§€. ë™ì˜ì–´ êµì²´: ì œì‹œí•˜ë©°, ì•½ì†í•˜ë©°, ì—´ë©°, ë³´ì—¬ë“œë¦¬ë©° ë“±.</rule>
    <rule id="slogan_once">ìºì¹˜í”„ë ˆì´ì¦ˆ("ì²­ë…„ì´ ëŒì•„ì˜¤ëŠ” ë¶€ì‚°")ë‚˜ ë¹„ìœ ("ì•„ì‹œì•„ì˜ ì‹±ê°€í¬ë¥´")ëŠ” ê²°ë¡ ë¶€ 1íšŒë§Œ. ë‹¤ë¥¸ ì„¹ì…˜ì—ì„œëŠ” ë³€í˜• ì‚¬ìš©.</rule>
    <rule id="natural_keyword">í‚¤ì›Œë“œëŠ” ì •ë³´ ë¬¸ì¥ì´ ì•„ë‹ˆë¼ ë§¥ë½ ë¬¸ì¥ìœ¼ë¡œ ì‚½ì…. í‚¤ì›Œë“œ ë¬¸ì¥ì—ëŠ” ìµœì†Œ 1ê°œ ì´ìƒ í¬í•¨: í–‰ì‚¬ ì •ë³´(ì¼ì‹œ/ì¥ì†Œ/ì°¸ì—¬ ë°©ë²•), ëŒ€í™” ì£¼ì œ, ì‹œë¯¼ í–‰ë™ ì œì•ˆ. í•´ë‹¹ ë¬¸ë‹¨ì˜ ì£¼ì¥/ê·¼ê±°ì™€ ê²°í•©í•´ ì“°ê³ , í‚¤ì›Œë“œë§Œìœ¼ë¡œ ëœ ì¥ì‹/ë‹¨ë… ë¬¸ì¥ ê¸ˆì§€.</rule>
    <rule id="no_single_sentence_echo">ê°™ì€ êµ¬ì¡°ì˜ ë‹¨ë¬¸ ë¬¸ì¥ì„ ì„¹ì…˜ ë§ë¯¸ë§ˆë‹¤ ë°˜ë³µ ê¸ˆì§€. íŠ¹íˆ "ì´ ë§Œë‚¨ì€ ~", "ì´ ìë¦¬ëŠ” ~", "ì´ ëœ»ê¹Šì€ ìë¦¬ëŠ” ~", "ì´ë²ˆ ë§Œë‚¨ì€ ~" íŒ¨í„´ì€ í•œ ë²ˆë§Œ ì‚¬ìš©.</rule>
    <rule id="no_datetime_location_ngram_repeat">ì¼ì‹œ+ì¥ì†Œê°€ í•¨ê»˜ ë“¤ì–´ê°„ êµ¬ë¬¸(ì˜ˆ: "3ì›” 1ì¼(ì¼) ì˜¤í›„ 2ì‹œ, ì„œë©´...")ì€ ê°™ì€ ì–´ìˆœìœ¼ë¡œ 3íšŒ ì´ìƒ ë°˜ë³µ ê¸ˆì§€. 2íšŒë¥¼ ë„˜ìœ¼ë©´ ì–´ìˆœ/í‘œí˜„ì„ ë°˜ë“œì‹œ ë³€í˜•í•  ê²ƒ.</rule>
    <rule id="no_meta_prompt_leak">í”„ë¡¬í”„íŠ¸/ê·œì¹™ ì„¤ëª… ë¬¸ì¥ì„ ë³¸ë¬¸ì— ë³µì‚¬í•˜ì§€ ë§ ê²ƒ. "ë¬¸ì œëŠ”~ì ê²€" ê°™ì€ ê·œì¹™ì„± ë©”íƒ€ ë¬¸ì¥ ìƒì„± ê¸ˆì§€.</rule>
    <rule id="paragraph_min_sentences">ì›ì¹™ì ìœ¼ë¡œ ê° <p>ëŠ” ìµœì†Œ 2ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±. ì˜ˆì™¸ëŠ” ê²°ë¡ ì˜ ë§ˆì§€ë§‰ CTA ë¬¸ë‹¨ 1ê°œë§Œ í—ˆìš©.</rule>
    <rule id="causal_clarity">ì„±ê³¼ ì–¸ê¸‰ ì‹œ ë³¸ì¸ì˜ êµ¬ì²´ì  ì—­í• /ì§ì±… ëª…ì‹œ. "40% ë“í‘œìœ¨ì„ ì´ëŒì–´ëƒˆë‹¤" â†’ "ì‹œë‹¹ìœ„ì›ì¥ìœ¼ë¡œì„œ ì§€ì—­ ì¡°ì§ì„ ì´ê´„í•˜ë©° 40% ë“í‘œìœ¨ ë‹¬ì„±ì— ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤"</rule>
  </mandatory_rules>
{material_uniqueness_guard}
{event_mode_rules}
{intro_stance_rules}

  <constraints warning="ìœ„ë°˜ ì‹œ ìë™ ë°˜ë ¤">
    <max_chars>{max_total_chars}</max_chars>
    <min_chars>{min_total_chars}</min_chars>
    <no_repeat>ê°™ì€ ë¬¸ì¥, ê°™ì€ í‘œí˜„ ë°˜ë³µ ê¸ˆì§€ (íŠ¹íˆ "~ë°”ëë‹ˆë‹¤" ë°˜ë³µ ê¸ˆì§€)</no_repeat>
    <html>ë¬¸ë‹¨ì€ &lt;p&gt;...&lt;/p&gt;, ì†Œì œëª©ì€ &lt;h2&gt;...&lt;/h2&gt;ë§Œ ì‚¬ìš©</html>
    <separate_pledges>ì„œë¡œ ë‹¤ë¥¸ ê³µì•½/ì •ì±…ì€ í•˜ë‚˜ì˜ ë³¸ë¡ ì— í•©ì¹˜ì§€ ë§ ê²ƒ</separate_pledges>
  </constraints>

  <output_format>í…œí”Œë¦¿ì—ì„œ ì§€ì‹œí•œ XML íƒœê·¸(title, content, hashtags)ë§Œ ì¶œë ¥. output ë˜í¼ë‚˜ ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ê¸ˆì§€.</output_format>
</structure_guide>
"""

        # SEO ì§€ì¹¨ ìƒì„±
        seo_instruction = build_seo_instruction({
            'keywords': params.get('userKeywords', []),
            'targetWordCount': params.get('targetWordCount', 2000)
        })

        # ì„ ê±°ë²• ì¤€ìˆ˜ ì§€ì¹¨ ìƒì„±
        user_status = user_profile.get('status', 'ì¤€ë¹„')
        election_instruction = get_prompt_instruction(user_status)

        party_stance_guide = params.get('partyStanceGuide') or ''
        context_injection_xml = ""
        if context_injection.strip():
            context_injection_xml = f"<context_injection>\n{context_injection.strip()}\n</context_injection>"

        return f"""
<structure_agent_prompt version="xml-v1">
  <template_prompt>{_xml_cdata(template_prompt)}</template_prompt>
  <party_stance_guide>{_xml_cdata(party_stance_guide)}</party_stance_guide>
  <seo_instruction>{_xml_cdata(seo_instruction)}</seo_instruction>
  <election_instruction>{_xml_cdata(election_instruction)}</election_instruction>
  {ref_section}
  {context_injection_xml}
  {bio_warning}
  {structure_enforcement}
</structure_agent_prompt>
""".strip()

    def build_author_bio(self, user_profile: Dict) -> tuple[str, str]:
        # ë°©ì–´ ì½”ë“œ - listë¡œ ì „ë‹¬ë˜ëŠ” ê²½ìš° ë°©ì–´
        if not isinstance(user_profile, dict):
            user_profile = {}

        name = user_profile.get('name', 'ì‚¬ìš©ì')
        party_name = user_profile.get('partyName', '')
        current_title = user_profile.get('customTitle') or user_profile.get('position', '')
        basic_bio = " ".join(filter(None, [party_name, current_title, name]))

        career = user_profile.get('careerSummary') or user_profile.get('bio', '')

        # ìŠ¬ë¡œê±´/í›„ì› ì•ˆë‚´ëŠ” ìƒì„± ë‹¨ê³„ì—ì„œ ì œì™¸í•˜ê³ , ìµœì¢… ì¶œë ¥ ì§ì „ì—ë§Œ ë¶€ì°©í•œë‹¤.
        return f"{basic_bio}\n{career}".strip(), name

    def is_current_lawmaker(self, user_profile: Dict) -> bool:
        # ë°©ì–´ ì½”ë“œ - listë¡œ ì „ë‹¬ë˜ê±°ë‚˜ Noneì¸ ê²½ìš° ë°©ì–´
        if not user_profile or not isinstance(user_profile, dict):
            return False
        status = user_profile.get('status', '')
        position = user_profile.get('position', '')
        title = user_profile.get('customTitle', '')

        elected_keywords = ['ì˜ì›', 'êµ¬ì²­ì¥', 'êµ°ìˆ˜', 'ì‹œì¥', 'ë„ì§€ì‚¬', 'êµìœ¡ê°']
        text_to_check = status + position + title
        return any(k in text_to_check for k in elected_keywords)

    def parse_response(self, response: str) -> Dict[str, str]:
        if not response:
            return {'content': '', 'title': ''}
        
        # XML Tag Extraction
        try:
            def extract_html_fallback(text: str) -> str:
                html_blocks = re.findall(
                    r'<(?:p|h[23])\b[^>]*>[\s\S]*?</(?:p|h[23])>',
                    text or '',
                    re.IGNORECASE,
                )
                return "\n".join(html_blocks).strip() if html_blocks else ''

            # Title extraction (ì—¬ëŸ¬ ë¸”ë¡ì´ ìˆì„ ë•ŒëŠ” ë§ˆì§€ë§‰ ìœ íš¨ ë¸”ë¡ ìš°ì„ )
            title_blocks = [
                (m.group(1) or "").strip()
                for m in re.finditer(r'<title>(.*?)</title>', response, re.DOTALL | re.IGNORECASE)
            ]
            title = next((t for t in reversed(title_blocks) if t), '')

            # Content extraction
            content_blocks = [
                (m.group(1) or "").strip()
                for m in re.finditer(r'<content>(.*?)</content>', response, re.DOTALL | re.IGNORECASE)
            ]
            content = ''
            if content_blocks:
                # 1) ì˜ˆì‹œ/í…œí”Œë¦¿ ë¸”ë¡ ì œê±° 2) êµ¬ì¡° ë°€ë„ + ê¸¸ì´ë¡œ ë³¸ë¬¸ ë¸”ë¡ ì„ íƒ
                candidates = []
                for idx, block in enumerate(content_blocks):
                    normalized = normalize_html_structure_tags(normalize_artifacts(block))
                    tag_density = len(re.findall(r'<(?:h2|p)\b', normalized, re.IGNORECASE))
                    plain_len = len(strip_html(normalized))
                    candidates.append(
                        {
                            'index': idx,
                            'content': normalized,
                            'tag_density': tag_density,
                            'plain_len': plain_len,
                            'is_example': is_example_like_block(normalized),
                        }
                    )

                real_candidates = [c for c in candidates if not c['is_example']]
                pool = real_candidates if real_candidates else candidates

                # Prefer substantial body text first, then structure density.
                # This prevents short outline/sample blocks from being selected
                # over the actual long-form draft.
                max_plain_len = max((c['plain_len'] for c in pool), default=0)
                min_plain_threshold = max(300, int(max_plain_len * 0.45))
                length_filtered_pool = [
                    c for c in pool
                    if c['plain_len'] >= min_plain_threshold
                ]
                selection_pool = length_filtered_pool if length_filtered_pool else pool
                selected = max(
                    selection_pool,
                    key=lambda c: (c['plain_len'], c['tag_density'], c['index']),
                )
                content = selected['content'].strip()
                selected_plain_len = selected['plain_len']

                # ì„ íƒëœ content ì¸ë±ìŠ¤ì™€ ê°™ì€ titleì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                selected_idx = selected['index']
                if selected_idx < len(title_blocks):
                    aligned_title = (title_blocks[selected_idx] or '').strip()
                    if aligned_title:
                        title = aligned_title

                # ì§§ì€ XML content ë¸”ë¡ì´ ì¡í˜”ëŠ”ë° ë³¸ë¬¸ HTMLì´ ì‘ë‹µì— ë”°ë¡œ ì¡´ì¬í•˜ë©´ í´ë°±ìœ¼ë¡œ êµì²´
                fallback_content = extract_html_fallback(response)
                fallback_plain_len = len(strip_html(fallback_content))
                fallback_is_example = is_example_like_block(fallback_content)
                if (
                    not fallback_is_example
                    and
                    selected_plain_len < 180
                    and fallback_plain_len >= max(260, selected_plain_len + 120)
                ):
                    print(
                        f"âš ï¸ [StructureAgent] ì§§ì€ XML content({selected_plain_len}ì) ê°ì§€, "
                        f"HTML í´ë°±({fallback_plain_len}ì)ë¡œ êµì²´"
                    )
                    content = fallback_content
            
            if not content:
                # Fallback: try to find just HTML tags if XML tags are missing
                print('âš ï¸ [StructureAgent] XML íƒœê·¸ ëˆ„ë½, HTML ì§ì ‘ ì¶”ì¶œ ì‹œë„')
                fallback_content = extract_html_fallback(response)
                if fallback_content:
                    content = fallback_content
                else:
                    content = response # ìµœí›„ë‹¨: ì „ì²´ í…ìŠ¤íŠ¸
            
            print(f"âœ… [StructureAgent] íŒŒì‹± ì„±ê³µ: content={len(content)}ì")
            return {'content': content, 'title': title}
            
        except Exception as e:
            print(f"âš ï¸ [StructureAgent] íŒŒì‹± ì—ëŸ¬: {str(e)}")
            return {'content': response, 'title': ''}

    def _split_plain_sentences(self, text: str) -> List[str]:
        if not text:
            return []
        chunks = re.findall(r'[^.!?ã€‚]+[.!?ã€‚]?', text)
        sentences = []
        for chunk in chunks:
            sentence = re.sub(r'\s+', ' ', chunk).strip()
            if sentence:
                sentences.append(sentence)
        return sentences

    def _find_meta_prompt_leak_sentences(self, content: str) -> List[str]:
        plain_text = re.sub(r'<[^>]*>', ' ', content or '')
        plain_text = re.sub(r'\s+', ' ', plain_text).strip()
        if not plain_text:
            return []

        patterns = [
            re.compile(r'ë¬¸ì œëŠ” .*í˜„ì¥ .*ë°ì´í„°.*ì ê²€í•´ì•¼', re.IGNORECASE),
            re.compile(r'ê´€ë ¨ .*ìŸì .*í•¨ê»˜ .*ë´ì•¼', re.IGNORECASE),
            re.compile(r'ê·¸ë˜ë„ .*ë¬¸ì œëŠ” .*ì ê²€í•´ì•¼', re.IGNORECASE),
            re.compile(r'í˜„ì¥ .*ë°ì´í„°.*í•¨ê»˜ .*ì ê²€', re.IGNORECASE),
        ]

        leaks: List[str] = []
        for sentence in self._split_plain_sentences(plain_text):
            if any(pattern.search(sentence) for pattern in patterns):
                leaks.append(sentence)
        return leaks

    def _count_event_fact_sentence_mentions(
        self,
        content: str,
        *,
        event_date_hint: str = '',
        event_location_hint: str = '',
    ) -> int:
        plain_text = re.sub(r'<[^>]*>', ' ', content or '')
        plain_text = re.sub(r'\s+', ' ', plain_text).strip()
        if not plain_text:
            return 0

        location_tokens: List[str] = []
        for token in ('ì„œë©´ ì˜ê´‘ë„ì„œ', 'ë¶€ì‚° ì˜ê´‘ë„ì„œ'):
            if token in plain_text:
                location_tokens.append(token)
        if event_location_hint:
            normalized_location_hint = re.sub(r'\s+', ' ', event_location_hint).strip()
            if normalized_location_hint and normalized_location_hint not in location_tokens:
                location_tokens.append(normalized_location_hint)

        date_tokens: List[str] = []
        if event_date_hint:
            for pattern in (
                r'\d{1,2}\s*ì›”\s*\d{1,2}\s*ì¼(?:\s*\([^)]+\))?',
                r'(?:ì˜¤ì „|ì˜¤í›„)\s*\d{1,2}\s*ì‹œ(?:\s*\d{1,2}\s*ë¶„)?',
            ):
                match = re.search(pattern, event_date_hint)
                if match:
                    date_tokens.append(re.sub(r'\s+', ' ', match.group(0)).strip())

        if not date_tokens:
            date_tokens = [r'\d{1,2}\s*ì›”\s*\d{1,2}\s*ì¼', r'(?:ì˜¤ì „|ì˜¤í›„)\s*\d{1,2}\s*ì‹œ']

        count = 0
        for sentence in self._split_plain_sentences(plain_text):
            has_location = any(token and token in sentence for token in location_tokens)
            has_date = any(re.search(token, sentence) for token in date_tokens)
            if has_location and has_date:
                count += 1
        return count

    def _find_overused_anchor_phrases(self, content: str) -> List[str]:
        plain_text = re.sub(r'<[^>]*>', ' ', content or '')
        plain_text = re.sub(r'\s+', ' ', plain_text).strip()
        if not plain_text:
            return []

        phrase_limits = {
            'ë¶€ì‚°í•­ ë¶€ë‘ ë…¸ë™ìì˜ ë§‰ë‚´ë¡œ': 2,
            'ì‹œë¯¼ ì—¬ëŸ¬ë¶„ê³¼ í•¨ê»˜': 2,
        }
        overused: List[str] = []
        for phrase, limit in phrase_limits.items():
            count = len(re.findall(re.escape(phrase), plain_text))
            if count > limit:
                overused.append(f'"{phrase}" {count}íšŒ')
        return overused

    def _repair_anchor_phrase_overuse(self, content: str) -> Tuple[str, List[Dict[str, Any]]]:
        working = str(content or '')
        if not working:
            return working, []

        rules: List[Dict[str, Any]] = [
            {
                'phrase': 'ì‹œë¯¼ ì—¬ëŸ¬ë¶„ê³¼ í•¨ê»˜',
                'limit': 2,
                'replacements': ['ì‹œë¯¼ê³¼ í•¨ê»˜', 'ì—¬ëŸ¬ë¶„ê³¼ í•¨ê»˜', 'ì§€ì—­ì‚¬íšŒì™€ í•¨ê»˜'],
            },
            {
                'phrase': 'ë¶€ì‚°í•­ ë¶€ë‘ ë…¸ë™ìì˜ ë§‰ë‚´ë¡œ',
                'limit': 2,
                'replacements': ['ë¶€ì‚°í•­ ë…¸ë™ì ê°€ì •ì˜ ë§‰ë‚´ë¡œ', 'ë¶€ì‚°í•­ í˜„ì¥ ë…¸ë™ì ì§‘ì•ˆì˜ ë§‰ë‚´ë¡œ'],
            },
        ]

        actions: List[Dict[str, Any]] = []
        for rule in rules:
            phrase = str(rule.get('phrase') or '').strip()
            limit = int(rule.get('limit') or 2)
            replacements = [str(item).strip() for item in (rule.get('replacements') or []) if str(item).strip()]
            if not phrase or not replacements:
                continue

            pattern = re.compile(re.escape(phrase))
            matches = list(pattern.finditer(working))
            if len(matches) <= limit:
                continue

            replaced = 0
            to_replace = list(reversed(matches[limit:]))
            for idx, match in enumerate(to_replace):
                replacement = replacements[idx % len(replacements)]
                start, end = match.span()
                working = working[:start] + replacement + working[end:]
                replaced += 1

            if replaced > 0:
                actions.append(
                    {
                        'type': 'anchor_phrase_overuse_repair',
                        'phrase': phrase,
                        'replaced': replaced,
                        'limit': limit,
                    }
                )

        return working, actions

    def validate_output(
        self,
        content: str,
        target_word_count: Any,
        stance_count: int = 0,
        *,
        context_analysis: Optional[Dict[str, Any]] = None,
        is_event_announcement: bool = False,
        event_date_hint: str = '',
        event_location_hint: str = '',
    ) -> Dict:
        if not content:
            return {
                'passed': False,
                'code': 'EMPTY_CONTENT',
                'reason': 'ë‚´ìš© ì—†ìŒ',
                'feedback': 'ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.'
            }
        
        plain_length = len(strip_html(content))

        if isinstance(target_word_count, dict):
            length_spec = target_word_count
        else:
            length_spec = self._build_length_spec(target_word_count, stance_count)

        min_length = length_spec['min_chars']
        max_length = length_spec['max_chars']
        expected_h2 = length_spec['expected_h2']
        per_section_recommended = length_spec['per_section_recommended']
        per_section_max = length_spec['per_section_max']
        total_sections = length_spec['total_sections']

        placeholder_count = len(re.findall(r'\[[^\]\n]{1,40}\]', content))
        if placeholder_count >= 2:
            return {
                'passed': False,
                'code': 'TEMPLATE_ECHO',
                'reason': f"ì˜ˆì‹œ/í”Œë ˆì´ìŠ¤í™€ë” ì”ì¡´ ({placeholder_count}ê°œ)",
                'feedback': 'ì˜ˆì‹œ ë¬¸êµ¬([ì œëª©], [êµ¬ì²´ì  ë‚´ìš©] ë“±)ë¥¼ ëª¨ë‘ ì œê±°í•˜ê³  ì‹¤ì œ ë³¸ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.'
            }

        if plain_length < min_length:
            deficit = min_length - plain_length
            return {
                'passed': False,
                'code': 'LENGTH_SHORT',
                'reason': f"ë¶„ëŸ‰ ë¶€ì¡± ({plain_length}ì < {min_length}ì)",
                'feedback': (
                    f"í˜„ì¬ ë¶„ëŸ‰({plain_length}ì)ì´ ìµœì†Œ ê¸°ì¤€({min_length}ì)ë³´ë‹¤ {deficit}ì ë¶€ì¡±í•©ë‹ˆë‹¤. "
                    f"ì„¹ì…˜ë‹¹ {per_section_recommended}ì ì•ˆíŒìœ¼ë¡œ êµ¬ì²´ ì‚¬ë¡€ë¥¼ ë³´ê°•í•˜ë˜, ì´ ë¶„ëŸ‰ì€ {max_length}ìë¥¼ ë„˜ê¸°ì§€ ë§ˆì‹­ì‹œì˜¤."
                )
            }
        
        if plain_length > max_length:
            excess = plain_length - max_length
            return {
                'passed': False,
                'code': 'LENGTH_LONG',
                'reason': f"ë¶„ëŸ‰ ì´ˆê³¼ ({plain_length}ì > {max_length}ì)",
                'feedback': (
                    f"í˜„ì¬ ë¶„ëŸ‰({plain_length}ì)ì´ ìµœëŒ€ ê¸°ì¤€({max_length}ì)ì„ {excess}ì ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. "
                    f"ê° ì„¹ì…˜ì„ {per_section_recommended}ì ì•ˆíŒ(ìµœëŒ€ {per_section_max}ì)ìœ¼ë¡œ ì••ì¶•í•˜ê³ , "
                    f"ì¤‘ë³µ ë¬¸ì¥ê³¼ ì¥í™©í•œ ìˆ˜ì‹ì–´ë¥¼ ì œê±°í•˜ì‹­ì‹œì˜¤."
                )
            }

        # í—ˆìš© íƒœê·¸ëŠ” h2/pë§Œ ì‚¬ìš©í•´ì•¼ í•˜ë¯€ë¡œ, ê¸°íƒ€ heading íƒœê·¸ëŠ” ì¦‰ì‹œ ë°˜ë ¤.
        disallowed_heading_count = len(re.findall(r'<h(?!2\b)[1-6]\b[^>]*>', content, re.IGNORECASE))
        if disallowed_heading_count > 0:
            return {
                'passed': False,
                'code': 'TAG_DISALLOWED',
                'reason': f"í—ˆìš©ë˜ì§€ ì•Šì€ heading íƒœê·¸ ì‚¬ìš© (h2 ì™¸ {disallowed_heading_count}ê°œ)",
                'feedback': 'ì†Œì œëª©ì€ <h2>ë§Œ ì‚¬ìš©í•˜ê³ , <h1>/<h3> ë“± ë‹¤ë¥¸ heading íƒœê·¸ëŠ” ì œê±°í•˜ì‹­ì‹œì˜¤.'
            }

        h2_open_tags = re.findall(r'<h2\b[^>]*>', content, re.IGNORECASE)
        h2_close_tags = re.findall(r'</h2\s*>', content, re.IGNORECASE)
        h2_count = len(h2_open_tags)
        if h2_count != len(h2_close_tags):
            return {
                'passed': False,
                'code': 'H2_MALFORMED',
                'reason': f"h2 íƒœê·¸ ì§ ë¶ˆì¼ì¹˜ (ì—´ë¦¼ {h2_count}ê°œ, ë‹«í˜ {len(h2_close_tags)}ê°œ)",
                'feedback': 'ëª¨ë“  ì†Œì œëª©ì€ <h2>...</h2> í˜•íƒœë¡œ ì •í™•íˆ ë‹«ì•„ ì£¼ì‹­ì‹œì˜¤.'
            }

        if h2_count < expected_h2:
            return {
                'passed': False,
                'code': 'H2_SHORT',
                'reason': f"ì†Œì œëª© ë¶€ì¡± (í˜„ì¬ {h2_count}ê°œ, ëª©í‘œ {expected_h2}ê°œ)",
                'feedback': (
                    f"ì†Œì œëª©(<h2>)ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ë„ì…ì„ ì œì™¸í•œ ë³¸ë¡ +ê²°ë¡  ê¸°ì¤€ìœ¼ë¡œ ì •í™•íˆ {expected_h2}ê°œì˜ "
                    f"<h2>ë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤."
                )
            }
        
        if h2_count > expected_h2:
            return {
                'passed': False,
                'code': 'H2_LONG',
                'reason': f"ì†Œì œëª© ê³¼ë‹¤ (í˜„ì¬ {h2_count}ê°œ, ëª©í‘œ {expected_h2}ê°œ)",
                'feedback': (
                    f"ì†Œì œëª©(<h2>)ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤. ë„ì…ì„ ì œì™¸í•œ ë³¸ë¡ +ê²°ë¡  ì†Œì œëª© ìˆ˜ë¥¼ ì •í™•íˆ {expected_h2}ê°œë¡œ "
                    f"ë§ì¶”ì‹­ì‹œì˜¤."
                )
            }

        p_open_tags = re.findall(r'<p\b[^>]*>', content, re.IGNORECASE)
        p_close_tags = re.findall(r'</p\s*>', content, re.IGNORECASE)
        p_count = len(p_open_tags)
        if p_count != len(p_close_tags):
            return {
                'passed': False,
                'code': 'P_MALFORMED',
                'reason': f"p íƒœê·¸ ì§ ë¶ˆì¼ì¹˜ (ì—´ë¦¼ {p_count}ê°œ, ë‹«í˜ {len(p_close_tags)}ê°œ)",
                'feedback': 'ëª¨ë“  ë¬¸ë‹¨ì€ <p>...</p> í˜•íƒœë¡œ ì •í™•íˆ ë‹«ì•„ ì£¼ì‹­ì‹œì˜¤.'
            }

        expected_min_p = total_sections * 2
        expected_max_p = total_sections * 4
        
        if p_count < expected_min_p:
             return {
                'passed': False,
                'code': 'P_SHORT',
                'reason': f"ë¬¸ë‹¨ ìˆ˜ ë¶€ì¡± (í˜„ì¬ {p_count}ê°œ, í•„ìš” {expected_min_p}ê°œ ì´ìƒ)",
                'feedback': (
                    f"ë¬¸ë‹¨ ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ì´ {total_sections}ê°œ ì„¹ì…˜ ê¸°ì¤€ìœ¼ë¡œ ìµœì†Œ {expected_min_p}ê°œ "
                    f"ë¬¸ë‹¨(ì„¹ì…˜ë‹¹ 2ê°œ ì´ìƒ)ì´ í•„ìš”í•©ë‹ˆë‹¤."
                )
            }

        if p_count > expected_max_p:
            return {
                'passed': False,
                'code': 'P_LONG',
                'reason': f"ë¬¸ë‹¨ ìˆ˜ ê³¼ë‹¤ (í˜„ì¬ {p_count}ê°œ, ìµœëŒ€ {expected_max_p}ê°œ)",
                'feedback': (
                    f"ë¬¸ë‹¨ ìˆ˜ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤. ì´ {total_sections}ê°œ ì„¹ì…˜ ê¸°ì¤€ìœ¼ë¡œ {expected_max_p}ê°œ ì´í•˜ë¡œ ì¤„ì´ê³ , "
                    f"ë¬¸ë‹¨ì„ í•©ì³ ì¤‘ë³µ ì„¤ëª…ì„ ì••ì¶•í•˜ì‹­ì‹œì˜¤."
                )
            }

        # ì¥ì†Œ í‚¤ì›Œë“œê°€ ë‹¨ë… 1ë¬¸ì¥ ë¬¸ë‹¨ìœ¼ë¡œ ë°˜ë³µë˜ëŠ” íŒ¨í„´ì„ í•˜ë“œ ì°¨ë‹¨í•œë‹¤.
        # (í•œ ë²ˆì˜ ì•ˆë‚´ì„± ë‹¨ë¬¸ì€ í—ˆìš©í•˜ë˜, 2íšŒ ì´ìƒ ë°˜ë³µë˜ë©´ ë°˜ë ¤)
        location_orphan_paragraphs: List[str] = []
        paragraph_blocks = re.findall(r'<p\b[^>]*>([\s\S]*?)</p\s*>', content, re.IGNORECASE)
        location_pattern = re.compile(r'(ì„œë©´\s*ì˜ê´‘ë„ì„œ|ë¶€ì‚°\s*ì˜ê´‘ë„ì„œ)', re.IGNORECASE)
        for block in paragraph_blocks:
            paragraph_text = re.sub(r'<[^>]*>', ' ', block)
            paragraph_text = re.sub(r'\s+', ' ', paragraph_text).strip()
            if not paragraph_text:
                continue
            if not location_pattern.search(paragraph_text):
                continue

            sentence_tokens = [
                token.strip()
                for token in re.split(r'(?<=[.!?ã€‚])\s+', paragraph_text)
                if token and token.strip()
            ]
            sentence_count = len(sentence_tokens) if sentence_tokens else (1 if paragraph_text else 0)
            if sentence_count <= 1:
                location_orphan_paragraphs.append(paragraph_text)

        if len(location_orphan_paragraphs) >= 2:
            samples = '; '.join(f'"{text[:40]}{"..." if len(text) > 40 else ""}"' for text in location_orphan_paragraphs[:2])
            return {
                'passed': False,
                'code': 'LOCATION_ORPHAN_REPEAT',
                'reason': f"ì¥ì†Œ ë‹¨ë¬¸ ë°˜ë³µ ({len(location_orphan_paragraphs)}íšŒ)",
                'feedback': (
                    "ì„œë©´/ë¶€ì‚° ì˜ê´‘ë„ì„œê°€ ë“¤ì–´ê°„ ë‹¨ë… 1ë¬¸ì¥ ë¬¸ë‹¨ì´ ë°˜ë³µë˜ì—ˆìŠµë‹ˆë‹¤. "
                    "ì¥ì†Œ í‘œí˜„ì€ ê¸°ì¡´ ë¬¸ë‹¨ì˜ ì£¼ì¥/ê·¼ê±°/í–‰ë™ ì œì•ˆê³¼ ê²°í•©í•´ ì„œìˆ í•˜ê³ , "
                    "ë‹¨ë… ì•ˆë‚´ ë¬¸ë‹¨ì€ ìµœëŒ€ 1íšŒë§Œ í—ˆìš©ë©ë‹ˆë‹¤. "
                    f"ë°˜ë³µ ì˜ˆì‹œ: {samples}"
                )
            }

        plain_text = re.sub(r'<[^>]*>', ' ', content)
        plain_text = re.sub(r'\s+', ' ', plain_text).strip()

        meta_leaks = self._find_meta_prompt_leak_sentences(content)
        if meta_leaks:
            sample = "; ".join(f'"{item[:45]}{"..." if len(item) > 45 else ""}"' for item in meta_leaks[:2])
            return {
                'passed': False,
                'code': 'META_PROMPT_LEAK',
                'reason': f"ë©”íƒ€ ë¬¸êµ¬ ëˆ„ìˆ˜ ({len(meta_leaks)}ë¬¸ì¥)",
                'feedback': (
                    "í”„ë¡¬í”„íŠ¸ ê·œì¹™ ì„¤ëª… ë¬¸ì¥ì´ ë³¸ë¬¸ìœ¼ë¡œ ì¶œë ¥ë˜ì—ˆìŠµë‹ˆë‹¤. "
                    "í•´ë‹¹ ë¬¸ì¥ì„ ì‚­ì œí•˜ê³  ì‹¤ì œ í–‰ì‚¬Â·ì •ì±… ë‚´ìš©ìœ¼ë¡œ ëŒ€ì²´í•˜ì‹­ì‹œì˜¤. "
                    f"ëˆ„ìˆ˜ ì˜ˆì‹œ: {sample}"
                )
            }

        overused_anchor_phrases = self._find_overused_anchor_phrases(content)
        if overused_anchor_phrases:
            detail = ", ".join(overused_anchor_phrases[:2])
            return {
                'passed': False,
                'code': 'PHRASE_REPEAT_CAP',
                'reason': f"ìƒíˆ¬ êµ¬ë¬¸ ë°˜ë³µ ê³¼ë‹¤ ({detail})",
                'feedback': (
                    "ëŒ€í‘œ ë¬¸êµ¬ ë°˜ë³µì´ ê³¼ë‹¤í•©ë‹ˆë‹¤. ë™ì¼ ì–´êµ¬ëŠ” ì›ê³  ì „ì²´ ìµœëŒ€ 2íšŒë¡œ ì œí•œí•˜ê³ , "
                    "ì´ˆê³¼ êµ¬ê°„ì€ ìƒˆë¡œìš´ ì‚¬ì‹¤/ê·¼ê±° ë¬¸ì¥ìœ¼ë¡œ êµì²´í•˜ì‹­ì‹œì˜¤."
                )
            }

        material_reuse_issues = self._detect_material_reuse_issues(
            content,
            context_analysis,
            max_mentions=1,
        )
        if material_reuse_issues:
            samples = "; ".join(
                f"[{item.get('type')}] {str(item.get('text') or '')[:36]}"
                for item in material_reuse_issues[:2]
            )
            return {
                'passed': False,
                'code': 'MATERIAL_REUSE',
                'reason': f"ë™ì¼ ì†Œì¬ ì¬ì‚¬ìš© ê°ì§€ ({len(material_reuse_issues)}ê°œ)",
                'feedback': (
                    "ê°™ì€ ì¸ìš©/ì¼í™”/ê·¼ê±°ê°€ ì—¬ëŸ¬ ì„¹ì…˜ì—ì„œ ë°˜ë³µë˜ì—ˆìŠµë‹ˆë‹¤. "
                    "ë³¸ë¡  ì„¹ì…˜ë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ ì†Œì¬ë¥¼ 1íšŒì”©ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì¬ì‘ì„±í•˜ì‹­ì‹œì˜¤. "
                    f"ì¤‘ë³µ ì˜ˆì‹œ: {samples}"
                ),
            }

        event_signal_patterns = [
            r'\d{1,2}\s*ì›”\s*\d{1,2}\s*ì¼',
            r'ì¶œíŒê¸°ë…íšŒ',
            r'í–‰ì‚¬',
            r'ì˜¤í›„\s*\d{1,2}\s*ì‹œ',
            r'ì„œë©´\s*ì˜ê´‘ë„ì„œ',
            r'ë¶€ì‚°\s*ì˜ê´‘ë„ì„œ',
        ]
        event_signal_hits = sum(
            1 for pattern in event_signal_patterns if re.search(pattern, plain_text, re.IGNORECASE)
        )
        if is_event_announcement or event_signal_hits >= 2:
            event_fact_mentions = self._count_event_fact_sentence_mentions(
                content,
                event_date_hint=event_date_hint,
                event_location_hint=event_location_hint,
            )
            if event_fact_mentions > 2:
                return {
                    'passed': False,
                    'code': 'EVENT_FACT_REPEAT',
                    'reason': f"í–‰ì‚¬ ì¼ì‹œ/ì¥ì†Œ ë¬¸ì¥ ë°˜ë³µ ê³¼ë‹¤ ({event_fact_mentions}íšŒ)",
                    'feedback': (
                        "í–‰ì‚¬ ì¼ì‹œ+ì¥ì†Œê°€ ê²°í•©ëœ ì•ˆë‚´ ë¬¸ì¥ì€ ë„ì… 1íšŒ, ê²°ë¡  1íšŒê¹Œì§€ë§Œ í—ˆìš©ë©ë‹ˆë‹¤. "
                        "ì¤‘ê°„ ë³¸ë¡ ì—ì„œëŠ” 'ì´ë²ˆ í–‰ì‚¬ í˜„ì¥'ì²˜ëŸ¼ ë³€í˜•í•´ ë°˜ë³µì„ ì¤„ì´ì‹­ì‹œì˜¤."
                    )
                }

            invite_patterns = {
                'ì§ì ‘ ë§Œë‚˜': r'ì§ì ‘\s*ë§Œë‚˜',
                'ì§„ì†”í•œ ì†Œí†µ': r'ì§„ì†”í•œ\s*ì†Œí†µ',
                'ê¸°ë‹¤ë¦¬ê² ìŠµë‹ˆë‹¤': r'ê¸°ë‹¤ë¦¬ê² ìŠµë‹ˆë‹¤',
            }
            overused_phrases = []
            for label, pattern in invite_patterns.items():
                count = len(re.findall(pattern, plain_text, re.IGNORECASE))
                if count > 2:
                    overused_phrases.append(f"{label} {count}íšŒ")

            if overused_phrases:
                detail = ", ".join(overused_phrases)
                return {
                    'passed': False,
                    'code': 'EVENT_INVITE_REDUNDANT',
                    'reason': f"í–‰ì‚¬ ì´ˆëŒ€ ë¬¸êµ¬ ë°˜ë³µ ê³¼ë‹¤ ({detail})",
                    'feedback': (
                        "í–‰ì‚¬ ì•ˆë‚´ë¬¸ì€ ì •ë³´ ì¤‘ì‹¬ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. "
                        "\"ì§ì ‘ ë§Œë‚˜\", \"ì§„ì†”í•œ ì†Œí†µ\", \"ê¸°ë‹¤ë¦¬ê² ìŠµë‹ˆë‹¤\" ë¥˜ ë¬¸êµ¬ëŠ” ê° 2íšŒ ì´í•˜ë¡œ ì¤„ì´ê³ , "
                        "ë°˜ë³µ êµ¬ê°„ì€ í–‰ì‚¬ í•µì‹¬ ì •ë³´(ì¼ì‹œ/ì¥ì†Œ/ì°¸ì—¬ ë°©ë²•) ë˜ëŠ” ìƒˆë¡œìš´ ê·¼ê±° ì„¤ëª…ìœ¼ë¡œ ëŒ€ì²´í•˜ì‹­ì‹œì˜¤."
                    )
                }

        return {'passed': True}
