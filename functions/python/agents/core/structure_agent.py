
import re
import json
import logging
import time
from typing import Dict, Any, Optional, List, Tuple

# API Call Timeout (seconds)
LLM_CALL_TIMEOUT = 120  # 2ë¶„ íƒ€ì„ì•„ì›ƒ
CONTEXT_ANALYZER_TIMEOUT = 60  # 1ë¶„ íƒ€ì„ì•„ì›ƒ

# Local imports
from ..common.classifier import classify_topic
from ..common.warnings import generate_non_lawmaker_warning, generate_family_status_warning
from ..common.theminjoo import get_party_stance
from ..common.election_rules import get_election_stage, get_prompt_instruction
from ..common.seo import build_seo_instruction
from ..common.constants import resolve_writing_method

# Template Builders
from ..templates.daily_communication import build_daily_communication_prompt
from ..templates.activity_report import build_activity_report_prompt
from ..templates.policy_proposal import build_policy_proposal_prompt
from ..templates.current_affairs import build_critical_writing_prompt, build_diagnosis_writing_prompt
from ..templates.local_issues import build_local_issues_prompt

# Template Builders Mapping
TEMPLATE_BUILDERS = {
    'emotional_writing': build_daily_communication_prompt,
    'logical_writing': build_policy_proposal_prompt, # Mapped buildLogicalWritingPrompt to policy proposal
    'direct_writing': build_activity_report_prompt,
    'critical_writing': build_critical_writing_prompt,
    'diagnostic_writing': build_diagnosis_writing_prompt,
    'analytical_writing': build_local_issues_prompt
}

logger = logging.getLogger(__name__)

def strip_html(text: str) -> str:
    if not text:
        return ''
    text = re.sub(r'<[^>]*>', '', text)
    return re.sub(r'\s+', '', text).strip()

def normalize_artifacts(text: str) -> str:
    if not text:
        return ''
    cleaned = text.strip()
    # ì½”ë“œíœìŠ¤ê°€ ê°ì‹¸ì ¸ ì˜¨ ê²½ìš° ë³¸ë¬¸ì„ ë³´ì¡´í•œ ì±„ íœìŠ¤ë§Œ ì œê±°
    cleaned = re.sub(
        r'```(?:[\w.+-]+)?\s*([\s\S]*?)\s*```',
        lambda m: m.group(1).strip(),
        cleaned,
    ).strip()
    cleaned = re.sub(r'^\s*\\"', '', cleaned)
    cleaned = re.sub(r'\\"?\s*$', '', cleaned)
    cleaned = re.sub(r'^\s*["â€œ]', '', cleaned)
    cleaned = re.sub(r'["â€]\s*$', '', cleaned)
    
    # Remove trailing metadata block only when marker appears near the tail as a standalone line.
    # (Do not truncate normal body text that happens to include "ì¹´í…Œê³ ë¦¬:" in a sentence.)
    lines = cleaned.splitlines()
    metadata_line_re = re.compile(
        r'^\s*\*{0,2}(ì¹´í…Œê³ ë¦¬|ê²€ìƒ‰ì–´ ì‚½ì… íšŸìˆ˜|ìƒì„± ì‹œê°„)\*{0,2}\s*:\s*',
        re.IGNORECASE,
    )
    tail_cut_index = None
    if lines:
        tail_window_start = max(0, len(lines) - 8)
        for i in range(tail_window_start, len(lines)):
            line = lines[i].strip()
            if not line:
                continue
            # HTML lineì€ ë³¸ë¬¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ metadata ì‹œì‘ì ìœ¼ë¡œ ë³´ì§€ ì•ŠìŒ
            if '<' in line and '>' in line:
                continue
            if metadata_line_re.match(line):
                tail_cut_index = i
                break
    if tail_cut_index is not None:
        cleaned = "\n".join(lines[:tail_cut_index]).strip()
    
    cleaned = re.sub(r'"content"\s*:\s*', '', cleaned)
    
    return cleaned.strip()


def normalize_html_structure_tags(text: str) -> str:
    """ê¸°ë³¸ êµ¬ì¡° íƒœê·¸ë¥¼ í‘œì¤€ í˜•íƒœë¡œ ì •ê·œí™”í•œë‹¤.

    ì—„ê²© ê¸°ì¤€ì€ ìœ ì§€í•˜ë˜, ëª¨ë¸ì´ ìƒì„±í•œ ë¶€ê°€ ì†ì„±/ëŒ€ì†Œë¬¸ì ì°¨ì´ë¡œ
    êµ¬ì¡° ê²€ì¦ì´ ì˜¤íƒìœ¼ë¡œ ì‹¤íŒ¨í•˜ëŠ” ìƒí™©ì„ ì¤„ì¸ë‹¤.
    """
    if not text:
        return ''
    normalized = text
    normalized = re.sub(r'<\s*h2\b[^>]*>', '<h2>', normalized, flags=re.IGNORECASE)
    normalized = re.sub(r'<\s*/\s*h2\s*>', '</h2>', normalized, flags=re.IGNORECASE)
    normalized = re.sub(r'<\s*p\b[^>]*>', '<p>', normalized, flags=re.IGNORECASE)
    normalized = re.sub(r'<\s*/\s*p\s*>', '</p>', normalized, flags=re.IGNORECASE)
    return normalized


def is_example_like_block(text: str) -> bool:
    """ì˜ˆì‹œ/í…œí”Œë¦¿ ë¸”ë¡ ì—¬ë¶€ë¥¼ íŒì •í•œë‹¤.

    ê·œì¹™ ì™„í™”ê°€ ì•„ë‹ˆë¼, ëª¨ë¸ì´ í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œë¥¼ ê·¸ëŒ€ë¡œ ì¬ì¶œë ¥í•œ ë¸”ë¡ì„
    ë³¸ë¬¸ í›„ë³´ì—ì„œ ì œì™¸í•˜ê¸° ìœ„í•œ ë°©ì–´ ë¡œì§ì´ë‹¤.
    """
    if not text:
        return True
    lowered = text.lower()
    placeholder_count = len(re.findall(r'\[[^\]\n]{1,40}\]', text))
    marker_count = sum(
        1
        for marker in (
            'sample_output',
            'reference_example',
            'placeholder',
            'ì˜ˆì‹œ',
            'ìƒ˜í”Œ',
            'ì—¬ê¸°ì—',
            'ì‘ì„±',
        )
        if marker in lowered
    )
    plain_len = len(strip_html(text))
    return (
        placeholder_count >= 2
        or ('<![cdata[' in lowered and plain_len < 900)
        or (marker_count >= 1 and plain_len < 900)
    )


def normalize_context_text(value: Any, *, sep: str = "\n\n") -> str:
    """list/tuple ì¤‘ì²© ì…ë ¥ê¹Œì§€ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ì •ê·œí™”."""
    if value is None:
        return ""
    if isinstance(value, str):
        return value.strip()
    if isinstance(value, (list, tuple, set)):
        parts: List[str] = []
        for item in value:
            normalized = normalize_context_text(item, sep=sep)
            if normalized:
                parts.append(normalized)
        return sep.join(parts)
    return str(value).strip()

from ..base_agent import Agent

class StructureAgent(Agent):
    def __init__(self, name: str = 'StructureAgent', options: Optional[Dict[str, Any]] = None):
        super().__init__(name, options)

        # ê³µí†µ Gemini í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© (ìƒˆ google-genai SDK)
        from ..common.gemini_client import get_client, DEFAULT_MODEL
        self.model_name = DEFAULT_MODEL

        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” í™•ì¸
        client = get_client()
        if client:
            print(f"ğŸ¤– [StructureAgent] ëª¨ë¸: {self.model_name}")
        else:
            print(f"âš ï¸ [StructureAgent] Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")

    def _sanitize_target_word_count(self, target_word_count: Any) -> int:
        try:
            parsed = int(float(target_word_count))
        except (TypeError, ValueError):
            return 2000
        return max(1600, min(parsed, 3200))

    def _build_length_spec(self, target_word_count: Any, stance_count: int = 0) -> Dict[str, int]:
        target_chars = self._sanitize_target_word_count(target_word_count)

        # ì„¹ì…˜ë‹¹ 400ì ë‚´ì™¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 5~7ì„¹ì…˜ ê³„íš
        total_sections = round(target_chars / 400)
        total_sections = max(5, min(7, total_sections))
        if stance_count > 0:
            total_sections = max(total_sections, min(7, stance_count + 2))

        body_sections = total_sections - 2
        per_section_recommended = max(360, min(420, round(target_chars / total_sections)))
        per_section_min = max(320, per_section_recommended - 50)
        per_section_max = min(460, per_section_recommended + 50)

        min_chars = max(int(target_chars * 0.88), total_sections * per_section_min)
        max_chars = min(int(target_chars * 1.18), total_sections * per_section_max)
        if max_chars <= min_chars:
            max_chars = min_chars + 180

        return {
            'target_chars': target_chars,
            'body_sections': body_sections,
            'total_sections': total_sections,
            'per_section_min': per_section_min,
            'per_section_max': per_section_max,
            'per_section_recommended': per_section_recommended,
            'min_chars': min_chars,
            'max_chars': max_chars,
            'expected_h2': total_sections - 1
        }

    def _build_retry_directive(self, validation: Dict[str, Any], length_spec: Dict[str, int]) -> str:
        code = validation.get('code')
        total_sections = length_spec['total_sections']
        body_sections = length_spec['body_sections']
        min_chars = length_spec['min_chars']
        max_chars = length_spec['max_chars']
        per_section_recommended = length_spec['per_section_recommended']
        expected_h2 = length_spec['expected_h2']

        if code == 'LENGTH_SHORT':
            return (
                f"ì¬ì‘ì„± ì‹œ ì´ ë¶„ëŸ‰ì„ {min_chars}~{max_chars}ìë¡œ ë§ì¶”ì‹­ì‹œì˜¤. "
                f"ì´ ì„¹ì…˜ì€ ë„ì… 1 + ë³¸ë¡  {body_sections} + ê²°ë¡  1(ì´ {total_sections})ë¡œ ìœ ì§€í•˜ê³ , "
                f"ì„¹ì…˜ë‹¹ {per_section_recommended}ì ë‚´ì™¸ë¡œ ë³´ê°•í•˜ì‹­ì‹œì˜¤."
            )

        if code == 'LENGTH_LONG':
            return (
                f"ì¬ì‘ì„± ì‹œ ì´ ë¶„ëŸ‰ì„ {max_chars}ì ì´í•˜ë¡œ ì••ì¶•í•˜ì‹­ì‹œì˜¤(ì ˆëŒ€ ì´ˆê³¼ ê¸ˆì§€). "
                f"ì¤‘ë³µ ë¬¸ì¥, ìˆ˜ì‹ì–´, ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ì œê±°í•˜ê³  ì„¹ì…˜ë‹¹ {per_section_recommended}ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì‹­ì‹œì˜¤."
            )

        if code in {'H2_SHORT', 'H2_LONG'}:
            return (
                f"ì„¹ì…˜ êµ¬ì¡°ë¥¼ ì •í™•íˆ ë§ì¶”ì‹­ì‹œì˜¤: ë„ì… 1 + ë³¸ë¡  {body_sections} + ê²°ë¡  1. "
                f"<h2>ëŠ” ë³¸ë¡ ê³¼ ê²°ë¡ ì—ë§Œ ì‚¬ìš©í•˜ì—¬ ì´ {expected_h2}ê°œë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤. "
                f"ì†Œì œëª© íƒœê·¸ëŠ” ì†ì„± ì—†ì´ ë°˜ë“œì‹œ <h2>í…ìŠ¤íŠ¸</h2> í˜•ì‹ë§Œ í—ˆìš©ë©ë‹ˆë‹¤."
            )

        if code in {'P_SHORT', 'P_LONG'}:
            return (
                f"ë¬¸ë‹¨ ìˆ˜ë¥¼ ì¡°ì •í•˜ì‹­ì‹œì˜¤. ì´ {total_sections}ê°œ ì„¹ì…˜ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ì€ 2~3ê°œì”© ìœ ì§€í•˜ê³ , "
                f"êµ°ë”ë”ê¸° ì—†ëŠ” ë¬¸ì¥ìœ¼ë¡œ ê¸¸ì´ ë²”ìœ„({min_chars}~{max_chars}ì)ë¥¼ ì§€í‚¤ì‹­ì‹œì˜¤."
            )

        return (
            f"ì´ {total_sections}ê°œ ì„¹ì…˜ êµ¬ì¡°ì™€ ë¶„ëŸ‰ ë²”ìœ„({min_chars}~{max_chars}ì)ë¥¼ ì •í™•íˆ ì¤€ìˆ˜í•˜ì—¬ ì¬ì‘ì„±í•˜ì‹­ì‹œì˜¤."
        )

    async def process(self, context: Dict[str, Any]) -> Dict[str, Any]:
        topic = context.get('topic', '')
        user_profile = context.get('userProfile', {})
        # ë°©ì–´ ì½”ë“œ - listë¡œ ì „ë‹¬ë˜ëŠ” ê²½ìš° ë°©ì–´
        if not isinstance(user_profile, dict):
            user_profile = {}
        category = context.get('category', '')
        sub_category = context.get('subCategory', '')
        instructions = normalize_context_text(context.get('instructions', ''))
        news_context = normalize_context_text(context.get('newsContext', ''))
        # ğŸ”‘ [NEW] ì…ì¥ë¬¸ê³¼ ë‰´ìŠ¤/ë°ì´í„° ë¶„ë¦¬
        stance_text = normalize_context_text(context.get('stanceText', ''))
        news_data_text = normalize_context_text(context.get('newsDataText', ''))
        target_word_count = context.get('targetWordCount', 2000)
        user_keywords = context.get('userKeywords', [])
        memory_context = normalize_context_text(context.get('memoryContext', ''), sep="\n")

        print(f"ğŸš€ [StructureAgent] ì‹œì‘ - ì¹´í…Œê³ ë¦¬: {category or '(ìë™)'}, ì£¼ì œ: {topic}")
        print(f"ğŸ“Š [StructureAgent] ì…ì¥ë¬¸: {len(stance_text)}ì, ë‰´ìŠ¤/ë°ì´í„°: {len(news_data_text)}ì")

        # 1. Determine Writing Method
        writing_method = ''
        if category and category != 'auto':
            writing_method = resolve_writing_method(category, sub_category)
            print(f"âœï¸ [StructureAgent] ì‘ë²• ì„ íƒ (ì¹´í…Œê³ ë¦¬ ê¸°ë°˜): {writing_method}")
        else:
            classification = await classify_topic(topic)
            writing_method = classification['writingMethod']
            print(f"ğŸ¤– [StructureAgent] ì‘ë²• ìë™ ì¶”ë¡ : {writing_method} (ì‹ ë¢°ë„: {classification.get('confidence')}, ì†ŒìŠ¤: {classification.get('source')})")

        # 2. Build Author Bio
        author_bio, author_name = self.build_author_bio(user_profile)

        # 3. Get Party Stance
        party_stance_guide = None
        try:
             party_stance_guide = get_party_stance(topic)
        except Exception as e:
             print(f"âš ï¸ [StructureAgent] ë‹¹ë¡  ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

        # 4. ContextAnalyzer (ì…ì¥ë¬¸/ë‰´ìŠ¤ ë¶„ë¦¬ ì²˜ë¦¬)
        context_analysis = await self.run_context_analyzer(
            stance_text or instructions,  # ì…ì¥ë¬¸ ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ì¡´ instructions ì‚¬ìš©
            news_data_text or news_context,  # ë‰´ìŠ¤ ë°ì´í„° ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ì¡´ newsContext ì‚¬ìš©
            author_name
        )
        stance_count = len(context_analysis.get('mustIncludeFromStance', [])) if context_analysis else 0
        length_spec = self._build_length_spec(target_word_count, stance_count)
        print(
            f"ğŸ“ [StructureAgent] ë¶„ëŸ‰ ê³„íš: {length_spec['total_sections']}ì„¹ì…˜, "
            f"{length_spec['min_chars']}~{length_spec['max_chars']}ì "
            f"(ì„¹ì…˜ë‹¹ {length_spec['per_section_recommended']}ì)"
        )

        # 5. Build Prompt
        prompt = self.build_prompt({
            'topic': topic,
            'category': category,
            'writingMethod': writing_method,
            'authorName': author_name,
            'authorBio': author_bio,
            'instructions': instructions,
            'newsContext': news_context,
            'targetWordCount': target_word_count,
            'partyStanceGuide': party_stance_guide,
            'contextAnalysis': context_analysis,
            'userProfile': user_profile,
            'memoryContext': memory_context,
            'userKeywords': user_keywords,
            'lengthSpec': length_spec
        })

        print(f"ğŸ“ [StructureAgent] í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ ({len(prompt)}ì)")

        # 6. Retry Loop (ê²€ì¦ ë¡œì§ ì—„ê²©í•˜ê²Œ ë³µêµ¬, ëŒ€ì‹  í”„ë¡¬í”„íŠ¸ë¥¼ ê°•í™”í•˜ì—¬ ì„±ê³µë¥  ì œê³ )
        max_retries = 2
        attempt = 0
        feedback = ''
        retry_directive = ''
        validation = {}
        last_error = None  # ë§ˆì§€ë§‰ ì˜ˆì™¸ ì¶”ì 
        last_content = ''
        last_title = ''

        while attempt <= max_retries:
            attempt += 1
            print(f"ğŸ”„ [StructureAgent] ìƒì„± ì‹œë„ {attempt}/{max_retries + 1}")

            current_prompt = prompt
            if feedback:
                retry_block = f"\n\n{retry_directive}" if retry_directive else ""
                current_prompt += (
                    f"\n\nğŸš¨ [ì¤‘ìš” - ì¬ì‘ì„± ì§€ì‹œ] ì´ì „ ì‘ì„±ë³¸ì´ ë‹¤ìŒ ì´ìœ ë¡œ ë°˜ë ¤ë˜ì—ˆìŠµë‹ˆë‹¤:\n"
                    f"\"{feedback}\"{retry_block}"
                )

            try:
                response = await self.call_llm(current_prompt)
                print(f"ğŸ“¥ [StructureAgent] LLM ì›ë³¸ ì‘ë‹µ ({len(response)}ì)")

                structured = self.parse_response(response)
                content = normalize_artifacts(structured['content'])
                content = normalize_html_structure_tags(content)
                title = normalize_artifacts(structured['title'])
                last_content = content
                last_title = title

                # íŒŒì‹±/ì •ë¦¬ ê³¼ì •ì—ì„œ ë³¸ë¬¸ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ì¶•ì•½ëœ ê²½ìš° ì¬ì‹œë„ ìœ ë„.
                # (ì˜ˆ: ë©”íƒ€ë°ì´í„° ì ˆë‹¨ ì˜¤íƒ, ë¸”ë¡ íŒŒì‹± ì‹¤íŒ¨)
                plain_len = len(strip_html(content))
                if plain_len < 120 and len(str(response or "")) > 1000:
                    raise Exception(f"íŒŒì‹± ë¹„ì •ìƒ ì¶•ì•½ ê°ì§€ ({plain_len}ì)")

                validation = self.validate_output(content, length_spec)

                if validation['passed']:
                    print(f"âœ… [StructureAgent] ê²€ì¦ í†µê³¼: {len(strip_html(content))}ì")

                    if not title.strip():
                        title = topic[:20] if topic else 'ìƒˆ ì›ê³ '

                    return {
                        'content': content,
                        'title': title,
                        'writingMethod': writing_method,
                        'contextAnalysis': context_analysis
                    }

                print(f"âš ï¸ [StructureAgent] ê²€ì¦ ì‹¤íŒ¨: {validation['reason']}")
                feedback = validation['feedback']
                retry_directive = self._build_retry_directive(validation, length_spec)
                last_error = None  # ê²€ì¦ ì‹¤íŒ¨ëŠ” ì˜ˆì™¸ê°€ ì•„ë‹˜

            except Exception as e:
                error_msg = str(e)
                print(f"âŒ [StructureAgent] ì—ëŸ¬ ë°œìƒ: {error_msg}")
                feedback = error_msg
                retry_directive = ''
                last_error = error_msg  # ì˜ˆì™¸ ë©”ì‹œì§€ ì €ì¥

            if attempt > max_retries:
                # ì—„ê²© ê¸°ì¤€ì€ ìœ ì§€í•˜ë˜, êµ¬ì¡°/ë¶„ëŸ‰ ê²€ì¦ ì‹¤íŒ¨ ì‹œ 1íšŒ ë³´ê°• ì¬ì‘ì„± ì‹œë„
                recoverable_codes = {
                    'LENGTH_SHORT',
                    'H2_SHORT',
                    'H2_LONG',
                    'H2_MALFORMED',
                    'P_SHORT',
                    'P_LONG',
                    'P_MALFORMED',
                    'TAG_DISALLOWED',
                    'TEMPLATE_ECHO',
                }
                if validation.get('code') in recoverable_codes and last_content:
                    recovered = await self.recover_structural_shortfall(
                        content=last_content,
                        title=last_title or (topic[:20] if topic else 'ìƒˆ ì›ê³ '),
                        topic=topic,
                        length_spec=length_spec,
                        failed_code=str(validation.get('code') or ''),
                        failed_reason=str(validation.get('reason') or ''),
                        failed_feedback=str(validation.get('feedback') or ''),
                    )
                    if recovered:
                        recovered_content, recovered_title = recovered
                        recovered_validation = self.validate_output(recovered_content, length_spec)
                        if recovered_validation.get('passed'):
                            print(
                                f"âœ… [StructureAgent] êµ¬ì¡°/ë¶„ëŸ‰ ë³´ê°• ë³µêµ¬ ì„±ê³µ: "
                                f"{len(strip_html(recovered_content))}ì"
                            )
                            return {
                                'content': recovered_content,
                                'title': recovered_title or (topic[:20] if topic else 'ìƒˆ ì›ê³ '),
                                'writingMethod': writing_method,
                                'contextAnalysis': context_analysis
                            }
                        validation = recovered_validation

                # ë§ˆì§€ë§‰ ì—ëŸ¬ê°€ ì˜ˆì™¸ë©´ ê·¸ ë©”ì‹œì§€ ì‚¬ìš©, ì•„ë‹ˆë©´ ê²€ì¦ ì‹¤íŒ¨ ì´ìœ  ì‚¬ìš©
                final_reason = last_error or validation.get('reason', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                raise Exception(f"StructureAgent ì‹¤íŒ¨ ({max_retries}íšŒ ì¬ì‹œë„ í›„): {final_reason}")

    async def call_llm(self, prompt: str) -> str:
        from ..common.gemini_client import generate_content_async

        print(f"ğŸ“¤ [StructureAgent] LLM í˜¸ì¶œ ì‹œì‘")
        start_time = time.time()

        try:
            response_text = await generate_content_async(
                prompt,
                model_name=self.model_name,
                temperature=0.1,  # êµ¬ì¡° ì¤€ìˆ˜ìœ¨ì„ ë†’ì´ê¸° ìœ„í•´ ë³€ë™ì„± ì¶•ì†Œ
                max_output_tokens=4096
            )

            elapsed = time.time() - start_time
            print(f"âœ… [StructureAgent] LLM ì‘ë‹µ ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")

            return response_text

        except Exception as e:
            elapsed = time.time() - start_time
            error_msg = str(e)
            print(f"âŒ [StructureAgent] LLM í˜¸ì¶œ ì‹¤íŒ¨ ({elapsed:.1f}ì´ˆ): {error_msg}")

            # íƒ€ì„ì•„ì›ƒ ê´€ë ¨ ì—ëŸ¬ ë©”ì‹œì§€ ê°œì„ 
            if 'timeout' in error_msg.lower() or 'deadline' in error_msg.lower():
                raise Exception(f"LLM í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ ({elapsed:.1f}ì´ˆ). Gemini APIê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            raise

    async def recover_length_shortfall(
        self,
        *,
        content: str,
        title: str,
        topic: str,
        length_spec: Dict[str, int],
    ) -> Optional[Tuple[str, str]]:
        current_len = len(strip_html(content))
        min_len = int(length_spec.get('min_chars', 0))
        max_len = int(length_spec.get('max_chars', 0))
        expected_h2 = int(length_spec.get('expected_h2', 0))

        # ì§€ë‚˜ì¹˜ê²Œ ì§§ì€ ê²½ìš°ëŠ” ë³´ê°•ë³´ë‹¤ ê·¼ë³¸ ì¬ì‘ì„±ì´ í•„ìš”í•˜ë¯€ë¡œ ìŠ¤í‚µ.
        if min_len <= 0 or current_len < int(min_len * 0.75):
            return None

        from ..common.gemini_client import generate_content_async

        gap = max(0, min_len - current_len)
        prompt = f"""
ë‹¹ì‹ ì€ ì—„ê²©í•œ í¸ì§‘ìì…ë‹ˆë‹¤. ì•„ë˜ ì›ê³ ëŠ” êµ¬ì¡°ëŠ” ëŒ€ì²´ë¡œ ë§ì§€ë§Œ ë¶„ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.
ê·œì¹™ì„ ì™„í™”í•˜ì§€ ë§ê³ , ê¸°ì¡´ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë‚´ìš©ì„ ë³´ê°•í•´ ìµœì†Œ ë¶„ëŸ‰ì„ ì¶©ì¡±ì‹œí‚¤ì‹­ì‹œì˜¤.

[ëª©í‘œ]
- í˜„ì¬ ë¶„ëŸ‰: {current_len}ì
- ìµœì†Œ ë¶„ëŸ‰: {min_len}ì
- ìµœëŒ€ ë¶„ëŸ‰: {max_len}ì
- ë³´ê°• í•„ìš”ëŸ‰: ìµœì†Œ {gap}ì
- <h2> ê°œìˆ˜ëŠ” ì •í™•íˆ {expected_h2}ê°œ ìœ ì§€

[ì ˆëŒ€ ê·œì¹™]
1) ê¸°ì¡´ <h2> ì œëª©ì€ ì‚­ì œ/ë³€ê²½í•˜ì§€ ë§ ê²ƒ.
2) <h2> ê°œìˆ˜ëŠ” ì •í™•íˆ {expected_h2}ê°œ ìœ ì§€í•  ê²ƒ.
3) ë¬¸ë‹¨ì€ <p>...</p>ë§Œ ì‚¬ìš©í•˜ê³  ëª¨ë“  íƒœê·¸ë¥¼ ì •í™•íˆ ë‹«ì„ ê²ƒ.
4) ê¸°ì¡´ ì‚¬ì‹¤/ì£¼ì¥ì„ ì™œê³¡í•˜ê±°ë‚˜ ìƒˆ ì‚¬ì‹¤ì„ ì§€ì–´ë‚´ì§€ ë§ ê²ƒ.
5) ì¥í™©í•œ ë°˜ë³µ ê¸ˆì§€. ê° ë‹¨ë½ì€ ìƒˆë¡œìš´ ê·¼ê±°ë‚˜ ì„¤ëª…ì„ ì¶”ê°€í•  ê²ƒ.
6) ìµœì¢… ë¶„ëŸ‰ì´ {min_len}~{max_len}ì ë²”ìœ„ë¥¼ ë°˜ë“œì‹œ ì¶©ì¡±í•  ê²ƒ.

[ì£¼ì œ]
{topic}

[ì›ê³  ì›ë¬¸]
<title>{title}</title>
<content>
{content}
</content>

[ì¶œë ¥ í˜•ì‹]
ì•„ë˜ XML íƒœê·¸ë§Œ ì¶œë ¥:
<title>...</title>
<content>...</content>
"""

        try:
            response_text = await generate_content_async(
                prompt,
                model_name=self.model_name,
                temperature=0.0,
                max_output_tokens=4096,
            )
            parsed = self.parse_response(response_text)
            recovered_content = normalize_html_structure_tags(normalize_artifacts(parsed.get('content', '')))
            recovered_title = normalize_artifacts(parsed.get('title', '')) or title
            if not recovered_content:
                return None
            return recovered_content, recovered_title
        except Exception as e:
            print(f"âš ï¸ [StructureAgent] ë¶„ëŸ‰ ë³´ê°• ë³µêµ¬ ì‹¤íŒ¨: {str(e)}")
            return None

    async def recover_structural_shortfall(
        self,
        *,
        content: str,
        title: str,
        topic: str,
        length_spec: Dict[str, int],
        failed_code: str,
        failed_reason: str,
        failed_feedback: str,
    ) -> Optional[Tuple[str, str]]:
        from ..common.gemini_client import generate_content_async

        current_len = len(strip_html(content))
        min_len = int(length_spec.get('min_chars', 0))
        max_len = int(length_spec.get('max_chars', 0))
        expected_h2 = int(length_spec.get('expected_h2', 0))
        total_sections = int(length_spec.get('total_sections', 5))
        min_p = total_sections * 2
        max_p = total_sections * 4

        prompt = f"""
ë‹¹ì‹ ì€ ì—„ê²©í•œ í¸ì§‘ìì…ë‹ˆë‹¤. ì•„ë˜ ì›ê³ ëŠ” êµ¬ì¡°/í˜•ì‹ ê²€ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.
ê·œì¹™ì„ ë‹¨ í•˜ë‚˜ë„ ì™„í™”í•˜ì§€ ë§ê³ , ì‹¤íŒ¨ ì‚¬ìœ ë¥¼ ë°˜ë“œì‹œ í•´ê²°í•œ ìµœì¢…ë³¸ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì‹­ì‹œì˜¤.

[ì‹¤íŒ¨ ì •ë³´]
- code: {failed_code}
- reason: {failed_reason}
- feedback: {failed_feedback}

[ëª©í‘œ ì œì•½]
- í˜„ì¬ ë¶„ëŸ‰: {current_len}ì
- ìµœì¢… ë¶„ëŸ‰: {min_len}~{max_len}ì
- <h2> ê°œìˆ˜: ì •í™•íˆ {expected_h2}ê°œ
- <p> ê°œìˆ˜: {min_p}~{max_p}ê°œ

[ì ˆëŒ€ ê·œì¹™]
1) í—ˆìš© íƒœê·¸ëŠ” <h2>, <p>ë§Œ ì‚¬ìš©.
2) ëª¨ë“  <h2>, <p> íƒœê·¸ëŠ” ì •í™•íˆ ì—´ê³  ë‹«ì„ ê²ƒ.
3) ë³¸ë¬¸ì—ëŠ” ì˜ˆì‹œ í”Œë ˆì´ìŠ¤í™€ë”([ì œëª©], [ë‚´ìš©], [êµ¬ì²´ì  ëŒ€ì•ˆ] ë“±)ë¥¼ ì ˆëŒ€ ë‚¨ê¸°ì§€ ë§ ê²ƒ.
4) ê¸°ì¡´ í•µì‹¬ ì˜ë¯¸/ì‚¬ì‹¤ì€ ìœ ì§€í•˜ë˜, í˜•ì‹ê³¼ êµ¬ì¡°ë¥¼ ì™„ì „í•˜ê²Œ êµì •í•  ê²ƒ.
5) ë¶„ëŸ‰ ë¶€ì¡±ì´ë©´ êµ¬ì²´ ê·¼ê±°ë¥¼ ë³´ê°•í•˜ê³ , ë¶„ëŸ‰ ì´ˆê³¼ë©´ ì¤‘ë³µì„ ì••ì¶•í•  ê²ƒ.
6) ìµœì¢… ì‘ë‹µì€ ë°˜ë“œì‹œ ì•„ë˜ XML íƒœê·¸ë§Œ ì¶œë ¥í•  ê²ƒ.

[ì£¼ì œ]
{topic}

[ì›ê³  ì›ë¬¸]
<title>{title}</title>
<content>
{content}
</content>

[ì¶œë ¥ í˜•ì‹]
<title>...</title>
<content>...</content>
"""

        try:
            response_text = await generate_content_async(
                prompt,
                model_name=self.model_name,
                temperature=0.0,
                max_output_tokens=4096,
            )
            parsed = self.parse_response(response_text)
            recovered_content = normalize_html_structure_tags(normalize_artifacts(parsed.get('content', '')))
            recovered_title = normalize_artifacts(parsed.get('title', '')) or title
            if not recovered_content:
                return None
            return recovered_content, recovered_title
        except Exception as e:
            print(f"âš ï¸ [StructureAgent] êµ¬ì¡°/ë¶„ëŸ‰ ë³´ê°• ë³µêµ¬ ì‹¤íŒ¨: {str(e)}")
            return None

    async def run_context_analyzer(self, stance_text: str, news_data_text: str, author_name: str) -> Optional[Dict]:
        from ..common.gemini_client import generate_content_async

        # ì…ì¥ë¬¸ì´ ì—†ìœ¼ë©´ ë¶„ì„ ìŠ¤í‚µ
        if len(stance_text) < 50:
            print(f"âš ï¸ [StructureAgent] ì…ì¥ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ ({len(stance_text)}ì) - ContextAnalyzer ìŠ¤í‚µ")
            return None

        print(f'ğŸ” [StructureAgent] ContextAnalyzer ì‹¤í–‰... (ì…ì¥ë¬¸: {len(stance_text)}ì, ë‰´ìŠ¤: {len(news_data_text)}ì)')
        start_time = time.time()

        if not news_data_text:
            # Fallback Logic: ë‰´ìŠ¤ ë°ì´í„° ì—†ì„ ë•Œ - ê·¸ëƒ¥ ì§„í–‰
            # (user_profileì´ ì´ í•¨ìˆ˜ scopeì— ì—†ì–´ì„œ fallback ë¡œì§ ë¹„í™œì„±í™”)
            print(f"âš ï¸ [StructureAgent] ë‰´ìŠ¤ ë°ì´í„° ì—†ìŒ - ì…ì¥ë¬¸ë§Œìœ¼ë¡œ ë¶„ì„ ì§„í–‰")

        context_prompt = f"""ë‹¹ì‹ ì€ ì •ì¹˜ ì½˜í…ì¸  ì „ëµê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì…ì¥ë¬¸ì„ ë¶„ì„í•˜ì—¬ ë¸”ë¡œê·¸ ì½˜í…ì¸  ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

[ì…ì¥ë¬¸ ì›ë¬¸]
{stance_text[:2500]}

[ë‰´ìŠ¤/ë°ì´í„° (ìˆìœ¼ë©´)]
{news_data_text[:2000] if news_data_text else '(ì—†ìŒ)'}

## ë¶„ì„ ê³¼ì œ

### 1. ê¸€ì˜ í•µì‹¬ ì˜ë„ (Intent)
ì•„ë˜ ì¤‘ ê°€ì¥ ì í•©í•œ ê²ƒ í•˜ë‚˜ë¥¼ ì„ íƒ:
- "donation_request": í›„ì› ìš”ì²­ (ê³„ì¢Œ, ì—°ë½ì²˜ í¬í•¨)
- "policy_promotion": ì •ì±…/ë¹„ì „ í™ë³´
- "event_announcement": ì¼ì •/í–‰ì‚¬ ì•ˆë‚´
- "activity_report": í™œë™ ë³´ê³ 
- "personal_message": ê°œì¸ ì†Œí†µ/ì¸ì‚¬

### 2. ì½˜í…ì¸  ì „ëµ (ContentStrategy)
ì´ ê¸€ì„ 2000ì ë¸”ë¡œê·¸ë¡œ í™•ì¥í•  ë•Œ:
- tone: ì–´ë–¤ í†¤ì•¤ë§¤ë„ˆ? ("ê°ì„± í˜¸ì†Œ", "ë…¼ë¦¬ì  ì„¤ë“", "ì •ë³´ ì „ë‹¬", "ì¹œê·¼í•œ ì†Œí†µ")
- structure: ì–´ë–¤ êµ¬ì¡°? ("ìŠ¤í† ë¦¬í…”ë§ â†’ ë¹„ì „ â†’ CTA", "ë¬¸ì œ â†’ í•´ë²• â†’ íš¨ê³¼", "ì¼ì • â†’ ë‚´ìš© â†’ ì°¸ì—¬ë°©ë²•")
- emphasis: ë¬´ì—‡ì„ ê°•ì¡°? (ë¦¬ìŠ¤íŠ¸ë¡œ)

### 3. í•µì‹¬ ì£¼ì¥ ì¶”ì¶œ (MustIncludeFromStance)
ê¸€ì“´ì´({author_name})ì˜ í•µì‹¬ ì£¼ì¥ ìµœëŒ€ 3ê°œ, ê°ê°ì— ëŒ€í•´:
- topic: í•µì‹¬ ì£¼ì¥ (ê°„ê²°í•œ ë¬¸ì¥)
- expansion_why: ì´ ì£¼ì¥ì´ í•„ìš”í•œ ë°°ê²½
- expansion_how: êµ¬ì²´ì  ì‹¤í˜„ ë°©ì•ˆ
- expansion_effect: ê¸°ëŒ€ë˜ëŠ” íš¨ê³¼

### 4. í•„ìˆ˜ ë³´ì¡´ ì •ë³´ (MustPreserve) âš ï¸ ì¤‘ìš”
ì›ë¬¸ì—ì„œ **ì ˆëŒ€ ëˆ„ë½ë˜ë©´ ì•ˆ ë˜ëŠ” êµ¬ì²´ì  ì •ë³´**ë¥¼ ì¶”ì¶œ:
- bankName: ì€í–‰ëª… (ì—†ìœ¼ë©´ null)
- accountNumber: ê³„ì¢Œë²ˆí˜¸ (ì—†ìœ¼ë©´ null)
- accountHolder: ì˜ˆê¸ˆì£¼ (ì—†ìœ¼ë©´ null)
- contactNumber: ì—°ë½ì²˜ (ì—†ìœ¼ë©´ null)
- instruction: ì•ˆë‚´ ë¬¸êµ¬ (ì—†ìœ¼ë©´ null)
- eventDate: ì¼ì‹œ (ì—†ìœ¼ë©´ null)
- eventLocation: ì¥ì†Œ (ì—†ìœ¼ë©´ null)
- ctaPhrase: CTA ë¬¸êµ¬, ì˜ˆ: "í•¨ê»˜í•´ ì£¼ì‹­ì‹œì˜¤" (ì—†ìœ¼ë©´ null)

ë°˜ë“œì‹œ ì•„ë˜ JSON í¬ë§·ìœ¼ë¡œ ì‘ë‹µ:
{{
  "intent": "donation_request",
  "contentStrategy": {{
    "tone": "ê°ì„± í˜¸ì†Œ",
    "structure": "ìŠ¤í† ë¦¬í…”ë§ â†’ ë¹„ì „ â†’ CTA",
    "emphasis": ["í›„ì› ë™ì°¸ ìœ ë„", "ì§„ì •ì„± ì „ë‹¬"]
  }},
  "mustIncludeFromStance": [
    {{
      "topic": "í•µì‹¬ ì£¼ì¥ 1",
      "expansion_why": "ë°°ê²½...",
      "expansion_how": "ë°©ì•ˆ...",
      "expansion_effect": "íš¨ê³¼..."
    }}
  ],
  "mustIncludeFacts": [],
  "mustPreserve": {{
    "bankName": "ì‹ í•œì€í–‰",
    "accountNumber": "140016005619",
    "accountHolder": "ì´ì¬ì„± í›„ì›íšŒ",
    "contactNumber": "01097262663",
    "instruction": "ì…ê¸ˆ í›„ ì„±í•¨ ë¬¸ì â†’ ì˜ìˆ˜ì¦ ë°œê¸‰",
    "eventDate": null,
    "eventLocation": null,
    "ctaPhrase": "ì§€ê¸ˆ ë°”ë¡œ í•¨ê»˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
  }}
}}"""

        try:
            response_text = await generate_content_async(
                context_prompt,
                model_name=self.model_name,
                temperature=0.0,  # ë¶„ì„ì€ ì •í™•ë„ ìš°ì„ 
                response_mime_type='application/json'
            )

            analysis = json.loads(response_text)

            elapsed = time.time() - start_time
            print(f"âœ… [StructureAgent] ContextAnalyzer ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")

            # Filter phrases
            if 'mustIncludeFromStance' in analysis and isinstance(analysis['mustIncludeFromStance'], list):
                filtered_list = []
                for item in analysis['mustIncludeFromStance']:
                    # ê¸°ì¡´ ë¬¸ìì—´ í˜¸í™˜ì„± (stringì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
                    if isinstance(item, str) and len(item.strip()) >= 5:
                         filtered_list.append({'topic': item, 'expansion_why': '', 'expansion_how': '', 'expansion_effect': ''})
                    # ë”•ì…”ë„ˆë¦¬ êµ¬ì¡° í•„í„°ë§
                    elif isinstance(item, dict) and item.get('topic'):
                        topic = item.get('topic', '').strip()
                        if len(topic) >= 2 and not topic.startswith('âš ï¸'):
                            filtered_list.append(item)
                analysis['mustIncludeFromStance'] = filtered_list

            return analysis
        except Exception as e:
            elapsed = time.time() - start_time
            error_msg = str(e)
            print(f"âš ï¸ [StructureAgent] ContextAnalyzer ì‹¤íŒ¨ ({elapsed:.1f}ì´ˆ): {error_msg} - ê±´ë„ˆëœ€")
            return None

    def build_prompt(self, params: Dict[str, Any]) -> str:
        # Extract params
        writing_method = params.get('writingMethod')
        template_builder = TEMPLATE_BUILDERS.get(writing_method, build_daily_communication_prompt)

        # userProfile ë°©ì–´ ì½”ë“œ - listë¡œ ì „ë‹¬ë˜ëŠ” ê²½ìš° ë°©ì–´
        user_profile = params.get('userProfile', {})
        if not isinstance(user_profile, dict):
            user_profile = {}

        # Build base template prompt
        template_prompt = template_builder({
            'topic': params.get('topic'),
            'authorBio': params.get('authorBio'),
            'authorName': params.get('authorName'),
            'instructions': params.get('instructions'),
            'keywords': params.get('userKeywords'),
            'targetWordCount': params.get('targetWordCount'),
            'personalizedHints': params.get('memoryContext'),
            'newsContext': params.get('newsContext'),
            'isCurrentLawmaker': self.is_current_lawmaker(user_profile),
            'politicalExperience': user_profile.get('politicalExperience', 'ì •ì¹˜ ì‹ ì¸'),
            'familyStatus': user_profile.get('familyStatus', '')
        })

        # Reference Materials Section
        instructions_text = normalize_context_text(params.get('instructions'))
        news_context_text = normalize_context_text(params.get('newsContext'))
        source_text = "\n\n---\n\n".join(filter(None, [instructions_text, news_context_text]))
        ref_section = ""
        if source_text.strip():
            ref_section = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ“š [1ì°¨ ìë£Œ] ì°¸ê³ ìë£Œ - ì›ê³ ì˜ í•µì‹¬ ì†ŒìŠ¤                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âš ï¸ **[CRITICAL] ì•„ë˜ ì°¸ê³ ìë£Œê°€ ì´ ì›ê³ ì˜ 1ì°¨ ìë£Œ(Primary Source)ì…ë‹ˆë‹¤.**
- ì²« ë²ˆì§¸ ìë£Œ: ì‘ì„±ìì˜ ì…ì¥ë¬¸/í˜ì´ìŠ¤ë¶ ê¸€ (í•µì‹¬ ë…¼ì¡°ì™€ ì£¼ì¥)
- ì´í›„ ìë£Œ: ë‰´ìŠ¤/ë°ì´í„° (ê·¼ê±°, íŒ©íŠ¸, ë°°ê²½ ì •ë³´)

**[ì°¸ê³ ìë£Œ ì›ë¬¸]**
{source_text[:6000]}

ğŸš¨ **[ìë£Œ ì²˜ë¦¬ ê·œì¹™ - ì¤‘ìš”]**
1. **ì •ë³´ ì¶”ì¶œ**: ì°¸ê³ ìë£Œì—ì„œ í•µì‹¬ íŒ©íŠ¸, ìˆ˜ì¹˜, ë…¼ì ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
2. **ì¬ì‘ì„± í•„ìˆ˜ (CRITICAL)**: ì¶”ì¶œí•œ ì •ë³´ë¥¼ **ë°˜ë“œì‹œ ìƒˆë¡œìš´ ë¬¸ì¥ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±**í•˜ì„¸ìš”. ì°¸ê³ ìë£Œì˜ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ˆì„¸ìš”.
3. **êµ¬ì–´ì²´ â†’ ë¬¸ì–´ì²´ ë³€í™˜**: ì¸í„°ë·°/ëŒ€í™”ì²´ ìë£Œì˜ ê²½ìš°, êµ¬ì–´ì²´ í‘œí˜„("ê·¸ë˜ì„œìš”", "ê±°ì˜ˆìš”", "~í•˜ê±°ë“ ìš”" ë“±)ì„ ë¬¸ì–´ì²´ë¡œ ë³€í™˜í•˜ì„¸ìš”.
4. **ì°½ì‘ ê¸ˆì§€**: ì°¸ê³ ìë£Œì— ì—†ëŠ” íŒ©íŠ¸, ìˆ˜ì¹˜ë¥¼ ì°½ì‘í•˜ì§€ ë§ˆì„¸ìš”.
5. **ì£¼ì œ ìœ ì§€**: ì°¸ê³ ìë£Œì˜ ì£¼ì œë¥¼ ë²—ì–´ë‚˜ì§€ ë§ˆì„¸ìš”.
6. **ë³´ì¡° ìë£Œ**: ì‚¬ìš©ì í”„ë¡œí•„(Bio)ì€ í™”ì ì •ì²´ì„±ê³¼ ì–´ì¡° ì°¸ê³ ìš©ì´ë©°, ë¶„ëŸ‰ì´ ë¶€ì¡±í•  ë•Œë§Œ í™œìš©í•˜ì„¸ìš”.

âŒ **ê¸ˆì§€ ì˜ˆì‹œ**:
- ì°¸ê³ ìë£Œ: "ì •í™•í•˜ê²Œ ì–˜ê¸°ë¥¼ í•˜ë©´ ê·¸ë˜ì„œ ì°½ì˜ì ì´ê³  ì •ë§ ì••ë„ì ì¸..."
- âŒ ì˜ëª»ëœ ì‚¬ìš©: "ì •í™•í•˜ê²Œ ì–˜ê¸°ë¥¼ í•˜ë©´ ê·¸ë˜ì„œ ì°½ì˜ì ì´ê³ ..." (ë³µë¶™)
- âœ… ì˜¬ë°”ë¥¸ ì‚¬ìš©: "ì°½ì˜ì ì´ê³  ì••ë„ì ì¸ ì½˜í…ì¸  ê¸°ë°˜ ì „ëµì´ í•µì‹¬ì…ë‹ˆë‹¤." (ì¬ì‘ì„±)
"""
            print(f"ğŸ“š [StructureAgent] ì°¸ê³ ìë£Œ ì£¼ì… ì™„ë£Œ: {len(source_text)}ì")
        else:
            print("âš ï¸ [StructureAgent] ì°¸ê³ ìë£Œ ì—†ìŒ - ì‚¬ìš©ì í”„ë¡œí•„ë§Œìœ¼ë¡œ ìƒì„±")

        # Context Injection
        context_injection = ""
        context_analysis = params.get('contextAnalysis')
        if context_analysis:
            stance_list = context_analysis.get('mustIncludeFromStance', [])
            
            # êµ¬ì¡°í™”ëœ stance ì²˜ë¦¬
            formatted_stances = []
            for i, p in enumerate(stance_list):
                if isinstance(p, dict):
                    topic = p.get('topic', '')
                    why_txt = p.get('expansion_why', '')
                    how_txt = p.get('expansion_how', '')
                    eff_txt = p.get('expansion_effect', '')
                    
                    block = f"""
{i+1}. **{topic}** (ë³¸ë¡  {i+1} ì£¼ì œ)
   - [Why/ë°°ê²½]: {why_txt}
   - [How/í•´ë²•]: {how_txt}
   - [Effect/íš¨ê³¼]: {eff_txt}"""
                    formatted_stances.append(block.strip())
                else:
                    # Fallback for string (legacy)
                    formatted_stances.append(f"{i+1}. {p}")

            stance_phrases = "\n\n".join(formatted_stances)
            stance_count = len(stance_list)
            
            if stance_count > 0:
                context_injection = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ”´ [MANDATORY] ë³¸ë¡  ì„¹ì…˜ë³„ í™•ì¥ ì„¤ê³„ë„ (Deep Expansion)       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ì•„ë˜ **{stance_count}ê°œ ì„¤ê³„ë„**ì— ë”°ë¼ ê° ë³¸ë¡  ì„¹ì…˜ì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
ë‹¨ìˆœíˆ ì£¼ì œë§Œ ì–¸ê¸‰í•˜ì§€ ë§ê³ , í•¨ê»˜ ì œê³µëœ [Why-How-Effect] ë…¼ë¦¬ë¥¼ ë¬¸ë‹¨ êµ¬ì„±ì— ë°˜ë“œì‹œ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤.

{stance_phrases}

ğŸ“Œ **ì‘ì„± ì§€ì¹¨**:
1. ìœ„ {stance_count}ê°œ ì£¼ì œë¥¼ ê°ê° **ë³„ë„ì˜ ë³¸ë¡  ì„¹ì…˜(H2)**ìœ¼ë¡œ êµ¬ì„±í•˜ì‹­ì‹œì˜¤.
2. ê° ì„¹ì…˜ ì‘ì„± ì‹œ, ìœ„ì—ì„œ ì„¤ê³„ëœ [Why], [How], [Effect]ë¥¼ í•µì‹¬ ìœ„ì£¼ë¡œ ë°˜ì˜í•´ ë…¼ë¦¬ë¥¼ ì™„ì„±í•˜ì‹­ì‹œì˜¤.
3. **[How] ë‹¨ê³„**ì—ì„œ ë‹¹ì‹ ì˜ Bio(ê²½ë ¥)ë¥¼ ê·¼ê±°ë¡œ í™œìš©í•˜ì—¬ ì „ë¬¸ì„±ì„ ë“œëŸ¬ë‚´ì‹­ì‹œì˜¤.
"""

            # ğŸ”´ [NEW] contentStrategy ì£¼ì…
            content_strategy = context_analysis.get('contentStrategy', {})
            if content_strategy:
                tone = content_strategy.get('tone', '')
                structure = content_strategy.get('structure', '')
                emphasis = content_strategy.get('emphasis', [])
                
                if tone or structure:
                    emphasis_str = ", ".join(emphasis) if emphasis else "ì—†ìŒ"
                    context_injection += f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ¯ [STRATEGY] ì½˜í…ì¸  ì „ëµ ê°€ì´ë“œ                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**[í†¤ì•¤ë§¤ë„ˆ]**: {tone}
**[êµ¬ì¡°]**: {structure}
**[ê°•ì¡°ì ]**: {emphasis_str}

ìœ„ ì „ëµì— ë§ì¶° ê¸€ì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
"""
                    print(f"ğŸ¯ [StructureAgent] ì½˜í…ì¸  ì „ëµ ì£¼ì…: {tone} / {structure}")

            # ğŸ”´ [NEW] mustPreserve ê¸°ë°˜ CTA ì •ë³´ ì£¼ì…
            must_preserve = context_analysis.get('mustPreserve', {})
            intent = context_analysis.get('intent', '')
            
            if must_preserve and intent == 'donation_request':
                bank_name = must_preserve.get('bankName')
                account_number = must_preserve.get('accountNumber')
                account_holder = must_preserve.get('accountHolder')
                contact_number = must_preserve.get('contactNumber')
                instruction = must_preserve.get('instruction')
                cta_phrase = must_preserve.get('ctaPhrase')
                
                # ìœ íš¨í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ ì£¼ì…
                if account_number or contact_number:
                    cta_parts = []
                    if bank_name and account_number:
                        cta_parts.append(f"- í›„ì›ê³„ì¢Œ: {bank_name} {account_number}")
                    if account_holder:
                        cta_parts.append(f"- ì˜ˆê¸ˆì£¼: {account_holder}")
                    if contact_number and instruction:
                        cta_parts.append(f"- ì—°ë½ì²˜: {contact_number} ({instruction})")
                    elif contact_number:
                        cta_parts.append(f"- ì—°ë½ì²˜: {contact_number}")
                    if cta_phrase:
                        cta_parts.append(f"- CTA ë¬¸êµ¬: \"{cta_phrase}\"")
                    
                    cta_text = "\n".join(cta_parts)
                    
                    context_injection += f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ’° [CRITICAL] í›„ì› ì •ë³´ - ê²°ë¡ ë¶€ì— ë°˜ë“œì‹œ í¬í•¨               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ì´ ê¸€ì˜ í•µì‹¬ ëª©ì ì€ **í›„ì› ìš”ì²­**ì…ë‹ˆë‹¤. 
ê²°ë¡ ë¶€ì—ì„œ ì•„ë˜ í›„ì› ì •ë³´ë¥¼ **ìì—°ìŠ¤ëŸ½ê²Œ ì•ˆë‚´**í•˜ì‹­ì‹œì˜¤.

**[í›„ì› ì•ˆë‚´ ì •ë³´]**
{cta_text}

ğŸ“Œ **ì‘ì„± ì§€ì¹¨**:
1. ê²°ë¡ ë¶€ì—ì„œ í›„ì› ì°¸ì—¬ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìš”ì²­í•˜ì‹­ì‹œì˜¤.
2. í›„ì› ê³„ì¢Œì™€ ì—°ë½ì²˜ ì •ë³´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨í•˜ì‹­ì‹œì˜¤.
3. CTA ë¬¸êµ¬("{cta_phrase or 'í•¨ê»˜í•´ ì£¼ì‹­ì‹œì˜¤'}")ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
4. í›„ì›ê¸ˆ ì˜ìˆ˜ì¦ ë°œê¸‰ ì•ˆë‚´ê°€ ìˆë‹¤ë©´ ì–¸ê¸‰í•˜ì‹­ì‹œì˜¤.
"""
                    print(f"ğŸ’° [StructureAgent] í›„ì› ì •ë³´ ì£¼ì…: {bank_name} {account_number} / {contact_number}")

            # ğŸ”´ [NEW] í–‰ì‚¬ ì•ˆë‚´ ì •ë³´ ì£¼ì…
            elif must_preserve and intent == 'event_announcement':
                event_date = must_preserve.get('eventDate')
                event_location = must_preserve.get('eventLocation')
                contact_number = must_preserve.get('contactNumber')
                cta_phrase = must_preserve.get('ctaPhrase')
                
                if event_date or event_location:
                    event_parts = []
                    if event_date:
                        event_parts.append(f"- ì¼ì‹œ: {event_date}")
                    if event_location:
                        event_parts.append(f"- ì¥ì†Œ: {event_location}")
                    if contact_number:
                        event_parts.append(f"- ë¬¸ì˜: {contact_number}")
                    
                    event_text = "\n".join(event_parts)
                    
                    context_injection += f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ“… [CRITICAL] í–‰ì‚¬ ì •ë³´ - ë³¸ë¬¸ì— ë°˜ë“œì‹œ í¬í•¨                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**[í–‰ì‚¬ ì•ˆë‚´ ì •ë³´]**
{event_text}

ğŸ“Œ ìœ„ ì •ë³´ë¥¼ ë³¸ë¬¸ê³¼ ê²°ë¡ ë¶€ì— ëª…í™•íˆ í¬í•¨í•˜ì‹­ì‹œì˜¤.
"""
                    print(f"ğŸ“… [StructureAgent] í–‰ì‚¬ ì •ë³´ ì£¼ì…: {event_date} / {event_location}")

        # Warning Generation
        bio_warning = ""
        non_lawmaker_warn = generate_non_lawmaker_warning(
            self.is_current_lawmaker(user_profile),
            user_profile.get('politicalExperience'),
            params.get('authorBio')
        )
        if non_lawmaker_warn:
            bio_warning += non_lawmaker_warn + "\n\n"
        
        if params.get('authorBio') and '"' in params.get('authorBio', ''):
            bio_warning += """
ğŸš¨ [BIO ì¸ìš© ê·œì¹™ - ì ˆëŒ€ ì¤€ìˆ˜]
- Bioì— ìˆëŠ” **í°ë”°ì˜´í‘œ(" ")ë¡œ ë¬¶ì¸ ë¬¸ì¥**ì€ ì‚¬ìš©ìì˜ í•µì‹¬ ì„œì‚¬ì…ë‹ˆë‹¤.
- ì´ ë¬¸ì¥ì€ **í•œ ê¸€ìë„ ìˆ˜ì •í•˜ì§€ ë§ê³  ì›ë¬¸ ê·¸ëŒ€ë¡œ** ì¸ìš©í•˜ì‹­ì‹œì˜¤.
- íŠ¹íˆ AIê°€ ì„ì˜ë¡œ **ì‚¬ëŒ ì´ë¦„ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ëŒ€ì²´**í•˜ëŠ” ê²ƒì€ ì ˆëŒ€ ê¸ˆì§€ì…ë‹ˆë‹¤.
  - âŒ ì˜ëª»ëœ ì˜ˆ: "ë²Œì¨ êµ­íšŒì˜ì› í–ˆì„ í…ë°" â†’ "ë²Œì¨ í™ê¸¸ë™ í–ˆì„ í…ë°"
  - âœ… ì˜¬ë°”ë¥¸ ì˜ˆ: "ë²Œì¨ êµ­íšŒì˜ì› í–ˆì„ í…ë°" (ì›ë¬¸ ê·¸ëŒ€ë¡œ)
"""

        # Modified Structure Enforcement: Dynamic based on stance_count
        stance_count = 0
        if context_analysis:
            stance_count = len(context_analysis.get('mustIncludeFromStance', []))

        length_spec = params.get('lengthSpec') or self._build_length_spec(
            params.get('targetWordCount', 2000),
            stance_count
        )
        body_section_count = length_spec['body_sections']
        total_section_count = length_spec['total_sections']
        min_total_chars = length_spec['min_chars']
        max_total_chars = length_spec['max_chars']
        per_section_min = length_spec['per_section_min']
        per_section_max = length_spec['per_section_max']
        per_section_recommended = length_spec['per_section_recommended']
        
        # ë™ì  ë³¸ë¡  êµ¬ì¡° ë¬¸ìì—´ ìƒì„±
        body_structure_lines = []
        for i in range(1, body_section_count + 1):
            body_structure_lines.append(
                f"{i+1}. ë³¸ë¡  {i} (1ì„¹ì…˜, 2~3ë¬¸ë‹¨, {per_section_min}~{per_section_max}ì) - HTML <h2> ì†Œì œëª© í•„ìˆ˜"
            )
        body_structure_str = "\n".join(body_structure_lines)
        
        # ì§€ì—­ ì •ë³´ ì¶”ì¶œ - ë²”ìš©ì„± í™•ë³´ ë° ë™ì  ë³€ìˆ˜
        region_metro = user_profile.get('regionMetro', '')
        region_district = user_profile.get('regionDistrict', '')
        user_region = f"{region_metro} {region_district}".strip()
        if not user_region:
            user_region = "ì§€ì—­ ì‚¬íšŒ"
            
        structure_enforcement = f"""
<structure_guide mode="strict">
  <strategy>E-A-T (ì „ë¬¸ì„±-ê¶Œìœ„-ì‹ ë¢°) ì „ëµìœ¼ë¡œ ì‘ì„±</strategy>

  <volume warning="ìœ„ë°˜ ì‹œ ì‹œìŠ¤í…œ ì˜¤ë¥˜">
    <per_section min="{per_section_min}" max="{per_section_max}" recommended="{per_section_recommended}"/>
    <paragraphs_per_section>2~3ê°œ ë¬¸ë‹¨, ë¬¸ë‹¨ë‹¹ 2~4ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ìœ„ì£¼ ì„œìˆ </paragraphs_per_section>
    <total sections="{total_section_count}" min="{min_total_chars}" max="{max_total_chars}"/>
    <caution>ì´ ë¶„ëŸ‰ ìƒí•œì„ ë„˜ê¸°ì§€ ì•Šë„ë¡ ì¤‘ë³µ ë¬¸ì¥ê³¼ ì¥í™©í•œ ìˆ˜ì‹ì–´ë¥¼ ì œê±°í•˜ê³ , ê·¼ê±° ì¤‘ì‹¬ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.</caution>
  </volume>

  <expansion_guide name="ì„¹ì…˜ë³„ ì‘ì„± 4ë‹¨ê³„">
    ê° ë³¸ë¡  ì„¹ì…˜ì„ ì•„ë˜ íë¦„ìœ¼ë¡œ ë°€ë„ ìˆê²Œ ì „ê°œí•˜ì‹­ì‹œì˜¤.
    <step name="Why" sentences="1~2">ì‹œë¯¼ë“¤ì´ ê²ªëŠ” ì‹¤ì œ ë¶ˆí¸í•¨ê³¼ í˜„ì¥ì˜ ê³ ì¶©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì§„ë‹¨</step>
    <step name="How+Expertise" sentences="2">ì‹¤í˜„ ê°€ëŠ¥í•œ í•´ê²°ì±… ì œì‹œ ë° ë³¸ì¸ì˜ Bio(ê²½ë ¥)ë¥¼ ì¸ìš©í•˜ì—¬ ì „ë¬¸ì„± ê°•ì¡°</step>
    <step name="Authority" sentences="1">ê³¼ê±° ì„±ê³¼ë‚˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤í–‰ ëŠ¥ë ¥ì„ ì¦ëª…</step>
    <step name="Effect+Trust" sentences="1~2">ë³€í™”ë  {user_region}ì˜ ë¯¸ë˜ ì²­ì‚¬ì§„ì„ ëª…í™•íˆ ì œì‹œ</step>
  </expansion_guide>

  <sections total="{total_section_count}">
    <intro paragraphs="2~3" chars="{per_section_recommended}" heading="ì—†ìŒ">
      <p>1ë¬¸ë‹¨: ì¸ì‚¿ë§ (&lt;p&gt;ì•ˆë…•í•˜ì„¸ìš”, OOOì…ë‹ˆë‹¤.&lt;/p&gt;)</p>
      <p>2ë¬¸ë‹¨: ì£¼ì œ ë„ì… ë° ë°°ê²½ ì„¤ëª…</p>
      <p>3ë¬¸ë‹¨: ê¸€ì˜ ë°©í–¥ì„± ì œì‹œ</p>
    </intro>
    {body_structure_str}
    <conclusion order="{total_section_count}" paragraphs="2~3" chars="{per_section_recommended}" heading="h2 í•„ìˆ˜"/>
  </sections>

  <h2_strategy name="ì†Œì œëª© ì‘ì„± ì „ëµ (AEO+SEO)">
    <type name="ì§ˆë¬¸í˜•" strength="AEO ìµœê°•">ì²­ë…„ ê¸°ë³¸ì†Œë“, ì‹ ì²­ ë°©ë²•ì€?</type>
    <type name="ëª…ì‚¬í˜•" strength="SEO ê¸°ë³¸">ë¶„ë‹¹êµ¬ ì •ìë™ ì£¼ì°¨ì¥ ì‹ ì„¤ ìœ„ì¹˜</type>
    <type name="ë°ì´í„°" strength="ì‹ ë¢°ì„±">2025ë…„ ìƒë°˜ê¸° 5ëŒ€ ì£¼ìš” ì„±ê³¼</type>
    <type name="ì ˆì°¨" strength="ì‹¤ìš©ì„±">ì²­ë…„ ê¸°ë³¸ì†Œë“ ì‹ ì²­ 3ë‹¨ê³„ ì ˆì°¨</type>
    <type name="ë¹„êµ" strength="ì°¨ë³„í™”">ê¸°ì¡´ ì •ì±… ëŒ€ë¹„ ê°œì„ ëœ 3ê°€ì§€</type>
    <banned>ì¶”ìƒì  í‘œí˜„("ë…¸ë ¥", "ì—´ì „", "ë§ˆìŒ"), ëª¨í˜¸í•œ ì œëª©("ì •ì±… ì•ˆë‚´", "ì†Œê°œ"), ì„œìˆ ì–´ í¬í•¨("~ì— ëŒ€í•œ ì„¤ëª…")</banned>
  </h2_strategy>

  <mandatory_rules>
    <rule id="html_tags">ì†Œì œëª©ì€ &lt;h2&gt;, ë¬¸ë‹¨ì€ &lt;p&gt; íƒœê·¸ë§Œ ì‚¬ìš© (ë§ˆí¬ë‹¤ìš´ ** ê¸ˆì§€)</rule>
    <rule id="no_slogan_repeat" severity="critical">ì…ì¥ë¬¸ì˜ ë§ºìŒë§/ìŠ¬ë¡œê±´ì„ ê° ì„¹ì…˜ ëë§ˆë‹¤ ë°˜ë³µ ê¸ˆì§€. ëª¨ë“  í˜¸ì†Œì™€ ë‹¤ì§ì€ ë§¨ ë§ˆì§€ë§‰ ê²°ë¡ ë¶€ì—ë§Œ.</rule>
    <rule id="sentence_completion">ë¬¸ì¥ì€ ì˜¬ë°”ë¥¸ ì¢…ê²° ì–´ë¯¸(~ì…ë‹ˆë‹¤, ~í•©ë‹ˆë‹¤, ~ì‹œì˜¤)ë¡œ ëë‚´ì•¼ í•¨. ê³ ì˜ì  ì˜¤íƒ€/ì˜ë¦° ë¬¸ì¥ ê¸ˆì§€.</rule>
    <rule id="keyword_per_section">ê° ì„¹ì…˜ë§ˆë‹¤ í‚¤ì›Œë“œ 1ê°œ ì´ìƒ í¬í•¨</rule>
    <rule id="separate_pledges">ê° ë³¸ë¡  ì„¹ì…˜ì€ ì„œë¡œ ë‹¤ë¥¸ ì£¼ì œ/ê³µì•½ì„ ë‹¤ë£° ê²ƒ</rule>
    <rule id="verb_diversity" severity="critical">ê°™ì€ ë™ì‚¬(ì˜ˆ: "ë˜ì§€ë©´ì„œ")ë¥¼ ì›ê³  ì „ì²´ì—ì„œ 3íšŒ ì´ìƒ ì‚¬ìš© ê¸ˆì§€. ë™ì˜ì–´ êµì²´: ì œì‹œí•˜ë©°, ì•½ì†í•˜ë©°, ì—´ë©°, ë³´ì—¬ë“œë¦¬ë©° ë“±.</rule>
    <rule id="slogan_once">ìºì¹˜í”„ë ˆì´ì¦ˆ("ì²­ë…„ì´ ëŒì•„ì˜¤ëŠ” ë¶€ì‚°")ë‚˜ ë¹„ìœ ("ì•„ì‹œì•„ì˜ ì‹±ê°€í¬ë¥´")ëŠ” ê²°ë¡ ë¶€ 1íšŒë§Œ. ë‹¤ë¥¸ ì„¹ì…˜ì—ì„œëŠ” ë³€í˜• ì‚¬ìš©.</rule>
    <rule id="natural_keyword">í‚¤ì›Œë“œë§Œìœ¼ë¡œ êµ¬ì„±ëœ ë‹¨ë… ë¬¸ì¥ ê¸ˆì§€. ì• ë¬¸ë‹¨ê³¼ ì—°ê²°ì–´ë¡œ ì´ì–´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë°°ì¹˜.</rule>
    <rule id="causal_clarity">ì„±ê³¼ ì–¸ê¸‰ ì‹œ ë³¸ì¸ì˜ êµ¬ì²´ì  ì—­í• /ì§ì±… ëª…ì‹œ. "40% ë“í‘œìœ¨ì„ ì´ëŒì–´ëƒˆë‹¤" â†’ "ì‹œë‹¹ìœ„ì›ì¥ìœ¼ë¡œì„œ ì§€ì—­ ì¡°ì§ì„ ì´ê´„í•˜ë©° 40% ë“í‘œìœ¨ ë‹¬ì„±ì— ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤"</rule>
  </mandatory_rules>

  <constraints warning="ìœ„ë°˜ ì‹œ ìë™ ë°˜ë ¤">
    <max_chars>{max_total_chars}</max_chars>
    <min_chars>{min_total_chars}</min_chars>
    <no_repeat>ê°™ì€ ë¬¸ì¥, ê°™ì€ í‘œí˜„ ë°˜ë³µ ê¸ˆì§€ (íŠ¹íˆ "~ë°”ëë‹ˆë‹¤" ë°˜ë³µ ê¸ˆì§€)</no_repeat>
    <html>ë¬¸ë‹¨ì€ &lt;p&gt;...&lt;/p&gt;, ì†Œì œëª©ì€ &lt;h2&gt;...&lt;/h2&gt;ë§Œ ì‚¬ìš©</html>
    <separate_pledges>ì„œë¡œ ë‹¤ë¥¸ ê³µì•½/ì •ì±…ì€ í•˜ë‚˜ì˜ ë³¸ë¡ ì— í•©ì¹˜ì§€ ë§ ê²ƒ</separate_pledges>
  </constraints>

  <output_format>í…œí”Œë¦¿ì—ì„œ ì§€ì‹œí•œ XML íƒœê·¸(title, content, hashtags)ë§Œ ì¶œë ¥. output ë˜í¼ë‚˜ ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ê¸ˆì§€.</output_format>
</structure_guide>
"""

        # SEO ì§€ì¹¨ ìƒì„±
        seo_instruction = build_seo_instruction({
            'keywords': params.get('userKeywords', []),
            'targetWordCount': params.get('targetWordCount', 2000)
        })

        # ì„ ê±°ë²• ì¤€ìˆ˜ ì§€ì¹¨ ìƒì„±
        user_status = user_profile.get('status', 'ì¤€ë¹„')
        election_instruction = get_prompt_instruction(user_status)

        return f"""
{template_prompt}

{params.get('partyStanceGuide') or ''}

{seo_instruction}

{election_instruction}

{ref_section}

{context_injection}

{bio_warning}

{structure_enforcement}
""".strip()

    def build_author_bio(self, user_profile: Dict) -> tuple[str, str]:
        # ë°©ì–´ ì½”ë“œ - listë¡œ ì „ë‹¬ë˜ëŠ” ê²½ìš° ë°©ì–´
        if not isinstance(user_profile, dict):
            user_profile = {}

        name = user_profile.get('name', 'ì‚¬ìš©ì')
        party_name = user_profile.get('partyName', '')
        current_title = user_profile.get('customTitle') or user_profile.get('position', '')
        basic_bio = " ".join(filter(None, [party_name, current_title, name]))

        career = user_profile.get('careerSummary') or user_profile.get('bio', '')
        slogan = f'"{user_profile.get("slogan")}"' if user_profile.get('slogan') else ''
        donation = f'[í›„ì› ì•ˆë‚´] {user_profile.get("donationInfo")}' if user_profile.get('donationInfo') else ''

        return f"{basic_bio}\n{career}\n{slogan}\n{donation}".strip(), name

    def is_current_lawmaker(self, user_profile: Dict) -> bool:
        # ë°©ì–´ ì½”ë“œ - listë¡œ ì „ë‹¬ë˜ê±°ë‚˜ Noneì¸ ê²½ìš° ë°©ì–´
        if not user_profile or not isinstance(user_profile, dict):
            return False
        status = user_profile.get('status', '')
        position = user_profile.get('position', '')
        title = user_profile.get('customTitle', '')

        elected_keywords = ['ì˜ì›', 'êµ¬ì²­ì¥', 'êµ°ìˆ˜', 'ì‹œì¥', 'ë„ì§€ì‚¬', 'êµìœ¡ê°']
        text_to_check = status + position + title
        return any(k in text_to_check for k in elected_keywords)

    def parse_response(self, response: str) -> Dict[str, str]:
        if not response:
            return {'content': '', 'title': ''}
        
        # XML Tag Extraction
        try:
            # Title extraction (ì—¬ëŸ¬ ë¸”ë¡ì´ ìˆì„ ë•ŒëŠ” ë§ˆì§€ë§‰ ìœ íš¨ ë¸”ë¡ ìš°ì„ )
            title_blocks = [
                (m.group(1) or "").strip()
                for m in re.finditer(r'<title>(.*?)</title>', response, re.DOTALL | re.IGNORECASE)
            ]
            title = next((t for t in reversed(title_blocks) if t), '')

            # Content extraction
            content_blocks = [
                (m.group(1) or "").strip()
                for m in re.finditer(r'<content>(.*?)</content>', response, re.DOTALL | re.IGNORECASE)
            ]
            content = ''
            if content_blocks:
                # 1) ì˜ˆì‹œ/í…œí”Œë¦¿ ë¸”ë¡ ì œê±° 2) êµ¬ì¡° ë°€ë„ + ê¸¸ì´ë¡œ ë³¸ë¬¸ ë¸”ë¡ ì„ íƒ
                candidates = []
                for idx, block in enumerate(content_blocks):
                    normalized = normalize_html_structure_tags(normalize_artifacts(block))
                    tag_density = len(re.findall(r'<(?:h2|p)\b', normalized, re.IGNORECASE))
                    plain_len = len(strip_html(normalized))
                    candidates.append(
                        {
                            'index': idx,
                            'content': normalized,
                            'tag_density': tag_density,
                            'plain_len': plain_len,
                            'is_example': is_example_like_block(normalized),
                        }
                    )

                real_candidates = [c for c in candidates if not c['is_example']]
                pool = real_candidates if real_candidates else candidates
                selected = max(
                    pool,
                    key=lambda c: (c['tag_density'], c['plain_len'], c['index']),
                )
                content = selected['content'].strip()

                # ì„ íƒëœ content ì¸ë±ìŠ¤ì™€ ê°™ì€ titleì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                selected_idx = selected['index']
                if selected_idx < len(title_blocks):
                    aligned_title = (title_blocks[selected_idx] or '').strip()
                    if aligned_title:
                        title = aligned_title
            
            if not content:
                # Fallback: try to find just HTML tags if XML tags are missing
                print('âš ï¸ [StructureAgent] XML íƒœê·¸ ëˆ„ë½, HTML ì§ì ‘ ì¶”ì¶œ ì‹œë„')
                html_blocks = re.findall(r'<(?:p|h[23])\b[^>]*>[\s\S]*?</(?:p|h[23])>', response, re.IGNORECASE)
                if html_blocks:
                    # ë‹¨ì¼ íƒœê·¸ ì¡°ê° ì—¬ëŸ¬ ê°œë¥¼ í•˜ë‚˜ë¡œ ì´ì–´ ìµœëŒ€í•œ ë³¸ë¬¸ì„ ë³µêµ¬
                    content = "\n".join(html_blocks)
                else:
                    content = response # ìµœí›„ë‹¨: ì „ì²´ í…ìŠ¤íŠ¸
            
            print(f"âœ… [StructureAgent] íŒŒì‹± ì„±ê³µ: content={len(content)}ì")
            return {'content': content, 'title': title}
            
        except Exception as e:
            print(f"âš ï¸ [StructureAgent] íŒŒì‹± ì—ëŸ¬: {str(e)}")
            return {'content': response, 'title': ''}

    def validate_output(self, content: str, target_word_count: Any, stance_count: int = 0) -> Dict:
        if not content:
            return {
                'passed': False,
                'code': 'EMPTY_CONTENT',
                'reason': 'ë‚´ìš© ì—†ìŒ',
                'feedback': 'ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.'
            }
        
        plain_length = len(strip_html(content))

        if isinstance(target_word_count, dict):
            length_spec = target_word_count
        else:
            length_spec = self._build_length_spec(target_word_count, stance_count)

        min_length = length_spec['min_chars']
        max_length = length_spec['max_chars']
        expected_h2 = length_spec['expected_h2']
        per_section_recommended = length_spec['per_section_recommended']
        per_section_max = length_spec['per_section_max']
        total_sections = length_spec['total_sections']

        placeholder_count = len(re.findall(r'\[[^\]\n]{1,40}\]', content))
        if placeholder_count >= 2:
            return {
                'passed': False,
                'code': 'TEMPLATE_ECHO',
                'reason': f"ì˜ˆì‹œ/í”Œë ˆì´ìŠ¤í™€ë” ì”ì¡´ ({placeholder_count}ê°œ)",
                'feedback': 'ì˜ˆì‹œ ë¬¸êµ¬([ì œëª©], [êµ¬ì²´ì  ë‚´ìš©] ë“±)ë¥¼ ëª¨ë‘ ì œê±°í•˜ê³  ì‹¤ì œ ë³¸ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.'
            }

        if plain_length < min_length:
            deficit = min_length - plain_length
            return {
                'passed': False,
                'code': 'LENGTH_SHORT',
                'reason': f"ë¶„ëŸ‰ ë¶€ì¡± ({plain_length}ì < {min_length}ì)",
                'feedback': (
                    f"í˜„ì¬ ë¶„ëŸ‰({plain_length}ì)ì´ ìµœì†Œ ê¸°ì¤€({min_length}ì)ë³´ë‹¤ {deficit}ì ë¶€ì¡±í•©ë‹ˆë‹¤. "
                    f"ì„¹ì…˜ë‹¹ {per_section_recommended}ì ì•ˆíŒìœ¼ë¡œ êµ¬ì²´ ì‚¬ë¡€ë¥¼ ë³´ê°•í•˜ë˜, ì´ ë¶„ëŸ‰ì€ {max_length}ìë¥¼ ë„˜ê¸°ì§€ ë§ˆì‹­ì‹œì˜¤."
                )
            }
        
        if plain_length > max_length:
            excess = plain_length - max_length
            return {
                'passed': False,
                'code': 'LENGTH_LONG',
                'reason': f"ë¶„ëŸ‰ ì´ˆê³¼ ({plain_length}ì > {max_length}ì)",
                'feedback': (
                    f"í˜„ì¬ ë¶„ëŸ‰({plain_length}ì)ì´ ìµœëŒ€ ê¸°ì¤€({max_length}ì)ì„ {excess}ì ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. "
                    f"ê° ì„¹ì…˜ì„ {per_section_recommended}ì ì•ˆíŒ(ìµœëŒ€ {per_section_max}ì)ìœ¼ë¡œ ì••ì¶•í•˜ê³ , "
                    f"ì¤‘ë³µ ë¬¸ì¥ê³¼ ì¥í™©í•œ ìˆ˜ì‹ì–´ë¥¼ ì œê±°í•˜ì‹­ì‹œì˜¤."
                )
            }

        # í—ˆìš© íƒœê·¸ëŠ” h2/pë§Œ ì‚¬ìš©í•´ì•¼ í•˜ë¯€ë¡œ, ê¸°íƒ€ heading íƒœê·¸ëŠ” ì¦‰ì‹œ ë°˜ë ¤.
        disallowed_heading_count = len(re.findall(r'<h(?!2\b)[1-6]\b[^>]*>', content, re.IGNORECASE))
        if disallowed_heading_count > 0:
            return {
                'passed': False,
                'code': 'TAG_DISALLOWED',
                'reason': f"í—ˆìš©ë˜ì§€ ì•Šì€ heading íƒœê·¸ ì‚¬ìš© (h2 ì™¸ {disallowed_heading_count}ê°œ)",
                'feedback': 'ì†Œì œëª©ì€ <h2>ë§Œ ì‚¬ìš©í•˜ê³ , <h1>/<h3> ë“± ë‹¤ë¥¸ heading íƒœê·¸ëŠ” ì œê±°í•˜ì‹­ì‹œì˜¤.'
            }

        h2_open_tags = re.findall(r'<h2\b[^>]*>', content, re.IGNORECASE)
        h2_close_tags = re.findall(r'</h2\s*>', content, re.IGNORECASE)
        h2_count = len(h2_open_tags)
        if h2_count != len(h2_close_tags):
            return {
                'passed': False,
                'code': 'H2_MALFORMED',
                'reason': f"h2 íƒœê·¸ ì§ ë¶ˆì¼ì¹˜ (ì—´ë¦¼ {h2_count}ê°œ, ë‹«í˜ {len(h2_close_tags)}ê°œ)",
                'feedback': 'ëª¨ë“  ì†Œì œëª©ì€ <h2>...</h2> í˜•íƒœë¡œ ì •í™•íˆ ë‹«ì•„ ì£¼ì‹­ì‹œì˜¤.'
            }

        if h2_count < expected_h2:
            return {
                'passed': False,
                'code': 'H2_SHORT',
                'reason': f"ì†Œì œëª© ë¶€ì¡± (í˜„ì¬ {h2_count}ê°œ, ëª©í‘œ {expected_h2}ê°œ)",
                'feedback': (
                    f"ì†Œì œëª©(<h2>)ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ë„ì…ì„ ì œì™¸í•œ ë³¸ë¡ +ê²°ë¡  ê¸°ì¤€ìœ¼ë¡œ ì •í™•íˆ {expected_h2}ê°œì˜ "
                    f"<h2>ë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤."
                )
            }
        
        if h2_count > expected_h2:
            return {
                'passed': False,
                'code': 'H2_LONG',
                'reason': f"ì†Œì œëª© ê³¼ë‹¤ (í˜„ì¬ {h2_count}ê°œ, ëª©í‘œ {expected_h2}ê°œ)",
                'feedback': (
                    f"ì†Œì œëª©(<h2>)ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤. ë„ì…ì„ ì œì™¸í•œ ë³¸ë¡ +ê²°ë¡  ì†Œì œëª© ìˆ˜ë¥¼ ì •í™•íˆ {expected_h2}ê°œë¡œ "
                    f"ë§ì¶”ì‹­ì‹œì˜¤."
                )
            }

        p_open_tags = re.findall(r'<p\b[^>]*>', content, re.IGNORECASE)
        p_close_tags = re.findall(r'</p\s*>', content, re.IGNORECASE)
        p_count = len(p_open_tags)
        if p_count != len(p_close_tags):
            return {
                'passed': False,
                'code': 'P_MALFORMED',
                'reason': f"p íƒœê·¸ ì§ ë¶ˆì¼ì¹˜ (ì—´ë¦¼ {p_count}ê°œ, ë‹«í˜ {len(p_close_tags)}ê°œ)",
                'feedback': 'ëª¨ë“  ë¬¸ë‹¨ì€ <p>...</p> í˜•íƒœë¡œ ì •í™•íˆ ë‹«ì•„ ì£¼ì‹­ì‹œì˜¤.'
            }

        expected_min_p = total_sections * 2
        expected_max_p = total_sections * 4
        
        if p_count < expected_min_p:
             return {
                'passed': False,
                'code': 'P_SHORT',
                'reason': f"ë¬¸ë‹¨ ìˆ˜ ë¶€ì¡± (í˜„ì¬ {p_count}ê°œ, í•„ìš” {expected_min_p}ê°œ ì´ìƒ)",
                'feedback': (
                    f"ë¬¸ë‹¨ ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ì´ {total_sections}ê°œ ì„¹ì…˜ ê¸°ì¤€ìœ¼ë¡œ ìµœì†Œ {expected_min_p}ê°œ "
                    f"ë¬¸ë‹¨(ì„¹ì…˜ë‹¹ 2ê°œ ì´ìƒ)ì´ í•„ìš”í•©ë‹ˆë‹¤."
                )
            }

        if p_count > expected_max_p:
            return {
                'passed': False,
                'code': 'P_LONG',
                'reason': f"ë¬¸ë‹¨ ìˆ˜ ê³¼ë‹¤ (í˜„ì¬ {p_count}ê°œ, ìµœëŒ€ {expected_max_p}ê°œ)",
                'feedback': (
                    f"ë¬¸ë‹¨ ìˆ˜ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤. ì´ {total_sections}ê°œ ì„¹ì…˜ ê¸°ì¤€ìœ¼ë¡œ {expected_max_p}ê°œ ì´í•˜ë¡œ ì¤„ì´ê³ , "
                    f"ë¬¸ë‹¨ì„ í•©ì³ ì¤‘ë³µ ì„¤ëª…ì„ ì••ì¶•í•˜ì‹­ì‹œì˜¤."
                )
            }

        return {'passed': True}
