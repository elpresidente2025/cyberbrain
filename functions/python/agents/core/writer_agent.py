import json
import logging
import re
from typing import Dict, Any, List, Optional
import time

# Local Imports
from ..templates.daily_communication import build_daily_communication_prompt
from ..templates.policy_proposal import build_policy_proposal_prompt
from ..templates.activity_report import build_activity_report_prompt
from ..templates.current_affairs import build_critical_writing_prompt, build_diagnosis_writing_prompt
from ..templates.local_issues import build_local_issues_prompt

from ..common.theminjoo import get_party_stance
from ..common.election_rules import get_election_stage
from ..common.style_analyzer import extract_style_from_text
from ..common.warnings import generate_non_lawmaker_warning, generate_family_status_warning
from ..common.constants import resolve_writing_method
from ..common.xml_builder import (
    build_context_analysis_section,
    build_scope_warning_section,
    build_tone_warning_section,
    build_style_guide_section,
    build_writing_rules_section,
    build_reference_section,
    build_sandwich_reminder_section,
    build_output_protocol_section,
    build_retry_section
)
from ..common.xml_parser import parse_ai_response

logger = logging.getLogger(__name__)

# Template Builders Mapping
TEMPLATE_BUILDERS = {
    'emotional_writing': build_daily_communication_prompt,
    'logical_writing': build_policy_proposal_prompt,
    'direct_writing': build_activity_report_prompt,
    'critical_writing': build_critical_writing_prompt,
    'diagnostic_writing': build_diagnosis_writing_prompt,
    'analytical_writing': build_local_issues_prompt
}

class WriterAgent:
    def __init__(self):
        from ..common.gemini_client import get_client, DEFAULT_MODEL
        self.model_name = DEFAULT_MODEL
        self._client = get_client()

    def get_required_context(self) -> List[str]:
        return ['topic', 'category', 'userProfile']

    async def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        # Unpack context
        topic = context.get('topic')
        category = context.get('category')
        sub_category = context.get('subCategory', '')
        user_profile = context.get('userProfile', {})
        memory_context = context.get('memoryContext', '')
        instructions = context.get('instructions', '')
        news_context = context.get('newsContext', '')
        target_word_count = context.get('targetWordCount', 2000)
        user_keywords = context.get('userKeywords', [])
        previous_results = context.get('previousResults', {})
        
        if not self._client:
            raise ValueError("Gemini API Key missing")

        logger.info(f"ğŸ” [WriterAgent] Input: instructions_len={len(instructions)}, news_len={len(news_context)}")

        # 1. Keyword Result (from Context)
        keyword_result = previous_results.get('KeywordAgent')
        context_keywords = []
        if keyword_result and 'data' in keyword_result:
            context_keywords = keyword_result['data'].get('keywords', [])
        elif kwargs_context_keywords := context.get('contextKeywords'): # Fallback if passed directly
             context_keywords = kwargs_context_keywords
             
        # Extract keyword strings
        context_keyword_strings = []
        for k in context_keywords[:5]:
             if isinstance(k, dict):
                 context_keyword_strings.append(k.get('keyword', ''))
             else:
                 context_keyword_strings.append(str(k))
        
        # 2. Style Analysis
        style_prompt = ''
        style_profile = user_profile.get('styleProfile')
        if not style_profile and user_profile.get('bio'):
            try:
                logger.info("â„¹ï¸ [WriterAgent] Doing real-time style analysis")
                style_profile = await extract_style_from_text(user_profile['bio'])
            except Exception as e:
                logger.warning(f"âŒ Style analysis failed: {e}")
        
        if style_profile:
            metrics = style_profile.get('metrics', {})
            sentence_len = metrics.get('sentence_length', {})
            ending_patterns = metrics.get('ending_patterns', {})
            
            style_prompt = f"""
- **ì–´ì¡° ë° íƒœë„**: {style_profile.get('tone_manner', 'ì •ë³´ ì—†ìŒ')} (ê¸°ê³„ì ì¸ ë¬¸ì²´ê°€ ì•„ë‹Œ, ì‘ì„±ìì˜ ê³ ìœ í•œ í†¤ì„ ëª¨ë°©í•˜ì‹­ì‹œì˜¤.)
- **ì‹œê·¸ë‹ˆì²˜ í‚¤ì›Œë“œ**: [{', '.join(style_profile.get('signature_keywords', []))}] - ì´ ë‹¨ì–´ë“¤ì„ ì ì¬ì ì†Œì— ì‚¬ìš©í•˜ì—¬ ì‘ì„±ìì˜ ì •ì²´ì„±ì„ ë“œëŸ¬ë‚´ì‹­ì‹œì˜¤.
- **ë¬¸ì¥ í˜¸í¡**: í‰ê·  {sentence_len.get('avg', 40)}ì ë‚´ì™¸ì˜ {sentence_len.get('distinct', 'ë¬¸ì¥')} ì‚¬ìš©.
- **ì¢…ê²° ì–´ë¯¸**: ì£¼ë¡œ {', '.join(ending_patterns.get('ratios', {}).keys())} ì‚¬ìš©.
- **ê¸ˆì§€ ë¬¸ì²´**: {style_profile.get('forbidden_style', 'ì–´ìƒ‰í•œ ë²ˆì—­íˆ¬')} ì‚¬ìš© ê¸ˆì§€.
"""

        # 3. Writing Method
        writing_method = resolve_writing_method(category, sub_category)
        
        # 4. Author Bio
        author_bio = self.build_author_bio(user_profile)
        author_name = user_profile.get('name', '')
        
        # 5. Template Builder
        template_builder = TEMPLATE_BUILDERS.get(writing_method, build_daily_communication_prompt)
        
        prompt_kwargs = {
            'topic': topic,
            'authorBio': author_bio,
            'authorName': author_name,
            'instructions': instructions,
            'keywords': context_keyword_strings,
            'targetWordCount': target_word_count,
            'personalizedHints': memory_context,
            'newsContext': news_context,
            'isCurrentLawmaker': self.is_current_lawmaker(user_profile),
            'politicalExperience': user_profile.get('politicalExperience', 'ì •ì¹˜ ì‹ ì¸'),
            'familyStatus': user_profile.get('familyStatus', ''),
            'negativePersona': previous_results.get('WriterAgent', {}).get('_opponentName') or context.get('_opponentName'),
            # For specific templates
            'validTargetCount': target_word_count, # Alias
            'relevantExample': news_context if writing_method == 'critical_writing' else '', # Hack for critical writing arg
        }
        
        # Check template signature to pass supported args only? 
        # Python keyword args handle this gracefully if we pass **kwargs, but we need to match function signatures.
        # My python port uses specific named arguments. 
        # I'll pass relevant ones. The templates I ported accept specific args.
        # I will build a common dict and let them unpack or handle it.
        # Actually in Python `build_X(options)` where options is a dict/object is how I ported it?
        # Checking `daily_communication.py`: `def build_daily_communication_prompt(options: Dict[str, Any]) -> str:`
        # Yes, they all take `options` dict.
        
        prompt = template_builder(prompt_kwargs)
        
        # 5.5 Party Stance
        party_stance_guide = None
        try:
            party_stance_guide = await get_party_stance(topic)
        except Exception as e:
            logger.warning(f"âš ï¸ Party stance lookup failed: {e}")
            
        # 6. Assemble Prompt Sections
        prompt_sections = []
        must_include_for_sandwich = ''
        
        # 6.1 Context Analyzer
        use_context_analyzer = True
        context_analysis = {}
        
        if (instructions or news_context) and use_context_analyzer:
            source_text = '\n'.join(filter(None, [instructions, news_context]))
            if len(source_text) >= 100:
                try:
                    logger.info('ğŸ” [WriterAgent] ContextAnalyzer start...')
                    context_analysis = await self.run_context_analyzer(source_text, author_name)
                    
                    if context_analysis and (context_analysis.get('mainEvent') or context_analysis.get('authorStance')):
                         # Process results
                         facts = context_analysis.get('mustIncludeFacts') or []
                         must_include_text = '\n'.join([f"{i+1}. {f}" for i, f in enumerate(facts)])
                         
                         raw_stance = context_analysis.get('mustIncludeFromStance') or []
                         # Filter logic from JS
                         filtered_stance = [
                             p for p in raw_stance 
                             if isinstance(p, str) and len(p.strip()) >= 5 and 
                             not p.strip().startswith(('âš ï¸', 'ìš°ì„ ìˆœìœ„:', 'ì˜ˆì‹œ íŒ¨í„´:', 'â†’ ì‹¤ì œ'))
                         ]
                         
                         must_include_stance_text = '\n'.join([f'{i+1}. "{f}"' for i, f in enumerate(filtered_stance)])
                         
                         # Save to context for validaiton
                         context['_extractedKeyPhrases'] = filtered_stance
                         context['_responsibilityTarget'] = context_analysis.get('responsibilityTarget')
                         context['_expectedTone'] = context_analysis.get('expectedTone')
                         context['_opponentName'] = context_analysis.get('opponentName')
                         
                         news_quotes = context_analysis.get('newsQuotes') or []
                         news_quotes_text = '\n'.join([f"{i+1}. {q}" for i, q in enumerate(news_quotes)])
                         
                         must_include_for_sandwich = f"""[âœ… ì…ì¥ë¬¸ í•µì‹¬ ë¬¸êµ¬]
{must_include_stance_text}

[âœ… ë‰´ìŠ¤ í•µì‹¬ íŒ©íŠ¸]
{must_include_text}

[âœ… ë‰´ìŠ¤ ì£¼ìš” ë°œì–¸]
{news_quotes_text}""".strip()

                         prompt_sections.append(build_context_analysis_section(context_analysis, author_name))
                         
                         scope_xml = build_scope_warning_section(context_analysis)
                         if scope_xml: prompt_sections.append(scope_xml)
                         
                         tone_xml = build_tone_warning_section(context_analysis)
                         if tone_xml: prompt_sections.append(tone_xml)
                         
                         logger.info("âœ… [WriterAgent] ContextAnalyzer success")
                    else:
                         logger.warning("âš ï¸ [WriterAgent] ContextAnalyzer returned insufficient data")
                except Exception as e:
                    logger.error(f"âŒ [WriterAgent] ContextAnalyzer error: {e}")
        
        # 6.7 Warnings
        warnings = self.build_warnings(user_profile, author_bio)
        if warnings:
            prompt_sections.append(warnings)
            
        if party_stance_guide:
            prompt_sections.append(party_stance_guide)
            
        # 6.7.6 References & Guides
        if instructions or news_context:
            prompt_sections.append(build_reference_section(instructions, news_context))
            prompt_sections.append(build_style_guide_section(style_prompt, author_name, target_word_count))
            prompt_sections.append(build_writing_rules_section(author_name, target_word_count))
            
        # Main Prompt
        prompt_sections.append(prompt)
        
        # Sandwich
        if must_include_for_sandwich:
             sandwich_xml = build_sandwich_reminder_section(must_include_for_sandwich)
             if sandwich_xml: prompt_sections.append(sandwich_xml)
             
        # Identity Lock
        identity_lock = f"""
<identity-lock priority="override">
  <status>FINAL_CHECK</status>
  <who-am-i>{author_name} ({user_profile.get('position', 'ì •ì¹˜ì¸')})</who-am-i>
  <who-is-opponent>{context.get('_opponentName') or 'ì°¸ê³ ìë£Œì˜ ì¸ë¬¼ë“¤'}</who-is-opponent>
  <instruction>
    ì§ì „ì˜ ì°¸ê³ ìë£Œì— ëª°ì…í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ë‹¹ì‹ ì€ ë¹„íŒí•˜ëŠ” ì£¼ì²´ì´ì§€, ë¹„íŒ ëŒ€ìƒì´ ì•„ë‹™ë‹ˆë‹¤.
    ë°˜ë“œì‹œ "ì €ëŠ” {author_name}ì…ë‹ˆë‹¤"ë¼ëŠ” ìê°ì„ ìœ ì§€í•˜ë©° ê¸€ì„ ë§ˆë¬´ë¦¬í•˜ì‹­ì‹œì˜¤.
  </instruction>
</identity-lock>
"""
        prompt_sections.append(identity_lock)
        
        # Protocol Override
        prompt_sections.append(build_output_protocol_section())
        
        final_prompt = '\n\n'.join(prompt_sections)
        logger.info(f"ğŸ“ [WriterAgent] Prompt generated ({len(final_prompt)} chars)")
        
        # Execute Generation Loop
        from ..common.gemini_client import generate_content_async
        min_char_count = max(1200, int(target_word_count * 0.85))
        max_attempts = 3
        attempt_count = 0
        content = None
        title = None
        last_response_text = ''

        while attempt_count < max_attempts:
            attempt_count += 1
            is_retry = attempt_count > 1

            current_prompt_text = final_prompt
            if is_retry:
                missing_keywords = [k for k in user_keywords if content and k not in content]
                current_len = len(content.replace('<[^>]*>', '')) if content else 0

                retry_xml = build_retry_section(
                    attempt_count, max_attempts, current_len, min_char_count, missing_keywords
                )
                current_prompt_text = retry_xml + '\n\n' + final_prompt

            try:
                temperature = 0.45 if is_retry else 0.4
                print(f"ğŸ”„ [WriterAgent] Attempt {attempt_count}/{max_attempts}")

                last_response_text = await generate_content_async(
                    current_prompt_text,
                    model_name=self.model_name,
                    temperature=temperature,
                    max_output_tokens=8192
                )

                parsed = parse_ai_response(last_response_text, f"{topic} ê´€ë ¨")
                content = parsed.get('content') or ''
                title = parsed.get('title') or f"{topic} ê´€ë ¨"

                char_count = len(content.replace('<[^>]*>', ''))
                print(f"ğŸ“Š [WriterAgent] Result len: {char_count}")

                if char_count >= min_char_count:
                    print("âœ… [WriterAgent] Length requirement met")
                    break
                else:
                    print("âš ï¸ [WriterAgent] Length insufficient")

            except Exception as e:
                logger.error(f"âŒ [WriterAgent] Attempt {attempt_count} error: {e}")
                
        if not content and last_response_text:
             logger.warning("âš ï¸ [WriterAgent] Final fallback parsing")
             fallback = parse_ai_response(last_response_text, f"{topic} ê´€ë ¨")
             content = fallback.get('content') or f"<p>{topic}ì— ëŒ€í•œ ì›ê³ ì…ë‹ˆë‹¤.</p>"
             title = fallback.get('title')
             
        final_char_count_val = len(content.replace('<[^>]*>', '')) if content else 0
        
        if final_char_count_val < min_char_count:
             # Warning only in Python version? JS throws error.
             # I'll log warning but return what we have to be safe.
             logger.error(f"WriterAgent length insufficient: {final_char_count_val}/{min_char_count}")
        
        return {
            'content': content,
            'title': title,
            'wordCount': final_char_count_val,
            'writingMethod': writing_method,
            'contextKeywords': context_keyword_strings,
            'searchTerms': user_keywords,
            'appliedStrategy': {'id': None, 'name': 'default'},
            'extractedKeyPhrases': context.get('_extractedKeyPhrases', [])
        }

    async def run_context_analyzer(self, source_text: str, author_name: str) -> Dict[str, Any]:
        from ..common.gemini_client import generate_content_async

        prompt = f"""ë‹¹ì‹ ì€ ì •ì¹˜ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì°¸ê³ ìë£Œë¥¼ ì½ê³  ìƒí™©ì„ ì •í™•íˆ íŒŒì•…í•˜ì„¸ìš”.

âš ï¸ **[ì¤‘ìš”] ì°¸ê³ ìë£Œ êµ¬ì¡° ì•ˆë‚´**:
- **ì²« ë²ˆì§¸ ìë£Œ**: ê¸€ ì‘ì„±ì({author_name or 'í™”ì'})ê°€ ì§ì ‘ ì‘ì„±í•œ **í˜ì´ìŠ¤ë¶ ê¸€ ë˜ëŠ” ì…ì¥ë¬¸**ì…ë‹ˆë‹¤. ì´ê²ƒì´ ê¸€ì˜ í•µì‹¬ ë…¼ì¡°ì™€ ì£¼ì¥ì…ë‹ˆë‹¤.
- **ë‘ ë²ˆì§¸ ì´í›„ ìë£Œ**: ë‰´ìŠ¤ ê¸°ì‚¬, ë°ì´í„° ë“± **ë°°ê²½ ì •ë³´ì™€ ê·¼ê±° ìë£Œ**ì…ë‹ˆë‹¤.

ë”°ë¼ì„œ:
1. ì²« ë²ˆì§¸ ìë£Œì—ì„œ **ê¸€ì“´ì´({author_name or 'í™”ì'})ì˜ ì…ì¥ê³¼ ë…¼ì¡°**ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
2. ë‘ ë²ˆì§¸ ì´í›„ì—ì„œ **ì‚¬ì‹¤ê´€ê³„, ì¸ìš©í•  ë°œì–¸, ë²•ì•ˆëª… ë“± íŒ©íŠ¸**ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
3. ê¸€ì“´ì´ëŠ” ì²« ë²ˆì§¸ ìë£Œì˜ ì…ì¥ì„ **ë” ì •êµí•˜ê³  í’ë¶€í•˜ê²Œ í™•ì¥**í•˜ëŠ” ê¸€ì„ ì›í•©ë‹ˆë‹¤.

[ì°¸ê³ ìë£Œ]
{source_text[:4000]}

[ê¸€ ì‘ì„±ì ì´ë¦„]
{author_name or '(ë¯¸ìƒ)'}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš” (ê° í•„ë“œëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±):
{{
  "issueScope": "ì´ìŠˆì˜ ë²”ìœ„ íŒë‹¨: 'CENTRAL_ISSUE' (ì¤‘ì•™ ì •ì¹˜/êµ­ê°€ ì´ìŠˆ), 'LOCAL_ISSUE' (ì§€ì—­ í˜„ì•ˆ), 'CENTRAL_ISSUE_WITH_LOCAL_IMPACT' (ì¤‘ì•™ ì´ìŠˆì´ë‚˜ ì§€ì—­ ì¸ì‚¬ê°€ ì—°ë£¨ë¨) ì¤‘ íƒ1",
  "localConflictPoint": "ì§€ì—­ì  ìŸì  ìš”ì•½ (ì˜ˆ: 'ë°•í˜•ì¤€ ì‹œì¥ì˜ ì‹ ê³µì•ˆ í†µì¹˜ ë°œì–¸ ë…¼ë€'). ì¤‘ì•™ ì´ìŠˆì¼ ê²½ìš° 'ì—†ìŒ'",
  "responsibilityTarget": "ë¹„íŒì´ë‚˜ ìš”êµ¬ì˜ ëŒ€ìƒì´ ë˜ëŠ” í•µì‹¬ ì£¼ì²´/ê¸°ê´€ (ì˜ˆ: 'ëŒ€í†µë ¹ì‹¤', 'êµ­íšŒ', 'ë¶€ì‚°ì‹œì¥', 'ì‹œì˜íšŒ'). í–‰ì •ì  ì±…ì„ ì£¼ì²´ë¥¼ ëª…í™•íˆ í•  ê²ƒ",
  "writingFrame": "ì´ ê¸€ì´ ì§€í–¥í•´ì•¼ í•  í•µì‹¬ ë…¼ë¦¬ í”„ë ˆì„ 1ì¤„ ìš”ì•½",
  "authorStance": "ì²« ë²ˆì§¸ ìë£Œ(ì…ì¥ë¬¸)ì—ì„œ ì¶”ì¶œí•œ ê¸€ì“´ì´ì˜ í•µì‹¬ ì£¼ì¥ 1ì¤„ ìš”ì•½",
  "mainEvent": "ë‘ ë²ˆì§¸ ì´í›„ ìë£Œ(ë‰´ìŠ¤)ì—ì„œ ì¶”ì¶œí•œ í•µì‹¬ ì‚¬ê±´ 1ì¤„ ìš”ì•½",
  "keyPlayers": [
    {{ "name": "ì¸ë¬¼ëª…", "action": "ì´ ì‚¬ëŒì´ í•œ í–‰ë™/ì£¼ì¥", "stance": "ì°¬ì„±/ë°˜ëŒ€/ì¤‘ë¦½" }}
  ],
  "authorRole": "ê¸€ ì‘ì„±ì({author_name or 'í™”ì'})ê°€ ì´ ìƒí™©ì—ì„œ ì·¨í•´ì•¼ í•  ì…ì¥ê³¼ ì—­í• ",
  "expectedTone": "ì´ ê¸€ì˜ ì˜ˆìƒ ë…¼ì¡° (ë°˜ë°•/ì§€ì§€/ë¶„ì„/ë¹„íŒ/í˜¸ì†Œ ì¤‘ íƒ1)",
  "opponentName": "ê¸€ì“´ì´ê°€ ë¹„íŒí•˜ê±°ë‚˜ ëŒ€ë¦½ê°ì„ ì„¸ìš°ëŠ” ëŒ€ìƒì˜ ì´ë¦„. ì—†ë‹¤ë©´ null",
  "mustIncludeFacts": ["ë‰´ìŠ¤ì—ì„œ ì¶”ì¶œí•œ ë°˜ë“œì‹œ ì–¸ê¸‰í•´ì•¼ í•  êµ¬ì²´ì  íŒ©íŠ¸ 5ê°œ"],
  "newsQuotes": ["ë‰´ìŠ¤ì— ë“±ì¥í•˜ëŠ” í•µì‹¬ ì¸ë¬¼ë“¤ì˜ ë°œì–¸ì„ 'ì°¸ê³ ìš©'ìœ¼ë¡œ ì¶”ì¶œ (3ê°œ ì´ìƒ)"],
  "mustIncludeFromStance": ["ì…ì¥ë¬¸ì˜ í•µì‹¬ ê³µì•½/ì •ì±…/ì£¼ì¥ (15ì ì´ìƒ ì™„ì „í•œ ë¬¸ì¥, ìµœëŒ€ 5ê°œ)"],
  "contextWarning": "ë§¥ë½ ì˜¤í•´ ë°©ì§€ë¥¼ ìœ„í•œ ì£¼ì˜ì‚¬í•­"
}}"""

        response_text = await generate_content_async(
            prompt,
            model_name=self.model_name,
            temperature=0.1,
            max_output_tokens=2000,
            response_mime_type='application/json'
        )

        try:
            return json.loads(response_text)
        except:
            # Fallback cleanup
            text = re.sub(r'```json|```', '', response_text).strip()
            return json.loads(text)

    def build_author_bio(self, user_profile: Dict[str, Any]) -> str:
        name = user_profile.get('name', 'ì‚¬ìš©ì')
        party_name = user_profile.get('partyName', '')
        current_title = user_profile.get('customTitle') or user_profile.get('position', '')
        
        basic_bio = ' '.join(filter(None, [party_name, current_title, name]))
        
        additional_info = []
        career = user_profile.get('careerSummary') or user_profile.get('bio', '')
        if career:
            if isinstance(career, list):
                additional_info.append(f"[ì£¼ìš” ê²½ë ¥] {', '.join(career[:3])}")
            else:
                trunc = career[:150] + '...' if len(career) > 150 else career
                additional_info.append(f"[ì£¼ìš” ê²½ë ¥] {trunc}")
                
        if user_profile.get('slogan'):
            additional_info.append(f"[ìŠ¬ë¡œê±´] \"{user_profile['slogan']}\"")

        if user_profile.get('donationInfo'):
            additional_info.append(f"[í›„ì› ì•ˆë‚´] \"{user_profile['donationInfo']}\"")

        if user_profile.get('coreValues'):
            vals = user_profile['coreValues']
            if isinstance(vals, list):
                vals = ', '.join(vals)
            additional_info.append(f"[í•µì‹¬ ê°€ì¹˜] {vals}")
            
        if additional_info:
            return f"{basic_bio}\n" + "\n".join(additional_info)
            
        return basic_bio

    def is_current_lawmaker(self, user_profile: Dict[str, Any]) -> bool:
        experience = user_profile.get('politicalExperience', '')
        return experience in ['ì´ˆì„ ', 'ì¬ì„ ', '3ì„ ì´ìƒ']

    def build_warnings(self, user_profile: Dict[str, Any], author_bio: str) -> str:
        warnings = []
        
        nm_warning = generate_non_lawmaker_warning({
            'isCurrentLawmaker': self.is_current_lawmaker(user_profile),
            'politicalExperience': user_profile.get('politicalExperience'),
            'authorBio': author_bio
        })
        if nm_warning: warnings.append(nm_warning.strip())
        
        fam_warning = generate_family_status_warning({
            'familyStatus': user_profile.get('familyStatus')
        })
        if fam_warning: warnings.append(fam_warning.strip())
        
        warnings.append("""ğŸš¨ [CRITICAL] ì‚¬ì‹¤ ê´€ê³„ ì™œê³¡ ê¸ˆì§€ (ë³¸ì¸ vs ê°€ì¡± êµ¬ë¶„):
- ì‘ì„±ì í”„ë¡œí•„(Bio)ì— ì–¸ê¸‰ëœ "ê°€ì¡±ì˜ ì§ì—…/ì´ë ¥"ì„ "ë‚˜(í™”ì)ì˜ ì§ì—…/ì´ë ¥"ìœ¼ë¡œ ì“°ì§€ ë§ˆì‹­ì‹œì˜¤.
- ì˜ˆ: "ì•„ë²„ì§€ê°€ ë¶€ë‘ ë…¸ë™ì" -> "ì €ëŠ” ë¶€ë‘ ë…¸ë™ì ì¶œì‹ ì…ë‹ˆë‹¤" (âŒ ì ˆëŒ€ ê¸ˆì§€: ì•„ë²„ì§€ê°€ ë…¸ë™ìì´ì§€ ë‚´ê°€ ì•„ë‹˜)
- ì˜ˆ: "ì•„ë²„ì§€ê°€ ë¶€ë‘ ë…¸ë™ì" -> "ë¶€ë‘ ë…¸ë™ìì˜€ë˜ ì•„ë²„ì§€ì˜ ë“±ì„ ë³´ë©° ìëìŠµë‹ˆë‹¤" (âœ… ì˜¬ë°”ë¥¸ í‘œí˜„)""")
        
        target_election = user_profile.get('targetElection', {})
        position = target_election.get('position') or user_profile.get('position', '')
        is_metro = any(x in position for x in ['ì‹œì¥', 'ë„ì§€ì‚¬', 'êµìœ¡ê°'])
        is_gugun = any(x in position for x in ['êµ¬ì²­ì¥', 'êµ°ìˆ˜', 'ê¸°ì´ˆì˜ì›'])
        if is_metro and not is_gugun:
             warnings.append(f"""ğŸš¨ [CRITICAL] ì§€ì—­ ë²”ìœ„ ì„¤ì • (ê´‘ì—­ ìì¹˜ë‹¨ì²´ì¥ê¸‰):
- ë‹¹ì‹ ì€ ì§€ê¸ˆ ê¸°ì´ˆì§€ìì²´(êµ¬/êµ°)ê°€ ì•„ë‹Œ **"ê´‘ì—­ ìì¹˜ë‹¨ì²´({user_profile.get('regionMetro', 'ì‹œ/ë„')} ì „ì²´"**ë¥¼ ëŒ€í‘œí•˜ëŠ” í›„ë³´ìì…ë‹ˆë‹¤.
- íŠ¹ì • êµ¬/êµ°ì—ë§Œ êµ­í•œëœ ê³µì•½ì´ë‚˜ ë¹„ì „ì„ ë©”ì¸ìœ¼ë¡œ ë‚´ì„¸ìš°ì§€ ë§ˆì‹­ì‹œì˜¤.
- ë°˜ë“œì‹œ **"{user_profile.get('regionMetro', 'ë¶€ì‚°')} ì „ì²´ì˜ ê· í˜• ë°œì „"**ì´ë‚˜ **"ì‹œì • ì „ì²´ì˜ ì‡„ì‹ "**ê³¼ ì—°ê²° ì§€ì–´ ê±°ì‹œì ì¸ ê´€ì ì—ì„œ ì„œìˆ í•˜ì‹­ì‹œì˜¤.""")

        if author_bio and '"' in author_bio:
             warnings.append("""ğŸš¨ [CRITICAL] Bio ì¸ìš©êµ¬ ë³´ì¡´ ë²•ì¹™:
- ì‘ì„±ì ì •ë³´(Bio)ì— ìˆëŠ” **í°ë”°ì˜´í‘œ(" ")ë¡œ ë¬¶ì¸ ë¬¸ì¥**ì€ ì‚¬ìš©ìì˜ í•µì‹¬ ì„œì‚¬(Narrative)ì´ë¯€ë¡œ, ê¸ˆì§€ì–´ë‚˜ ë¯¼ê°í•œ ë‹¨ì–´(ì˜ˆ: êµ­íšŒì˜ì›)ê°€ í¬í•¨ë˜ì–´ ìˆë”ë¼ë„ **ì ˆëŒ€ ìˆ˜ì •/ì‚­ì œ/ê²€ì—´í•˜ì§€ ë§ê³  ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¸ìš©**í•˜ì‹­ì‹œì˜¤.""")
             
        return '\n\n'.join(warnings) if warnings else ''
