"""ì›ê³  í’ˆì§ˆ/ì„ ê±°ë²•/í‚¤ì›Œë“œ íœ´ë¦¬ìŠ¤í‹± ê²€ì¦ ëª¨ë“ˆ.

Node.js `functions/services/posts/validation.js`ì˜ í•µì‹¬ ê²€ì¦ ë¡œì§ í¬íŒ….
"""

from __future__ import annotations

import json
import html
import logging
import re
from typing import Any, Awaitable, Callable, Dict, List, Optional, Sequence

from agents.common.election_rules import get_election_stage
from agents.common.fact_guard import extract_numeric_tokens, find_unsupported_numeric_tokens
from agents.common.legal import ViolationDetector

from .corrector import apply_corrections, summarize_violations
from .critic import has_hard_violations, run_critic_review, summarize_guidelines
from .generation_stages import GENERATION_STAGES, create_progress_state, create_retry_message

logger = logging.getLogger(__name__)


# ============================================================================
# ì„ ê±°ë²• í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ìƒìˆ˜
# ============================================================================

ALLOWED_ENDINGS: List[re.Pattern[str]] = [
    re.compile(r"ì…ë‹ˆë‹¤\.?$"),
    re.compile(r"ìŠµë‹ˆë‹¤\.?$"),
    re.compile(r"ë©ë‹ˆë‹¤\.?$"),
    re.compile(r"í–ˆìŠµë‹ˆë‹¤\.?$"),
    re.compile(r"ë˜ì—ˆìŠµë‹ˆë‹¤\.?$"),
    re.compile(r"ì˜€ìŠµë‹ˆë‹¤\.?$"),
    re.compile(r"ì—ˆìŠµë‹ˆë‹¤\.?$"),
    re.compile(r"í•´ì•¼\s*í•©ë‹ˆë‹¤\.?$"),
    re.compile(r"ë˜ì–´ì•¼\s*í•©ë‹ˆë‹¤\.?$"),
    re.compile(r"í•„ìš”í•©ë‹ˆë‹¤\.?$"),
    re.compile(r"ë°”ëë‹ˆë‹¤\.?$"),
    re.compile(r"ìƒê°í•©ë‹ˆë‹¤\.?$"),
    re.compile(r"ë´…ë‹ˆë‹¤\.?$"),
    re.compile(r"ì••ë‹ˆë‹¤\.?$"),
    re.compile(r"ëŠë‚ë‹ˆë‹¤\.?$"),
    re.compile(r"[ê¹Œìš”ê¹Œ]\?$"),
    re.compile(r"[ìŠµì]ë‹ˆê¹Œ\?$"),
    re.compile(r"ë¼ê³ \s*í•©ë‹ˆë‹¤\.?$"),
    re.compile(r"ë‹µë‹ˆë‹¤\.?$"),
]

EXPLICIT_PLEDGE_PATTERNS: List[re.Pattern[str]] = [
    re.compile(r"ì•½ì†ë“œë¦½ë‹ˆë‹¤"),
    re.compile(r"ì•½ì†í•©ë‹ˆë‹¤"),
    re.compile(r"ê³µì•½í•©ë‹ˆë‹¤"),
    re.compile(r"ë°˜ë“œì‹œ.*í•˜ê² ìŠµë‹ˆë‹¤"),
    re.compile(r"ê¼­.*í•˜ê² ìŠµë‹ˆë‹¤"),
    re.compile(r"ì œê°€.*í•˜ê² ìŠµë‹ˆë‹¤"),
    re.compile(r"ì €ëŠ”.*í•˜ê² ìŠµë‹ˆë‹¤"),
    re.compile(r"ë‹¹ì„ ë˜ë©´"),
    re.compile(r"ë‹¹ì„ \s*í›„"),
]


def _strip_html(text: str) -> str:
    return re.sub(r"\s+", " ", re.sub(r"<[^>]*>", " ", text or "")).strip()


def extract_sentences(text: str) -> List[str]:
    plain_text = _strip_html(text)
    if not plain_text:
        return []
    return [
        sentence.strip()
        for sentence in re.split(r"(?<=[.?!])\s+", plain_text)
        if sentence and len(sentence.strip()) > 10
    ]


def is_allowed_ending(sentence: str) -> bool:
    return any(pattern.search(sentence or "") for pattern in ALLOWED_ENDINGS)


def is_explicit_pledge(sentence: str) -> bool:
    return any(pattern.search(sentence or "") for pattern in EXPLICIT_PLEDGE_PATTERNS)


def contains_pledge_candidate(sentence: str) -> bool:
    return bool(re.search(r"ê² [ìŠµì–´]", sentence or ""))


def _extract_json_object(raw: str) -> Optional[Dict[str, Any]]:
    text = (raw or "").strip()
    if not text:
        return None
    text = re.sub(r"```(?:json)?\s*([\s\S]*?)```", r"\1", text).strip()
    try:
        parsed = json.loads(text)
        return parsed if isinstance(parsed, dict) else None
    except json.JSONDecodeError:
        pass
    match = re.search(r"\{[\s\S]*\}", text)
    if not match:
        return None
    try:
        parsed = json.loads(match.group(0))
        return parsed if isinstance(parsed, dict) else None
    except json.JSONDecodeError:
        return None


async def check_pledges_with_llm(
    sentences: Sequence[str],
    model_name: str = "gemini-2.5-flash",
) -> List[Dict[str, Any]]:
    if not sentences:
        return []

    prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ì„ ê±°ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ë¬¸ì¥ë“¤ì´ "ì •ì¹˜ì¸ ë³¸ì¸ì˜ ì„ ê±° ê³µì•½/ì•½ì†"ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.

[íŒë‹¨ ê¸°ì¤€]
- ê³µì•½ O: ì •ì¹˜ì¸ ë³¸ì¸ì´ ì£¼ì–´ë¡œ, ë¯¸ë˜ì— ~í•˜ê² ë‹¤ëŠ” ì•½ì†
  ì˜ˆ: "ì¼ìë¦¬ë¥¼ ë§Œë“¤ê² ìŠµë‹ˆë‹¤", "êµí†µ ë¬¸ì œë¥¼ í•´ê²°í•˜ê² ìŠµë‹ˆë‹¤"

- ê³µì•½ X: ë‹¤ìŒì€ ê³µì•½ì´ ì•„ë‹˜
  ì˜ˆ: "ë¹„ê°€ ì˜¤ê² ìŠµë‹ˆë‹¤" (ë‚ ì”¨ ì˜ˆì¸¡)
  ì˜ˆ: "ì¢‹ì€ ê²°ê³¼ê°€ ìˆê² ìŠµë‹ˆë‹¤" (í¬ë§/ê¸°ëŒ€)
  ì˜ˆ: "ì •ë¶€ê°€ í•´ì•¼ê² ìŠµë‹ˆë‹¤" (ì œ3ì ë‹¹ìœ„)
  ì˜ˆ: "í•¨ê»˜ ë§Œë“¤ì–´ê°€ê² ìŠµë‹ˆë‹¤" (ì‹œë¯¼ ì°¸ì—¬ í˜¸ì†Œ, ë§¥ë½ì— ë”°ë¼)

[ê²€ì¦ ëŒ€ìƒ ë¬¸ì¥]
{chr(10).join(f'{i + 1}. "{s}"' for i, s in enumerate(sentences))}

[ì¶œë ¥ í˜•ì‹ - JSON]
{{
  "results": [
    {{ "index": 1, "isPledge": true/false, "reason": "íŒë‹¨ ê·¼ê±°" }},
    ...
  ]
}}"""

    try:
        from agents.common.gemini_client import generate_content_async

        response = await generate_content_async(
            prompt,
            model_name=model_name,
            temperature=0.1,
            response_mime_type="application/json",
        )
        parsed = _extract_json_object(response) or {}
        results = parsed.get("results")
        if not isinstance(results, list):
            raise ValueError("results í•„ë“œ ì—†ìŒ")

        normalized: list[Dict[str, Any]] = []
        for idx, item in enumerate(results):
            if not isinstance(item, dict):
                continue
            item_index = int(item.get("index", idx + 1))
            source_idx = max(1, item_index) - 1
            sentence = sentences[source_idx] if source_idx < len(sentences) else sentences[idx]
            normalized.append(
                {
                    "sentence": sentence,
                    "isPledge": bool(item.get("isPledge")),
                    "reason": str(item.get("reason") or "íŒë‹¨ ê·¼ê±° ì—†ìŒ"),
                }
            )
        return normalized
    except Exception as exc:
        logger.warning("LLM ê³µì•½ ê²€ì¦ ì‹¤íŒ¨, ë³´ìˆ˜ì  ì²˜ë¦¬: %s", exc)
        return [
            {"sentence": sentence, "isPledge": True, "reason": "LLM ê²€ì¦ ì‹¤íŒ¨ - ë³´ìˆ˜ì  ì²˜ë¦¬"}
            for sentence in sentences
        ]


def _collect_bribery_violations(plain_text: str) -> List[Dict[str, Any]]:
    violations: list[Dict[str, Any]] = []
    for item in ViolationDetector.check_bribery_risk(plain_text):
        matches = item.get("matches") or []
        sentence = matches[0] if matches else ""
        violations.append(
            {
                "sentence": sentence,
                "type": "BRIBERY",
                "reason": item.get("reason") or "ê¸°ë¶€í–‰ìœ„ ê¸ˆì§€ ìœ„ë°˜ ìœ„í—˜",
            }
        )
    return violations


def _collect_fact_violations(plain_text: str) -> List[Dict[str, Any]]:
    violations: list[Dict[str, Any]] = []
    for item in ViolationDetector.check_fact_claims(plain_text):
        matches = item.get("matches") or item.get("claims") or []
        sentence = matches[0] if matches else ""
        severity = str(item.get("severity") or "").upper()
        violations.append(
            {
                "sentence": sentence,
                "type": "FACT_CRITICAL" if severity == "CRITICAL" else "FACT_WARNING",
                "reason": item.get("reason") or "í—ˆìœ„ì‚¬ì‹¤/ë¹„ë°© ìœ„í—˜",
            }
        )
    return violations


async def detect_election_law_violation_hybrid(
    content: str,
    status: str | None,
    title: str = "",
    *,
    model_name: str = "gemini-2.5-flash",
) -> Dict[str, Any]:
    if not status:
        return {"passed": True, "violations": [], "skipped": True}

    election_stage = get_election_stage(status)
    if not election_stage or election_stage.get("name") != "STAGE_1":
        return {"passed": True, "violations": [], "skipped": True}

    full_text = f"{title or ''} {content or ''}"
    sentences = extract_sentences(full_text)
    violations: list[Dict[str, Any]] = []
    llm_candidates: list[str] = []

    for sentence in sentences:
        if is_explicit_pledge(sentence):
            violations.append(
                {
                    "sentence": sentence[:60] + ("..." if len(sentence) > 60 else ""),
                    "type": "EXPLICIT_PLEDGE",
                    "reason": "ëª…ì‹œì  ê³µì•½ í‘œí˜„",
                }
            )
            continue
        if is_allowed_ending(sentence):
            continue
        if contains_pledge_candidate(sentence):
            llm_candidates.append(sentence)

    if llm_candidates:
        llm_results = await check_pledges_with_llm(llm_candidates, model_name=model_name)
        for result in llm_results:
            if result.get("isPledge"):
                sentence = str(result.get("sentence") or "")
                violations.append(
                    {
                        "sentence": sentence[:60] + ("..." if len(sentence) > 60 else ""),
                        "type": "LLM_DETECTED",
                        "reason": str(result.get("reason") or "ê³µì•½ì„± í‘œí˜„"),
                    }
                )

    plain_text = _strip_html(full_text)
    violations.extend(_collect_bribery_violations(plain_text))
    violations.extend(_collect_fact_violations(plain_text))

    return {
        "passed": len(violations) == 0,
        "violations": violations,
        "status": status,
        "stage": election_stage.get("name"),
        "stats": {
            "totalSentences": len(sentences),
            "llmChecked": len(llm_candidates),
            "violationCount": len(violations),
        },
    }


# ============================================================================
# íœ´ë¦¬ìŠ¤í‹± í’ˆì§ˆ ê²€ì¦
# ============================================================================


def detect_sentence_repetition(content: str) -> Dict[str, Any]:
    plain_text = _strip_html(content)
    sentences = [
        sentence.strip()
        for sentence in re.split(r"(?<=[.?!])\s+", plain_text)
        if sentence and len(sentence.strip()) > 20
    ]
    normalized = [re.sub(r"\s+", "", sentence).lower() for sentence in sentences]
    counts: Dict[str, Dict[str, Any]] = {}
    repeated_sentences: list[str] = []

    for idx, sentence in enumerate(normalized):
        if sentence not in counts:
            counts[sentence] = {"count": 0, "original": sentences[idx]}
        counts[sentence]["count"] += 1

    for value in counts.values():
        if value["count"] >= 2:
            original = str(value["original"])
            repeated_sentences.append(f"\"{original[:50]}...\" ({value['count']}íšŒ ë°˜ë³µ)")

    return {"passed": len(repeated_sentences) == 0, "repeatedSentences": repeated_sentences}


def detect_phrase_repetition(content: str) -> Dict[str, Any]:
    plain_text = _strip_html(content)
    words = [word for word in re.split(r"\s+", plain_text) if word]
    phrase_count: Dict[str, int] = {}

    for n in range(3, 7):
        for idx in range(0, len(words) - n + 1):
            phrase = " ".join(words[idx : idx + n])
            if len(phrase) < 10:
                continue
            phrase_count[phrase] = phrase_count.get(phrase, 0) + 1

    over_limit = sorted(
        [(phrase, count) for phrase, count in phrase_count.items() if count >= 3],
        key=lambda item: len(item[0]),
        reverse=True,
    )

    covered: set[str] = set()
    repeated_phrases: list[str] = []
    for phrase, count in over_limit:
        if any(existing.find(phrase) >= 0 for existing in covered):
            continue
        covered.add(phrase)
        repeated_phrases.append(f"\"{phrase[:40]}{'...' if len(phrase) > 40 else ''}\" ({count}íšŒ ë°˜ë³µ)")

    return {"passed": len(repeated_phrases) == 0, "repeatedPhrases": repeated_phrases}


def detect_near_duplicate_sentences(content: str, threshold: float = 0.6) -> Dict[str, Any]:
    plain_text = _strip_html(content)
    sentences = [
        sentence.strip()
        for sentence in re.split(r"(?<=[.?!])\s+", plain_text)
        if sentence and len(sentence.strip()) > 25
    ]
    word_sets: list[set[str]] = []
    for sentence in sentences:
        words = [word for word in re.split(r"\s+", re.sub(r"[.?!,]", "", sentence)) if len(word) >= 2]
        word_sets.append(set(words))

    similar_pairs: list[Dict[str, Any]] = []
    for i in range(len(sentences)):
        for j in range(i + 1, len(sentences)):
            set_a = word_sets[i]
            set_b = word_sets[j]
            if len(set_a) < 3 or len(set_b) < 3:
                continue
            intersection = len(set_a.intersection(set_b))
            union = len(set_a.union(set_b))
            similarity = (intersection / union) if union else 0
            if similarity < threshold:
                continue
            if similarity >= 0.95:
                continue
            similar_pairs.append(
                {
                    "a": sentences[i][:50] + ("..." if len(sentences[i]) > 50 else ""),
                    "b": sentences[j][:50] + ("..." if len(sentences[j]) > 50 else ""),
                    "similarity": round(similarity * 100),
                }
            )

    return {"passed": len(similar_pairs) == 0, "similarPairs": similar_pairs}


def detect_election_law_violation(content: str, status: str | None, title: str = "") -> Dict[str, Any]:
    if not status:
        return {"passed": True, "violations": [], "skipped": True}

    election_stage = get_election_stage(status)
    if not election_stage or election_stage.get("name") != "STAGE_1":
        return {"passed": True, "violations": [], "skipped": True}

    plain_text = _strip_html(f"{title or ''} {content or ''}")

    pledge_patterns = [
        r"ì¶”ì§„í•˜ê² ìŠµë‹ˆë‹¤",
        r"ì‹¤í˜„í•˜ê² ìŠµë‹ˆë‹¤",
        r"ë§Œë“¤ê² ìŠµë‹ˆë‹¤",
        r"í•´ë‚´ê² ìŠµë‹ˆë‹¤",
        r"ì „ê°œí•˜ê² ìŠµë‹ˆë‹¤",
        r"ì œê³µí•˜ê² ìŠµë‹ˆë‹¤",
        r"í™œì„±í™”í•˜ê² ìŠµë‹ˆë‹¤",
        r"ê°œì„ í•˜ê² ìŠµë‹ˆë‹¤",
        r"í™•ëŒ€í•˜ê² ìŠµë‹ˆë‹¤",
        r"ê°•í™”í•˜ê² ìŠµë‹ˆë‹¤",
        r"ì„¤ë¦½í•˜ê² ìŠµë‹ˆë‹¤",
        r"êµ¬ì¶•í•˜ê² ìŠµë‹ˆë‹¤",
        r"ë§ˆë ¨í•˜ê² ìŠµë‹ˆë‹¤",
        r"ì§€ì›í•˜ê² ìŠµë‹ˆë‹¤",
        r"í•´ê²°í•˜ê² ìŠµë‹ˆë‹¤",
        r"ë°”ê¾¸ê² ìŠµë‹ˆë‹¤",
        r"í¼ì¹˜ê² ìŠµë‹ˆë‹¤",
        r"ì´ë£¨ê² ìŠµë‹ˆë‹¤",
        r"ì—´ê² ìŠµë‹ˆë‹¤",
        r"ì„¸ìš°ê² ìŠµë‹ˆë‹¤",
        r"ì´ë¤„ë‚´ê² ìŠµë‹ˆë‹¤",
        r"í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤",
        r"ë“œë¦¬ê² ìŠµë‹ˆë‹¤",
        r"ì•½ì†ë“œë¦¬ê² ìŠµë‹ˆë‹¤",
        r"ë°”ê¿‰ë‹ˆë‹¤",
        r"ë§Œë“­ë‹ˆë‹¤",
        r"ì´ë£¹ë‹ˆë‹¤",
        r"í•´ê²°í•©ë‹ˆë‹¤",
        r"ì•½ì†í•©ë‹ˆë‹¤",
        r"ì‹¤í˜„í•©ë‹ˆë‹¤",
        r"ì±…ì„ì§‘ë‹ˆë‹¤",
    ]

    violations: list[str] = []
    for pattern in pledge_patterns:
        matches = re.findall(pattern, plain_text)
        if matches:
            violations.append(f"\"{matches[0]}\" ({len(matches)}íšŒ) - ê³µì•½ì„± í‘œí˜„")

    bribery_items = ViolationDetector.check_bribery_risk(plain_text)
    for item in bribery_items:
        violations.append(f"ğŸ”´ {item.get('reason') or 'ê¸°ë¶€í–‰ìœ„ ê¸ˆì§€ ìœ„ë°˜ ìœ„í—˜'}")

    fact_items = ViolationDetector.check_fact_claims(plain_text)
    for item in fact_items:
        severity = str(item.get("severity") or "").upper()
        emoji = "ğŸ”´" if severity == "CRITICAL" else "âš ï¸"
        violations.append(f"{emoji} {item.get('reason') or 'í—ˆìœ„ì‚¬ì‹¤/ë¹„ë°© ìœ„í—˜'}")

    return {
        "passed": len(violations) == 0,
        "violations": violations,
        "status": status,
        "stage": election_stage.get("name"),
        "hasCritical": bool(bribery_items) or any(
            str(item.get("severity") or "").upper() == "CRITICAL" for item in fact_items
        ),
    }


def validate_title_quality(
    title: str,
    user_keywords: Optional[Sequence[str]] = None,
    content: str = "",
    options: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    options = options or {}
    strict_facts = options.get("strictFacts") is True
    user_keywords = list(user_keywords or [])

    if not title:
        return {"passed": True, "issues": [], "details": {}}

    issues: list[Dict[str, Any]] = []
    details: Dict[str, Any] = {
        "length": len(title),
        "maxLength": 25,
        "keywordPosition": None,
        "abstractExpressions": [],
        "hasNumbers": False,
    }

    if len(title) < 10:
        issues.append(
            {
                "type": "title_too_short",
                "severity": "critical",
                "description": f"ì œëª©ì´ ë„ˆë¬´ ì§§ìŒ ({len(title)}ì)",
                "instruction": "10ì ì´ìƒìœ¼ë¡œ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”. ë‹¨ìˆœ í‚¤ì›Œë“œ ë‚˜ì—´ ê¸ˆì§€.",
            }
        )

    if len(title) > 25:
        issues.append(
            {
                "type": "title_length",
                "severity": "critical",
                "description": f"ì œëª© {len(title)}ì â†’ 25ì ì´ˆê³¼ (ë„¤ì´ë²„ì—ì„œ ì˜ë¦¼)",
                "instruction": "25ì ì´ë‚´ë¡œ ì¤„ì´ì„¸ìš”. ë¶ˆí•„ìš”í•œ ì¡°ì‚¬, ë¶€ì œëª©(:, -) ì œê±°.",
            }
        )

    if user_keywords:
        primary_kw = user_keywords[0]
        kw_index = title.find(primary_kw)
        details["keywordPosition"] = kw_index

        if kw_index == -1:
            issues.append(
                {
                    "type": "keyword_missing",
                    "severity": "high",
                    "description": f"í•µì‹¬ í‚¤ì›Œë“œ \"{primary_kw}\" ì œëª©ì— ì—†ìŒ",
                    "instruction": f"\"{primary_kw}\"ë¥¼ ì œëª© ì•ë¶€ë¶„ì— í¬í•¨í•˜ì„¸ìš”.",
                }
            )
        elif kw_index > 10:
            issues.append(
                {
                    "type": "keyword_position",
                    "severity": "medium",
                    "description": f"í‚¤ì›Œë“œ \"{primary_kw}\" ìœ„ì¹˜ {kw_index}ì â†’ ë„ˆë¬´ ë’¤ìª½",
                    "instruction": "í•µì‹¬ í‚¤ì›Œë“œëŠ” ì œëª© ì•ìª½ 8ì ì´ë‚´ì— ë°°ì¹˜í•˜ì„¸ìš” (ì•ìª½ 1/3 ë²•ì¹™).",
                }
            )

        clean_title = re.sub(r"\s+", "", title)
        clean_kw = re.sub(r"\s+", "", primary_kw)
        if clean_kw and clean_kw in clean_title and len(clean_title) <= len(clean_kw) + 4:
            issues.append(
                {
                    "type": "title_too_generic",
                    "severity": "critical",
                    "description": "ì œëª©ì´ í‚¤ì›Œë“œì™€ ë„ˆë¬´ ìœ ì‚¬í•¨ (ë‹¨ìˆœ ëª…ì‚¬í˜•)",
                    "instruction": "ì„œìˆ ì–´ì¸ \"í˜„ì•ˆ ì§„ë‹¨\", \"í•µì‹¬ ë¶„ì„\", \"ì´ìŠˆ ì ê²€\" ë“±ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì—¬ êµ¬ì²´í™”í•˜ì„¸ìš”.",
                }
            )

    if content:
        title_numeric_tokens = extract_numeric_tokens(title)
        content_numeric_tokens = extract_numeric_tokens(content)
        if title_numeric_tokens:
            if not content_numeric_tokens:
                issues.append(
                    {
                        "type": "title_number_mismatch",
                        "severity": "high",
                        "description": "ì œëª©ì— ìˆ˜ì¹˜ê°€ ìˆìœ¼ë‚˜ ë³¸ë¬¸ì— ê·¼ê±° ìˆ˜ì¹˜ ì—†ìŒ",
                        "instruction": "ë³¸ë¬¸ì— ì‹¤ì œë¡œ ìˆëŠ” ìˆ˜ì¹˜/ë‹¨ìœ„ë¥¼ ì œëª©ì— ì‚¬ìš©í•˜ì„¸ìš”.",
                    }
                )
            else:
                missing_tokens = [token for token in title_numeric_tokens if token not in content_numeric_tokens]
                if missing_tokens:
                    issues.append(
                        {
                            "type": "title_number_mismatch",
                            "severity": "high",
                            "description": f"ì œëª© ìˆ˜ì¹˜/ë‹¨ìœ„ê°€ ë³¸ë¬¸ê³¼ ë¶ˆì¼ì¹˜: {', '.join(missing_tokens)}",
                            "instruction": "ë³¸ë¬¸ì— ì‹¤ì œë¡œ ë“±ì¥í•˜ëŠ” ìˆ˜ì¹˜/ë‹¨ìœ„ë¥¼ ì œëª©ì— ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.",
                        }
                    )

    abstract_patterns = [
        ("ë¹„ì „", r"ë¹„ì „"),
        ("í˜ì‹ ", r"í˜ì‹ "),
        ("ë°œì „", r"ë°œì „"),
        ("ë…¸ë ¥", r"ë…¸ë ¥"),
        ("ìµœì„ ", r"ìµœì„ "),
        ("ì•½ì†", r"ì•½ì†"),
        ("ë‹¤ì§", r"ë‹¤ì§"),
        ("í•¨ê»˜", r"í•¨ê»˜"),
        ("í™•ì¶©", r"í™•ì¶©"),
        ("ê°œì„ ", r"ê°œì„ "),
        ("ì¶”ì§„", r"ì¶”ì§„"),
        ("ì‹œê¸‰", r"ì‹œê¸‰"),
        ("ê°•í™”", r"ê°•í™”"),
        ("ì¦ì§„", r"ì¦ì§„"),
        ("ë„ëª¨", r"ë„ëª¨"),
        ("í–¥ìƒ", r"í–¥ìƒ"),
        ("í™œì„±í™”", r"í™œì„±í™”"),
        ("ì„ ë„", r"ì„ ë„"),
        ("ì„ ì§„", r"ì„ ì§„"),
        ("ë¯¸ë˜", r"ë¯¸ë˜"),
    ]
    found_abstract = [word for word, pattern in abstract_patterns if re.search(pattern, title)]
    if found_abstract:
        details["abstractExpressions"] = found_abstract
        issues.append(
            {
                "type": "abstract_expression",
                "severity": "medium",
                "description": f"ì¶”ìƒì  í‘œí˜„ ì‚¬ìš©: {', '.join(found_abstract)}",
                "instruction": "êµ¬ì²´ì  ìˆ˜ì¹˜ë‚˜ ì‚¬ì‹¤ë¡œ ëŒ€ì²´í•˜ì„¸ìš”. ì˜ˆ: \"ë°œì „\" â†’ \"40% ì¦ê°€\", \"ë¹„ì „\" â†’ \"3ëŒ€ í•µì‹¬ ì •ì±…\"",
            }
        )

    details["hasNumbers"] = bool(re.search(r"\d", title))
    if (not details["hasNumbers"]) and issues and (not strict_facts):
        issues.append(
            {
                "type": "no_numbers",
                "severity": "low",
                "description": "ìˆ«ì/êµ¬ì²´ì  ë°ì´í„° ì—†ìŒ",
                "instruction": "ê°€ëŠ¥í•˜ë©´ ìˆ«ìë¥¼ í¬í•¨í•˜ì„¸ìš”. ì˜ˆ: \"3ëŒ€ ì •ì±…\", \"120ì–µ í™•ë³´\", \"40% ê°œì„ \"",
            }
        )

    has_blocking_issue = any(issue.get("severity") in {"critical", "high"} for issue in issues)
    return {"passed": not has_blocking_issue, "issues": issues, "details": details}


def run_heuristic_validation_sync(
    content: str,
    status: str,
    title: str = "",
    options: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    options = options or {}
    fact_allowlist = options.get("factAllowlist")

    issues: list[str] = []

    repetition_result = detect_sentence_repetition(content)
    if not repetition_result.get("passed", True):
        issues.append(f"âš ï¸ ë¬¸ì¥ ë°˜ë³µ ê°ì§€: {', '.join(repetition_result.get('repeatedSentences', []))}")

    phrase_result = detect_phrase_repetition(content)
    if not phrase_result.get("passed", True):
        issues.append(f"âš ï¸ êµ¬ë¬¸ ë°˜ë³µ ê°ì§€: {', '.join(phrase_result.get('repeatedPhrases', []))}")

    near_dup_result = detect_near_duplicate_sentences(content)
    if not near_dup_result.get("passed", True):
        summary = ", ".join(
            f"\"{pair['a']}\" â‰ˆ \"{pair['b']}\" ({pair['similarity']}%)"
            for pair in (near_dup_result.get("similarPairs") or [])[:3]
        )
        issues.append(f"âš ï¸ ìœ ì‚¬ ë¬¸ì¥ ê°ì§€: {summary}")

    election_result = detect_election_law_violation(content, status, title)
    if not election_result.get("passed", True):
        issues.append(f"âš ï¸ ì„ ê±°ë²• ìœ„ë°˜ í‘œí˜„: {', '.join(election_result.get('violations', []))}")

    fact_check_result = None
    if fact_allowlist:
        content_check = find_unsupported_numeric_tokens(content, fact_allowlist)
        title_check = find_unsupported_numeric_tokens(title, fact_allowlist) if title else {"passed": True, "unsupported": []}
        fact_check_result = {"content": content_check, "title": title_check}

    return {
        "passed": len(issues) == 0,
        "issues": issues,
        "details": {
            "repetition": repetition_result,
            "electionLaw": election_result,
            "factCheck": fact_check_result,
        },
    }


async def run_heuristic_validation(
    content: str,
    status: str,
    title: str = "",
    options: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    options = options or {}
    use_llm = options.get("useLLM", True)
    user_keywords = list(options.get("userKeywords") or [])
    fact_allowlist = options.get("factAllowlist")
    model_name = options.get("modelName", "gemini-2.5-flash")

    issues: list[str] = []

    repetition_result = detect_sentence_repetition(content)
    if not repetition_result.get("passed", True):
        issues.append(f"âš ï¸ ë¬¸ì¥ ë°˜ë³µ ê°ì§€: {', '.join(repetition_result.get('repeatedSentences', []))}")

    phrase_result = detect_phrase_repetition(content)
    if not phrase_result.get("passed", True):
        issues.append(f"âš ï¸ êµ¬ë¬¸ ë°˜ë³µ ê°ì§€: {', '.join(phrase_result.get('repeatedPhrases', []))}")

    near_dup_result = detect_near_duplicate_sentences(content)
    if not near_dup_result.get("passed", True):
        summary = ", ".join(
            f"\"{pair['a']}\" â‰ˆ \"{pair['b']}\" ({pair['similarity']}%)"
            for pair in (near_dup_result.get("similarPairs") or [])[:3]
        )
        issues.append(f"âš ï¸ ìœ ì‚¬ ë¬¸ì¥ ê°ì§€: {summary}")

    if use_llm:
        election_result = await detect_election_law_violation_hybrid(
            content,
            status,
            title,
            model_name=model_name,
        )
        if not election_result.get("passed", True):
            violation_summary = ", ".join(
                f"\"{item.get('sentence', '')}\" ({item.get('reason', '')})"
                for item in (election_result.get("violations") or [])
            )
            issues.append(f"âš ï¸ ì„ ê±°ë²• ìœ„ë°˜: {violation_summary}")
    else:
        election_result = detect_election_law_violation(content, status, title)
        if not election_result.get("passed", True):
            issues.append(f"âš ï¸ ì„ ê±°ë²• ìœ„ë°˜ í‘œí˜„: {', '.join(election_result.get('violations', []))}")

    title_result = validate_title_quality(
        title,
        user_keywords=user_keywords,
        content=content,
        options={"strictFacts": bool(fact_allowlist)},
    )
    if not title_result.get("passed", True):
        blocking_title_issues = [
            issue.get("description")
            for issue in (title_result.get("issues") or [])
            if issue.get("severity") in {"critical", "high"}
        ]
        if blocking_title_issues:
            issues.append(f"âš ï¸ ì œëª© í’ˆì§ˆ ë¬¸ì œ: {', '.join(blocking_title_issues)}")

    fact_check_result = None
    if fact_allowlist:
        content_check = find_unsupported_numeric_tokens(content, fact_allowlist)
        title_check = find_unsupported_numeric_tokens(title, fact_allowlist) if title else {"passed": True, "unsupported": []}
        fact_check_result = {"content": content_check, "title": title_check}

    return {
        "passed": len(issues) == 0,
        "issues": issues,
        "details": {
            "repetition": repetition_result,
            "electionLaw": election_result,
            "titleQuality": title_result,
            "factCheck": fact_check_result,
        },
    }


# ============================================================================
# ì´ˆë‹¹ì  í˜‘ë ¥ / í•µì‹¬ ë¬¸êµ¬ / ë¹„íŒ ëŒ€ìƒ ê²€ì¦
# ============================================================================


BIPARTISAN_FORBIDDEN_PHRASES = [
    "ì •ì‹ ì„ ì´ì–´ë°›ì•„",
    "ëœ»ì„ ë°›ë“¤ì–´",
    "ë°°ì›Œì•¼ í•©ë‹ˆë‹¤",
    "ë°°ìš¸ ì ",
    "ê¹Šì€ ìš¸ë¦¼",
    "ìš©ê¸°ì— ë°•ìˆ˜",
    "ê·€ê°ì´ ë©ë‹ˆë‹¤",
    "ë³¸ë°›ì•„ì•¼",
    "ì¡´ê²½í•©ë‹ˆë‹¤",
    "ë©˜í† ",
    "ìŠ¤ìŠ¹",
    "ê¹Šì€ ê°ëª…",
    "ìš°ë¦¬ë³´ë‹¤ ë‚«ë‹¤",
    "ìš°ë¦¬ë³´ë‹¤ í›¨ì”¬ ë‚«ë‹¤",
    "ìš°ë¦¬ëŠ” ì €ë ‡ê²Œ ëª»í•œë‹¤",
    "ì •ì±…ì´ 100% ë§ë‹¤",
    "ì „ì ìœ¼ë¡œ ë™ì˜í•œë‹¤",
    "ì™„ì „íˆ ì˜³ë‹¤",
    "ì •ì¹˜ì¸ ì¤‘ ìµœê³ ",
    "ìœ ì¼í•˜ê²Œ ë¯¿ì„ ìˆ˜ ìˆë‹¤",
    "ê°€ì¥ í›Œë¥­í•˜ë‹¤",
    "ê°œì¸ì ìœ¼ë¡œ ì¢‹ì•„í•œë‹¤",
    "í—Œì‹ ì ì¸ ë…¸ë ¥",
    "í—Œì‹ ì ì¸ ëª¨ìŠµ",
]


def detect_bipartisan_forbidden_phrases(content: str) -> Dict[str, Any]:
    violations: list[str] = []
    corrected = content or ""

    for phrase in BIPARTISAN_FORBIDDEN_PHRASES:
        if phrase not in corrected:
            continue
        violations.append(phrase)
        if phrase == "ê·€ê°ì´ ë©ë‹ˆë‹¤":
            corrected = corrected.replace(phrase, "ì£¼ëª©í•  ë§Œí•©ë‹ˆë‹¤")
        elif phrase == "ë°°ì›Œì•¼ í•©ë‹ˆë‹¤":
            corrected = corrected.replace(phrase, "ì°¸ê³ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        elif phrase == "ê¹Šì€ ê°ëª…":
            corrected = corrected.replace(phrase, "ê´€ì‹¬")
        elif "í—Œì‹ ì ì¸" in phrase:
            corrected = corrected.replace(phrase, "ê¾¸ì¤€í•œ ë…¸ë ¥")
        else:
            corrected = corrected.replace(phrase, "")

    corrected = re.sub(r"\s+", " ", corrected)
    corrected = re.sub(r"\s+\.", ".", corrected).strip()
    return {"hasForbidden": len(violations) > 0, "violations": violations, "correctedContent": corrected}


def calculate_praise_proportion(content: str, rival_names: Optional[Sequence[str]] = None) -> Dict[str, Any]:
    rival_names = list(rival_names or [])
    if not rival_names:
        return {"percentage": 0, "exceedsLimit": False, "rivalMentions": 0}

    sentences = extract_sentences(content or "")
    rival_mention_sentences = 0
    for sentence in sentences:
        if any(name in sentence for name in rival_names):
            rival_mention_sentences += 1

    percentage = round((rival_mention_sentences / len(sentences)) * 100) if sentences else 0
    return {
        "percentage": percentage,
        "exceedsLimit": percentage > 15,
        "rivalMentions": rival_mention_sentences,
        "totalSentences": len(sentences),
    }


def validate_bipartisan_praise(content: str, options: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    options = options or {}
    rival_names = list(options.get("rivalNames") or [])
    category = str(options.get("category") or "")

    if ("bipartisan" not in category) and ("ì´ˆë‹¹ì " not in category):
        return {"passed": True, "issues": [], "correctedContent": content}

    issues: list[str] = []
    forbidden_result = detect_bipartisan_forbidden_phrases(content or "")
    if forbidden_result["hasForbidden"]:
        issues.append(
            f"âš ï¸ ì´ˆë‹¹ì  í˜‘ë ¥ ê¸ˆì§€ í‘œí˜„ ê°ì§€ ë° ìë™ ìˆ˜ì •: {', '.join(forbidden_result['violations'])}"
        )

    proportion_result = calculate_praise_proportion(forbidden_result["correctedContent"], rival_names)
    if proportion_result.get("exceedsLimit"):
        issues.append(
            f"âš ï¸ ê²½ìŸì ì¹­ì°¬ ë¹„ì¤‘ ì´ˆê³¼: {proportion_result['percentage']}% "
            f"({proportion_result['rivalMentions']}/{proportion_result['totalSentences']} ë¬¸ì¥) - ê¶Œì¥ 15% ì´í•˜"
        )

    return {
        "passed": len(issues) == 0,
        "issues": issues,
        "correctedContent": forbidden_result["correctedContent"],
        "details": {"forbiddenPhrases": forbidden_result, "praiseProportion": proportion_result},
    }


def validate_key_phrase_inclusion(content: str, required_phrases: Optional[Sequence[str]] = None) -> Dict[str, Any]:
    required_phrases = list(required_phrases or [])
    if not content or not required_phrases:
        return {"passed": True, "missing": [], "included": [], "details": {}}

    plain_content = _strip_html(content)
    included: list[Dict[str, str]] = []
    missing: list[str] = []
    details: Dict[str, Any] = {}

    for phrase in required_phrases:
        if not phrase or len(phrase) < 5:
            continue
        exact_match = phrase in plain_content
        core_words = [
            word
            for word in re.split(r"\s+", re.sub(r"[.?!,~]", "", phrase))
            if len(word) >= 4 and not re.match(r"^(ìˆìŠµë‹ˆë‹¤|ì—†ìŠµë‹ˆë‹¤|í•©ë‹ˆë‹¤|ì…ë‹ˆë‹¤|ê²ƒì…ë‹ˆë‹¤|ì•„ë‹™ë‹ˆë‹¤)$", word)
        ]
        core_word_matches = [word for word in core_words if word in plain_content]
        paraphrase_match = bool(core_words) and len(core_word_matches) >= (len(core_words) + 1) // 2

        details[phrase] = {
            "exactMatch": exact_match,
            "paraphraseMatch": paraphrase_match,
            "coreWords": core_words,
            "coreWordMatches": core_word_matches,
            "included": exact_match or paraphrase_match,
        }

        if exact_match or paraphrase_match:
            included.append({"phrase": phrase, "matchType": "exact" if exact_match else "paraphrase"})
        else:
            missing.append(phrase)

    has_exact_match = any(item.get("matchType") == "exact" for item in included)
    all_included = len(missing) == 0
    passed = all_included and (len(required_phrases) <= 1 or has_exact_match)

    return {
        "passed": passed,
        "missing": missing,
        "included": included,
        "hasExactMatch": has_exact_match,
        "details": details,
        "message": (
            None
            if passed
            else (
                f"í•µì‹¬ ë¬¸êµ¬ ëˆ„ë½: {', '.join(f'\"{item[:30]}...\"' for item in missing)}"
                if missing
                else "ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¸ìš©ëœ ë¬¸êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œ 1ê°œëŠ” ì›ë¬¸ ì¸ìš©ì´ í•„ìš”í•©ë‹ˆë‹¤."
            )
        ),
    }


def validate_criticism_target(content: str, responsibility_target: str) -> Dict[str, Any]:
    if not content or not responsibility_target:
        return {"passed": True, "targetMentioned": False, "count": 0}

    plain_content = re.sub(r"<[^>]*>", " ", content)
    target_parts = [part for part in re.split(r"\s+", responsibility_target) if part]
    target_name = target_parts[0] if target_parts else responsibility_target
    escaped_name = re.escape(target_name)

    matches = re.findall(escaped_name, plain_content)
    count = len(matches)
    count_passed = count >= 2

    intent_reversal_patterns = [
        re.compile(rf"{escaped_name}[^.]*(?:í˜‘ë ¥|ì¡´ì¤‘|í•¨ê»˜|ë…¸ë ¥|ì¸ì •|ê³µë¡œ|ì„±ê³¼)"),
        re.compile(rf"(?:í˜‘ë ¥|ì¡´ì¤‘|í•¨ê»˜)í•˜ì—¬[^.]*{escaped_name}"),
        re.compile(rf"{escaped_name}[^.]*(?:ì˜\s*ë…¸ë ¥|ê³¼\s*í˜‘ë ¥|ê³¼\s*í•¨ê»˜|ì„\s*ì¡´ì¤‘)"),
    ]

    intent_reversal_count = 0
    intent_reversal_matches: list[str] = []
    for pattern in intent_reversal_patterns:
        detected = pattern.findall(plain_content)
        intent_reversal_count += len(detected)
        intent_reversal_matches.extend(detected)

    criticism_patterns = [
        re.compile(rf"{escaped_name}[^.]*(?:ì—­ë¶€ì¡±|í•œê³„|ë¬¸ì œ|ì±…ì„|ë¹„íŒ|ì‹¤íŒ¨|ë¶€ì¡±)"),
        re.compile(rf"(?:ì—­ë¶€ì¡±|í•œê³„|ë¬¸ì œ|ì±…ì„|ë¹„íŒ|ì‹¤íŒ¨|ë¶€ì¡±)[^.]*{escaped_name}"),
    ]
    criticism_context_count = sum(len(pattern.findall(plain_content)) for pattern in criticism_patterns)
    has_intent_reversal = intent_reversal_count > 0 and intent_reversal_count > criticism_context_count
    passed = count_passed and (not has_intent_reversal)

    message = None
    if not count_passed:
        message = f"ë¹„íŒ ëŒ€ìƒ \"{target_name}\" ì–¸ê¸‰ ë¶€ì¡± (í˜„ì¬ {count}íšŒ, ìµœì†Œ 2íšŒ í•„ìš”)"
    elif has_intent_reversal:
        message = (
            f"ğŸ”´ ì˜ë„ ì—­ì „ ê°ì§€: ë¹„íŒ ëŒ€ìƒ \"{target_name}\"ì´(ê°€) ê¸ì •ì  ë§¥ë½(í˜‘ë ¥/ì¡´ì¤‘/í•¨ê»˜)ìœ¼ë¡œ ì–¸ê¸‰ë¨. "
            f"ì›ë³¸ì˜ ë¹„íŒì  ë…¼ì¡°ë¥¼ ìœ ì§€í•˜ì„¸ìš”. [ê°ì§€ëœ í‘œí˜„: {', '.join(intent_reversal_matches[:2])}]"
        )

    return {
        "passed": passed,
        "targetMentioned": count > 0,
        "count": count,
        "targetName": target_name,
        "hasIntentReversal": has_intent_reversal,
        "intentReversalCount": intent_reversal_count,
        "criticismContextCount": criticism_context_count,
        "message": message,
    }


# ============================================================================
# í‚¤ì›Œë“œ ì‚½ì… ê²€ì¦
# ============================================================================


def count_keyword_occurrences(content: str, keyword: str) -> int:
    clean_content = re.sub(r"<[^>]*>", "", content or "")
    escaped = re.escape(keyword or "")
    if not escaped:
        return 0
    return len(re.findall(escaped, clean_content))


def build_keyword_variants(keyword: str) -> List[str]:
    trimmed = str(keyword or "").strip()
    if not trimmed:
        return []
    parts = [part for part in re.split(r"\s+", trimmed) if part]
    variants: list[str] = []
    if len(parts) >= 2:
        first = parts[0]
        rest = " ".join(parts[1:])
        variants.append(f"{first}ì˜ {rest}")
        variants.append(f"{rest} {first}")
    deduped: list[str] = []
    seen: set[str] = set()
    for item in variants:
        if item and item != trimmed and item not in seen:
            seen.add(item)
            deduped.append(item)
    return deduped


def count_keyword_coverage(content: str, keyword: str) -> int:
    if not keyword:
        return 0
    keywords = [keyword, *build_keyword_variants(keyword)]
    return sum(count_keyword_occurrences(content, item) for item in keywords)


def _keyword_user_threshold(user_keywords: Optional[Sequence[str]] = None) -> tuple[int, int]:
    normalized = [item for item in (user_keywords or []) if item]
    kw_count = len(normalized) if normalized else 1
    user_min_count = 3 if kw_count >= 2 else 5
    user_max_count = user_min_count + 1
    return user_min_count, user_max_count


def _parse_keyword_sections(content: str) -> List[Dict[str, Any]]:
    sections: list[Dict[str, Any]] = []
    h2_matches = list(re.finditer(r"<h2[^>]*>[\s\S]*?<\/h2>", content or "", re.IGNORECASE))

    if not h2_matches:
        return [
            {
                "type": "single",
                "startIndex": 0,
                "endIndex": len(content or ""),
                "content": content or "",
            }
        ]

    first_h2_start = h2_matches[0].start()
    if first_h2_start > 0:
        sections.append(
            {
                "type": "intro",
                "startIndex": 0,
                "endIndex": first_h2_start,
                "content": (content or "")[:first_h2_start],
            }
        )

    for idx, match in enumerate(h2_matches):
        start_index = match.start()
        end_index = h2_matches[idx + 1].start() if idx < len(h2_matches) - 1 else len(content or "")
        section_type = "conclusion" if idx == len(h2_matches) - 1 else f"body{idx + 1}"
        sections.append(
            {
                "type": section_type,
                "startIndex": start_index,
                "endIndex": end_index,
                "content": (content or "")[start_index:end_index],
            }
        )

    return sections


def _section_priority(section_type: str) -> int:
    if section_type.startswith("body"):
        return 0
    if section_type == "conclusion":
        return 1
    if section_type == "intro":
        return 2
    return 3


def _select_keyword_section_indexes(
    sections: Sequence[Dict[str, Any]],
    keyword: str,
    needed: int,
) -> List[int]:
    if not sections or needed <= 0:
        return []

    indexed = list(enumerate(sections))
    ranked = sorted(
        indexed,
        key=lambda item: (
            count_keyword_coverage(str(item[1].get("content") or ""), keyword),
            _section_priority(str(item[1].get("type") or "")),
            item[0],
        ),
    )
    if not ranked:
        return []

    chosen: list[int] = []
    while len(chosen) < needed:
        progressed = False
        for idx, _section in ranked:
            chosen.append(idx)
            progressed = True
            if len(chosen) >= needed:
                break
        if not progressed:
            break
    return chosen[:needed]


def _build_keyword_enforcement_sentence(keyword: str, section_type: str, variant_index: int = 0) -> str:
    safe_kw = html.escape(str(keyword or "").strip())
    section_key = "single"
    if section_type == "intro":
        section_key = "intro"
    elif section_type == "conclusion":
        section_key = "conclusion"
    elif section_type.startswith("body"):
        section_key = "body"

    templates = {
        "intro": [
            "{kw} ì´ìŠˆëŠ” ì‹œë¯¼ ìƒí™œê³¼ ë§ë‹¿ì€ í•µì‹¬ í˜„ì•ˆì…ë‹ˆë‹¤.",
            "ì§€ê¸ˆ {kw} ì˜ì œëŠ” í˜„ì¥ì—ì„œ ì²´ê°ë„ê°€ ë†’ì€ ë¬¸ì œì…ë‹ˆë‹¤.",
        ],
        "body": [
            "{kw} ë¬¸ì œëŠ” í˜„ì¥ ì‚¬ë¡€ì™€ ë°ì´í„°ë¡œ í•¨ê»˜ ì ê²€í•´ì•¼ í•©ë‹ˆë‹¤.",
            "{kw} ê´€ë ¨ ìŸì ì€ ìƒí™œ ë¶ˆí¸ê³¼ ì •ì±… íš¨ê³¼ë¥¼ í•¨ê»˜ ë´ì•¼ í•©ë‹ˆë‹¤.",
        ],
        "conclusion": [
            "ëìœ¼ë¡œ {kw} ê³¼ì œëŠ” ì§€ì†ì ì¸ ì ê²€ê³¼ ì‹¤í–‰ì´ í•„ìš”í•©ë‹ˆë‹¤.",
            "{kw} ì˜ì œëŠ” ë§ˆì§€ë§‰ê¹Œì§€ ì±…ì„ ìˆê²Œ í™•ì¸í•´ì•¼ í•  ì‚¬ì•ˆì…ë‹ˆë‹¤.",
        ],
        "single": [
            "{kw} ì´ìŠˆëŠ” ì§€ê¸ˆ ê°€ì¥ ìš°ì„ ì ìœ¼ë¡œ ì ê²€í•´ì•¼ í•  ê³¼ì œì…ë‹ˆë‹¤.",
            "{kw} ê´€ë ¨ ë…¼ì ì€ ì‚¬ì‹¤ê³¼ í˜„ì¥ ì¤‘ì‹¬ìœ¼ë¡œ ê³„ì† ì‚´í´ë´ì•¼ í•©ë‹ˆë‹¤.",
        ],
    }
    options = templates.get(section_key, templates["single"])
    template = options[variant_index % len(options)]
    return template.format(kw=safe_kw)


def enforce_keyword_requirements(
    content: str,
    user_keywords: Optional[Sequence[str]] = None,
    auto_keywords: Optional[Sequence[str]] = None,
    target_word_count: Optional[int] = None,
    max_iterations: int = 2,
) -> Dict[str, Any]:
    working_content = str(content or "")
    user_keywords = [item for item in (user_keywords or []) if item]
    auto_keywords = [item for item in (auto_keywords or []) if item]

    initial_result = validate_keyword_insertion(
        working_content,
        user_keywords,
        auto_keywords,
        target_word_count,
    )
    if not working_content or (not user_keywords and not auto_keywords):
        return {
            "content": working_content,
            "edited": False,
            "insertions": [],
            "keywordResult": initial_result,
        }
    if initial_result.get("valid"):
        return {
            "content": working_content,
            "edited": False,
            "insertions": [],
            "keywordResult": initial_result,
        }

    insertions: list[Dict[str, Any]] = []
    per_keyword_insertions: Dict[str, int] = {}
    current_result = initial_result

    for _ in range(max_iterations):
        details = (current_result.get("details") or {}).get("keywords") or {}
        sections = _parse_keyword_sections(working_content)
        if not details or not sections:
            break

        insertion_plan: Dict[int, List[Dict[str, Any]]] = {}
        needs_fix = False

        for keyword in [*user_keywords, *auto_keywords]:
            keyword_info = details.get(keyword) or {}
            expected = int(keyword_info.get("expected") or (1 if keyword in auto_keywords else _keyword_user_threshold(user_keywords)[0]))
            coverage = int(keyword_info.get("coverage") or 0)
            deficit = max(0, expected - coverage)
            if deficit <= 0:
                continue

            needs_fix = True
            target_indexes = _select_keyword_section_indexes(sections, keyword, deficit)
            for section_idx in target_indexes:
                if section_idx < 0 or section_idx >= len(sections):
                    continue
                section = sections[section_idx]
                variant_index = per_keyword_insertions.get(keyword, 0)
                sentence = _build_keyword_enforcement_sentence(
                    keyword,
                    str(section.get("type") or ""),
                    variant_index,
                )
                per_keyword_insertions[keyword] = variant_index + 1
                end_index = int(section.get("endIndex") or 0)
                insertion_plan.setdefault(end_index, []).append(
                    {
                        "keyword": keyword,
                        "section": section_idx,
                        "sectionType": section.get("type"),
                        "sentence": sentence,
                    }
                )

        if not needs_fix or not insertion_plan:
            break

        for position in sorted(insertion_plan.keys(), reverse=True):
            payload = insertion_plan[position]
            paragraphs = "".join(f"\n<p>{item['sentence']}</p>" for item in payload)
            working_content = working_content[:position] + paragraphs + working_content[position:]
            insertions.extend(payload)

        current_result = validate_keyword_insertion(
            working_content,
            user_keywords,
            auto_keywords,
            target_word_count,
        )
        if current_result.get("valid"):
            break

    return {
        "content": working_content,
        "edited": working_content != str(content or ""),
        "insertions": insertions,
        "keywordResult": current_result,
    }


def build_fallback_draft(params: Optional[Dict[str, Any]] = None) -> str:
    params = params or {}
    topic = str(params.get("topic") or "í˜„ì•ˆ").strip()
    full_name = str(params.get("fullName") or "").strip()
    user_keywords = list(params.get("userKeywords") or [])

    greeting = f"ì¡´ê²½í•˜ëŠ” ì‹œë¯¼ ì—¬ëŸ¬ë¶„, {full_name}ì…ë‹ˆë‹¤." if full_name else "ì¡´ê²½í•˜ëŠ” ì‹œë¯¼ ì—¬ëŸ¬ë¶„."
    keyword_sentences = [f"{keyword}ì™€ ê´€ë ¨í•œ í˜„í™©ì„ ì ê²€í•©ë‹ˆë‹¤." for keyword in user_keywords[:5] if keyword]
    keyword_paragraph = f"<p>{' '.join(keyword_sentences)}</p>" if keyword_sentences else ""

    blocks = [
        f"<p>{greeting} {topic}ì— ëŒ€í•´ í•µì‹¬ í˜„í™©ì„ ì •ë¦¬í•©ë‹ˆë‹¤.</p>",
        "<h2>í˜„ì•ˆ ê°œìš”</h2>",
        f"<p>{topic}ì˜ êµ¬ì¡°ì  ë°°ê²½ê³¼ ìµœê·¼ íë¦„ì„ ê°ê´€ì ìœ¼ë¡œ ì‚´í´ë´…ë‹ˆë‹¤.</p>",
        keyword_paragraph,
        "<h2>í•µì‹¬ ìŸì </h2>",
        "<p>ì›ì¸ê³¼ ì˜í–¥ì„ êµ¬ë¶„í•´ ì‚¬ì‹¤ê´€ê³„ë¥¼ ì •ë¦¬í•˜ê³ , ë…¼ì˜ê°€ í•„ìš”í•œ ì§€ì ì„ í™•ì¸í•©ë‹ˆë‹¤.</p>",
        "<h2>í™•ì¸ ê³¼ì œ</h2>",
        "<p>ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•œ ë°ì´í„°ì™€ ì ê²€ ê³¼ì œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.</p>",
        f"<p>{full_name} ë“œë¦¼</p>" if full_name else "",
    ]
    return "\n".join(block for block in blocks if block)


def validate_keyword_insertion(
    content: str,
    user_keywords: Optional[Sequence[str]] = None,
    auto_keywords: Optional[Sequence[str]] = None,
    target_word_count: Optional[int] = None,
) -> Dict[str, Any]:
    _ = target_word_count
    user_keywords = [item for item in (user_keywords or []) if item]
    auto_keywords = [item for item in (auto_keywords or []) if item]
    plain_text = re.sub(r"\s", "", re.sub(r"<[^>]*>", "", content or ""))
    actual_word_count = len(plain_text)

    user_min_count, user_max_count = _keyword_user_threshold(user_keywords)
    auto_min_count = 1

    results: Dict[str, Dict[str, Any]] = {}
    all_valid = True

    for keyword in user_keywords:
        exact_count = count_keyword_occurrences(content, keyword)
        coverage_count = count_keyword_coverage(content, keyword)
        is_under_min = coverage_count < user_min_count
        is_over_max = exact_count > user_max_count or coverage_count > user_max_count
        is_valid = (not is_under_min) and (not is_over_max)
        results[keyword] = {
            "count": coverage_count,
            "exactCount": exact_count,
            "coverage": coverage_count,
            "expected": user_min_count,
            "max": user_max_count,
            "valid": is_valid,
            "type": "user",
        }
        if not is_valid:
            all_valid = False

    for keyword in auto_keywords:
        exact_count = count_keyword_occurrences(content, keyword)
        coverage_count = count_keyword_coverage(content, keyword)
        is_valid = coverage_count >= auto_min_count
        results[keyword] = {
            "count": coverage_count,
            "exactCount": exact_count,
            "coverage": coverage_count,
            "expected": auto_min_count,
            "valid": is_valid,
            "type": "auto",
        }

    all_keywords = [*user_keywords, *auto_keywords]
    total_keyword_chars = 0
    for keyword in all_keywords:
        occurrences = count_keyword_coverage(content, keyword)
        total_keyword_chars += len(re.sub(r"\s", "", keyword)) * occurrences
    density = (total_keyword_chars / actual_word_count * 100) if actual_word_count else 0

    return {
        "valid": all_valid,
        "details": {
            "keywords": results,
            "density": {
                "value": f"{density:.2f}",
                "valid": True,
                "optimal": 1.5 <= density <= 2.5,
            },
            "wordCount": actual_word_count,
        },
    }


async def _generate_draft_text(
    prompt: str,
    model_name: str,
    generate_fn: Optional[Callable[..., Awaitable[str]]] = None,
) -> str:
    if generate_fn:
        try:
            candidate = generate_fn(prompt, model_name)
        except TypeError:
            candidate = generate_fn(prompt)
        result = await candidate
        return str(result or "")

    from agents.common.gemini_client import generate_content_async

    return await generate_content_async(
        prompt,
        model_name=model_name,
        temperature=1.0,
    )


async def validate_and_retry(
    *,
    prompt: str,
    model_name: str,
    full_name: str | None = None,
    full_region: str | None = None,
    target_word_count: Optional[int] = None,
    user_keywords: Optional[Sequence[str]] = None,
    auto_keywords: Optional[Sequence[str]] = None,
    status: str | None = None,
    fact_allowlist: Optional[Sequence[str]] = None,
    rag_context: Optional[str] = None,
    author_name: Optional[str] = None,
    topic: Optional[str] = None,
    on_progress: Optional[Callable[[Dict[str, Any]], None]] = None,
    max_attempts: int = 3,
    max_critic_attempts: int = 2,
    generate_fn: Optional[Callable[..., Awaitable[str]]] = None,
) -> str:
    """AI ì‘ë‹µ ìƒì„± + íœ´ë¦¬ìŠ¤í‹± ê²€ì¦ + Critic/Corrector ë£¨í”„."""

    _ = (full_region, target_word_count, auto_keywords)
    user_keywords = list(user_keywords or [])
    status_value = status or ""
    author = author_name or full_name
    critic_model = "gemini-2.5-flash"
    corrector_model = "gemini-2.5-flash"

    def notify_progress(stage_id: str, additional_info: Optional[Dict[str, Any]] = None) -> None:
        if not callable(on_progress):
            return
        try:
            on_progress(create_progress_state(stage_id, additional_info or {}))
        except Exception as exc:
            logger.warning("Progress ì½œë°± ì˜¤ë¥˜: %s", exc)

    best_version: Optional[str] = None
    best_score = 0
    draft: Optional[str] = None
    heuristic_passed = False

    notify_progress("DRAFTING")

    for attempt in range(1, max_attempts + 1):
        logger.info("ì›ê³  ìƒì„± ì‹œë„ (%s/%s)", attempt, max_attempts)

        try:
            candidate = await _generate_draft_text(prompt, model_name, generate_fn=generate_fn)
        except Exception as exc:
            logger.warning("ì›ê³  ìƒì„± ì‹¤íŒ¨ (%s/%s): %s", attempt, max_attempts, exc)
            continue

        if not candidate or len(candidate.strip()) < 100:
            logger.warning("ì‘ë‹µì´ ë„ˆë¬´ ì§§ì•„ ì¬ì‹œë„í•©ë‹ˆë‹¤ (%s/%s)", attempt, max_attempts)
            continue

        notify_progress("BASIC_CHECK", {"attempt": attempt})
        heuristic_result = await run_heuristic_validation(
            candidate,
            status_value,
            "",
            {
                "useLLM": False,
                "factAllowlist": fact_allowlist,
                "userKeywords": user_keywords,
                "modelName": model_name,
            },
        )

        issues = list(heuristic_result.get("issues") or [])
        draft = candidate

        if heuristic_result.get("passed", False):
            heuristic_passed = True
            best_version = candidate
            best_score = max(best_score, 70)
            logger.info("íœ´ë¦¬ìŠ¤í‹± ê²€ì¦ í†µê³¼ (%s/%s)", attempt, max_attempts)
            break

        estimated_score = max(10, 70 - (len(issues) * 15))
        if estimated_score > best_score:
            best_score = estimated_score
            best_version = candidate

        logger.warning("íœ´ë¦¬ìŠ¤í‹± ê²€ì¦ ì‹¤íŒ¨ (%s/%s): %s", attempt, max_attempts, issues)
        if attempt < max_attempts:
            notify_progress("DRAFTING", {"attempt": attempt + 1})

    if not heuristic_passed:
        logger.error("%síšŒ ì‹œë„ í›„ì—ë„ íœ´ë¦¬ìŠ¤í‹± ê²€ì¦ ì‹¤íŒ¨", max_attempts)
        fallback = best_version or build_fallback_draft(
            {
                "topic": topic,
                "fullName": full_name,
                "userKeywords": user_keywords,
            }
        )
        notify_progress("COMPLETED", {"warning": "í’ˆì§ˆ ê²€ì¦ ì¼ë¶€ ì‹¤íŒ¨", "score": best_score})
        return fallback

    guidelines = summarize_guidelines(status_value, topic)
    current_draft = draft or ""
    critic_attempt = 0

    while critic_attempt < max_critic_attempts:
        critic_attempt += 1
        retry_msg = create_retry_message(critic_attempt, max_critic_attempts, best_score)
        notify_progress(
            "EDITOR_REVIEW",
            {
                "attempt": critic_attempt,
                "message": retry_msg.get("message"),
                "detail": retry_msg.get("detail"),
            },
        )

        critic_report = await run_critic_review(
            draft=current_draft,
            rag_context=rag_context,
            guidelines=guidelines,
            status=status_value,
            topic=topic,
            author_name=author,
            model_name=critic_model,
        )

        score = int(critic_report.get("score") or 0)
        if score > best_score:
            best_score = score
            best_version = current_draft

        if critic_report.get("passed") or (not critic_report.get("needsRetry")):
            notify_progress("FINALIZING")
            final_check = await run_heuristic_validation(
                current_draft,
                status_value,
                "",
                {
                    "useLLM": True,
                    "factAllowlist": fact_allowlist,
                },
            )

            if not final_check.get("passed", True):
                details = final_check.get("details") or {}
                election_law = details.get("electionLaw") or {}
                violations = election_law.get("violations") or []
                if violations:
                    correction_result = await apply_corrections(
                        draft=current_draft,
                        violations=[
                            {
                                "type": "HARD",
                                "field": "content",
                                "issue": item.get("reason"),
                                "suggestion": f"\"{item.get('sentence', '')}\" í‘œí˜„ì„ ìˆ˜ì •í•˜ì„¸ìš”",
                                "severity": "HARD",
                                "location": "ë³¸ë¬¸",
                                "problematic": item.get("sentence", ""),
                            }
                            for item in violations
                        ],
                        rag_context=rag_context,
                        author_name=author,
                        status=status_value,
                        model_name=corrector_model,
                    )
                    if correction_result.get("success") and (not correction_result.get("unchanged")):
                        current_draft = str(correction_result.get("corrected") or current_draft)

            notify_progress("COMPLETED", {"score": score})
            return current_draft

        violations = list(critic_report.get("violations") or [])
        if has_hard_violations(critic_report):
            notify_progress("CORRECTING", {"violations": summarize_violations(violations)})
            correction_result = await apply_corrections(
                draft=current_draft,
                violations=violations,
                rag_context=rag_context,
                author_name=author,
                status=status_value,
                model_name=corrector_model,
            )
            if correction_result.get("success") and (not correction_result.get("unchanged")):
                current_draft = str(correction_result.get("corrected") or current_draft)
            else:
                logger.warning("Corrector ìˆ˜ì • ì‹¤íŒ¨: %s", correction_result.get("error") or "ë³€ê²½ ì—†ìŒ")
        else:
            notify_progress("COMPLETED", {"score": score, "warnings": len(violations)})
            return current_draft

    notify_progress(
        "COMPLETED",
        {
            "score": best_score,
            "warning": "ì¼ë¶€ í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ - ìˆ˜ë™ ê²€í†  ê¶Œì¥",
        },
    )
    final_draft = best_version if best_score >= 70 else current_draft
    return final_draft or current_draft or (draft or "")


async def evaluate_quality_with_llm(content: str, model_name: str) -> Dict[str, Any]:
    """Legacy í˜¸í™˜ í•¨ìˆ˜ (Critic ëŒ€ì²´ ì´ì „ API)."""

    _ = (content, model_name)
    return {"passed": True, "issues": [], "suggestions": []}


# JS í˜¸í™˜ ë³„ì¹­
extractSentences = extract_sentences
isAllowedEnding = is_allowed_ending
isExplicitPledge = is_explicit_pledge
containsPledgeCandidate = contains_pledge_candidate
checkPledgesWithLLM = check_pledges_with_llm
detectElectionLawViolationHybrid = detect_election_law_violation_hybrid
detectSentenceRepetition = detect_sentence_repetition
detectPhraseRepetition = detect_phrase_repetition
detectNearDuplicateSentences = detect_near_duplicate_sentences
detectElectionLawViolation = detect_election_law_violation
validateTitleQuality = validate_title_quality
runHeuristicValidationSync = run_heuristic_validation_sync
runHeuristicValidation = run_heuristic_validation
detectBipartisanForbiddenPhrases = detect_bipartisan_forbidden_phrases
calculatePraiseProportion = calculate_praise_proportion
validateBipartisanPraise = validate_bipartisan_praise
validateKeyPhraseInclusion = validate_key_phrase_inclusion
validateCriticismTarget = validate_criticism_target
countKeywordOccurrences = count_keyword_occurrences
buildKeywordVariants = build_keyword_variants
countKeywordCoverage = count_keyword_coverage
buildFallbackDraft = build_fallback_draft
validateKeywordInsertion = validate_keyword_insertion
enforceKeywordRequirements = enforce_keyword_requirements
validateAndRetry = validate_and_retry
evaluateQualityWithLLM = evaluate_quality_with_llm


__all__ = [
    "ALLOWED_ENDINGS",
    "EXPLICIT_PLEDGE_PATTERNS",
    "BIPARTISAN_FORBIDDEN_PHRASES",
    "GENERATION_STAGES",
    "extract_sentences",
    "is_allowed_ending",
    "is_explicit_pledge",
    "contains_pledge_candidate",
    "check_pledges_with_llm",
    "detect_election_law_violation_hybrid",
    "detect_sentence_repetition",
    "detect_phrase_repetition",
    "detect_near_duplicate_sentences",
    "detect_election_law_violation",
    "validate_title_quality",
    "run_heuristic_validation_sync",
    "run_heuristic_validation",
    "detect_bipartisan_forbidden_phrases",
    "calculate_praise_proportion",
    "validate_bipartisan_praise",
    "validate_key_phrase_inclusion",
    "validate_criticism_target",
    "count_keyword_occurrences",
    "build_keyword_variants",
    "count_keyword_coverage",
    "build_fallback_draft",
    "validate_keyword_insertion",
    "enforce_keyword_requirements",
    "validate_and_retry",
    "evaluate_quality_with_llm",
]
